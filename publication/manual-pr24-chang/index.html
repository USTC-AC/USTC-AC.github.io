<!DOCTYPE html><html lang="en-us" >

<head><script src="/livereload.js?mindelay=10&amp;v=2&amp;port=1313&amp;path=livereload" data-no-instant defer></script>
  <meta charset="utf-8" />
  <meta name="viewport" content="width=device-width, initial-scale=1" />
  <meta name="author" content="Indigo6" src="https://github.com/Indigo6"  />
  <meta name="copyright" content="Copyright © Indigo6. All rights reserved."  src="https://github.com/Indigo6" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge" />
  
  
  
    <meta name="generator" content="Wowchemy 5.3.0 for Hugo" />
  

  <!--busuanzi view count -->
  <script async src="//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script>
  
  

  
  
  
  
  
    
    
    
  
  

  <meta name="author" content="Lin Fang" />

  
  
  
    
  
  <meta name="description" content="Pose-robust personalized facial expression recognition is rather challenging, as facial expressions are subject-related and pose-dependent. Multi-source domain adaptation tries to leverage knowledge from multiple source domains to boost the performance of the target domain. For pose-robust personalized facial expression recognition, the images of the source domain are from multiple sources since the images are under different poses. Thus, in this paper, we propose a novel unsupervised multi-source domain adaptation framework for pose-robust personalized facial expression recognition. The proposed framework consists of five components: a source encoder, a target encoder, an expression classifier, a view discriminator, and a domain discriminator. The source encoder and target encoder learn facial representations from facial images in the training and testing sets, respectively. The expression classifier recognizes expressions from the learned representations. The view discriminator classifies poses. The domain discriminator distinguishes the learned representations of the source domain from those of the target domain. The source encoder works cooperatively with the expression classifier and adversarially with the view discriminator. The target encoder aims to learn domain robust representations and fool the domain discriminator, while the domain discriminator tries to distinguish between the source and target domains. Through adversarial learning, the distribution of the learned representations from the source domain converges to that from the target domain. Thus, the feature representation extracted by the target encoder is pose-invariant and target subject-specific. Experimental results demonstrate the superiority of the proposed method compared to related works." />

  
  <link rel="alternate" hreflang="en-us" href="http://localhost:1313/publication/manual-pr24-chang/" />

  









  




  
  

  
  
  
    <meta name="theme-color" content="#1565c0" />
  

  
  

  
  
  
  
    
    
      <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/academicons/1.9.0/css/academicons.min.css" integrity="sha512-W4yqoT1+8NLkinBLBZko+dFB2ZbHsYLDdr50VElllRcNt2Q4/GSs6u71UHKxB7S6JEMCp5Ve4xjh3eGQl/HRvg==" crossorigin="anonymous" media="print" onload="this.media='all'">
    
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.14.0/css/all.min.css" integrity="sha256-FMvZuGapsJLjouA6k7Eo2lusoAX9i0ShlWFG6qt7SLc=" crossorigin="anonymous" media="print" onload="this.media='all'">

    
    
    
    
      
      
    
    
    

    
    
    
      
    
    

    
    
    
      <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/leaflet/1.7.1/leaflet.min.css" integrity="sha512-1xoFisiGdy9nvho8EgXuXvnpR5GAMSjFwp40gSRE3NwdUdIMIKuPa7bqoUhLD0O/5tPNhteAsE5XyyMi5reQVA==" crossorigin="anonymous" media="print" onload="this.media='all'">
    

    

    
    
      
      

      
      

      
    
      
      

      
      

      
    
      
      

      
      

      
    
      
      

      
      

      
    
      
      

      
      

      
    
      
      

      
      

      
    
      
      

      
      

      
    
      
      

      
      

      
    
      
      

      
      

      
    
      
      

      
      

      
    
      
      

      
      

      
    
      
      

      
      

      
    
      
      

      
      

      
    
      
      

      
      

      
    
      
      

      
      
        
      

      
    
      
      

      
      

      
    
      
      

      
      

      
    
  

  
  
  

  
  
  
  
  
  <link rel="stylesheet" href="/css/wowchemy.css" />

  



  

  

  




  
  
  

  

  
    <link rel="manifest" href="/manifest.webmanifest" />
  

  <link rel="icon" type="image/png" href="/media/icon_huc3da470f1a231b6fd15916059be12d5d_66382_32x32_fill_lanczos_center_3.png" />
  <link rel="apple-touch-icon" type="image/png" href="/media/icon_huc3da470f1a231b6fd15916059be12d5d_66382_180x180_fill_lanczos_center_3.png" />

  <link rel="canonical" href="http://localhost:1313/publication/manual-pr24-chang/" />

  
  
  
  
  
  
  
  
    
    
  
  
  <meta property="twitter:card" content="summary" />
  
  <meta property="og:site_name" content="USTC-AC" />
  <meta property="og:url" content="http://localhost:1313/publication/manual-pr24-chang/" />
  <meta property="og:title" content="Pose-robust personalized facial expression recognition through unsupervised multi-source domain adaptation | USTC-AC" />
  <meta property="og:description" content="Pose-robust personalized facial expression recognition is rather challenging, as facial expressions are subject-related and pose-dependent. Multi-source domain adaptation tries to leverage knowledge from multiple source domains to boost the performance of the target domain. For pose-robust personalized facial expression recognition, the images of the source domain are from multiple sources since the images are under different poses. Thus, in this paper, we propose a novel unsupervised multi-source domain adaptation framework for pose-robust personalized facial expression recognition. The proposed framework consists of five components: a source encoder, a target encoder, an expression classifier, a view discriminator, and a domain discriminator. The source encoder and target encoder learn facial representations from facial images in the training and testing sets, respectively. The expression classifier recognizes expressions from the learned representations. The view discriminator classifies poses. The domain discriminator distinguishes the learned representations of the source domain from those of the target domain. The source encoder works cooperatively with the expression classifier and adversarially with the view discriminator. The target encoder aims to learn domain robust representations and fool the domain discriminator, while the domain discriminator tries to distinguish between the source and target domains. Through adversarial learning, the distribution of the learned representations from the source domain converges to that from the target domain. Thus, the feature representation extracted by the target encoder is pose-invariant and target subject-specific. Experimental results demonstrate the superiority of the proposed method compared to related works." /><meta property="og:image" content="http://localhost:1313/media/icon_huc3da470f1a231b6fd15916059be12d5d_66382_512x512_fill_lanczos_center_3.png" />
    <meta property="twitter:image" content="http://localhost:1313/media/icon_huc3da470f1a231b6fd15916059be12d5d_66382_512x512_fill_lanczos_center_3.png" /><meta property="og:locale" content="en-us" />
  
    
      <meta
        property="article:published_time"
        content="2024-02-19T09:50:00&#43;00:00"
      />
    
    <meta property="article:modified_time" content="2024-02-19T09:50:00&#43;00:00">
  

  


    









<script type="application/ld+json">
{
  "@context": "https://schema.org",
  "@type": "Article",
  "mainEntityOfPage": {
    "@type": "WebPage",
    "@id": "http://localhost:1313/publication/manual-pr24-chang/"
  },
  "headline": "Pose-robust personalized facial expression recognition through unsupervised multi-source domain adaptation",
  
  "datePublished": "2024-02-19T09:50:00Z",
  "dateModified": "2024-02-19T09:50:00Z",
  
  "author": {
    "@type": "Person",
    "name": "Shangfei Wang"
  },
  
  "publisher": {
    "@type": "Organization",
    "name": "USTC-AC",
    "logo": {
      "@type": "ImageObject",
      "url": "http://localhost:1313/media/icon_huc3da470f1a231b6fd15916059be12d5d_66382_192x192_fill_lanczos_center_3.png"
    }
  },
  "description": "Pose-robust personalized facial expression recognition is rather challenging, as facial expressions are subject-related and pose-dependent. Multi-source domain adaptation tries to leverage knowledge from multiple source domains to boost the performance of the target domain. For pose-robust personalized facial expression recognition, the images of the source domain are from multiple sources since the images are under different poses. Thus, in this paper, we propose a novel unsupervised multi-source domain adaptation framework for pose-robust personalized facial expression recognition. The proposed framework consists of five components: a source encoder, a target encoder, an expression classifier, a view discriminator, and a domain discriminator. The source encoder and target encoder learn facial representations from facial images in the training and testing sets, respectively. The expression classifier recognizes expressions from the learned representations. The view discriminator classifies poses. The domain discriminator distinguishes the learned representations of the source domain from those of the target domain. The source encoder works cooperatively with the expression classifier and adversarially with the view discriminator. The target encoder aims to learn domain robust representations and fool the domain discriminator, while the domain discriminator tries to distinguish between the source and target domains. Through adversarial learning, the distribution of the learned representations from the source domain converges to that from the target domain. Thus, the feature representation extracted by the target encoder is pose-invariant and target subject-specific. Experimental results demonstrate the superiority of the proposed method compared to related works."
}
</script>

  

  

  

  





  <title>Pose-robust personalized facial expression recognition through unsupervised multi-source domain adaptation | USTC-AC</title>
</head>


<body id="top" data-spy="scroll" data-offset="70" data-target='#TableOfContents' class='page-wrapper   ' data-wc-page-id="f3476ef2c63c61480f00ba21e28252d6" >

  
  
  
  
  
  
  
  
  
  <script src="/js/wowchemy-init.min.js"></script>

  


<aside class="search-modal" id="search">
  <div class="container">
    <section class="search-header">

      <div class="row no-gutters justify-content-between mb-3">
        <div class="col-6">
          <h1>Search</h1>
        </div>
        <div class="col-6 col-search-close">
          <a class="js-search" href="#" aria-label="Close"><i class="fas fa-times-circle text-muted" aria-hidden="true"></i></a>
        </div>
      </div>

      <div id="search-box">
        
        <input name="q" id="search-query" placeholder="Search..." autocapitalize="off"
        autocomplete="off" autocorrect="off" spellcheck="false" type="search" class="form-control"
        aria-label="Search...">
        
      </div>

      
      

      

    </section>
    <section class="section-search-results">

      <div id="search-hits">
        
      </div>

    </section>
  </div>
</aside>



  <div class="page-header">
    












<nav class="navbar navbar-expand-lg navbar-light compensate-for-scrollbar" id="navbar-main">
  <div class="container-xl">

    
    <div class="d-none d-lg-inline-flex">
      <a class="navbar-brand" href="/">USTC-AC</a>
    </div>
    

    
    <button type="button" class="navbar-toggler" data-toggle="collapse"
            data-target="#navbar-content" aria-controls="navbar-content" aria-expanded="false" aria-label="Toggle navigation">
    <span><i class="fas fa-bars"></i></span>
    </button>
    

    
    <div class="navbar-brand-mobile-wrapper d-inline-flex d-lg-none">
      <a class="navbar-brand" href="/">USTC-AC</a>
    </div>
    

    
    
    <div class="navbar-collapse main-menu-item collapse justify-content-end" id="navbar-content">

      
      <ul class="navbar-nav d-md-inline-flex">
        

        

        
        
        

        

        
        
        
        

        
          
            
            
          
          
        

        <li class="nav-item">
          <a class="nav-link " href="/post"><span>News</span></a>
        </li>

        
        

        

        
        
        
          
        

        

        
        
        
        

        
          
            
            
          
          
        

        <li class="nav-item">
          <a class="nav-link " href="/people"><span>People</span></a>
        </li>

        
        

        

        
        
        
          
        

        

        
        
        
        

        
          
            
            
          
          
        

        <li class="nav-item">
          <a class="nav-link " href="/graduated"><span>Graduated</span></a>
        </li>

        
        

        

        
        
        
          
        

        

        
        
        
        

        
          
            
            
          
          
        

        <li class="nav-item">
          <a class="nav-link " href="/research"><span>Research</span></a>
        </li>

        
        

        

        
        
        
          
        

        

        
        
        
        

        
          
            
            
          
          
        

        <li class="nav-item">
          <a class="nav-link " href="/publication"><span>Publications</span></a>
        </li>

        
        

        

        
        
        
          
        

        

        
        
        
        

        
          
            
            
          
          
        

        <li class="nav-item">
          <a class="nav-link " href="/datasets"><span>Datasets</span></a>
        </li>

        
        

        

        
        
        
          
        

        

        
        
        
        

        
          
            
            
          
          
        

        <li class="nav-item">
          <a class="nav-link " href="/contact"><span>Contact</span></a>
        </li>

        
        

      

        
      </ul>
    </div>

    <ul class="nav-icons navbar-nav flex-row ml-auto d-flex pl-md-2">

      
      

      
      
      <li class="nav-item">
        <a class="nav-link js-search" href="#" aria-label="Search"><i class="fas fa-search" aria-hidden="true"></i></a>
      </li>
      

      
      
      <li class="nav-item dropdown theme-dropdown">
        <a href="#" class="nav-link" data-toggle="dropdown" aria-haspopup="true" aria-label="Display preferences">
          <i class="fas fa-moon" aria-hidden="true"></i>
        </a>
        <div class="dropdown-menu">
          <a href="#" class="dropdown-item js-set-theme-light">
            <span>Light</span>
          </a>
          <a href="#" class="dropdown-item js-set-theme-dark">
            <span>Dark</span>
          </a>
          <a href="#" class="dropdown-item js-set-theme-auto">
            <span>Automatic</span>
          </a>
        </div>
      </li>
      

      
      

    </ul>

  </div>
</nav>


  </div>

  <div class="page-body">
    








<div class="pub">

  












  

  
  
  
<div class="article-container pt-3">
  <h1>Pose-robust personalized facial expression recognition through unsupervised multi-source domain adaptation</h1>

  

  
    


<div class="article-metadata">

  
  
  
  
  <div>
    

  <span >
      <a href="/author/shangfei-wang/">Shangfei Wang</a></span>, <span >
      <a href="/author/yanan-chang/">Ya&#39;nan Chang</a></span>, <span >
      <a href="/author/qiong-li/">Qiong Li</a></span>, <span >
      <a href="/author/can-wang/">Can Wang</a></span>
  </div>
  
  

  
  <span class="article-date">
    
    
      
    
    February 2024
  </span>
  

  

  

  
  
  
  
  
  

  
  

</div>

    




<div class="btn-links mb-3">
  
  








  
    
  



<a class="btn btn-outline-primary btn-page-header" href="https://pdf.sciencedirectassets.com/272206/1-s2.0-S0031320324X00023/1-s2.0-S0031320324000621/main.pdf?X-Amz-Security-Token=IQoJb3JpZ2luX2VjEDkaCXVzLWVhc3QtMSJGMEQCIEzaJf%2FmB7ntFppIvPij5dPe3Ma7D3D7Bp4HpY4C056XAiAeWiqUuHIzAIXXW1yMN1cpnMxbS1OuwY%2FOQj%2Fm7RC4aiqzBQgiEAUaDDA1OTAwMzU0Njg2NSIMlt2F2GKtBz73EaMwKpAFGWs9FTCBH1IxaXKPq%2BUXW3QtUVYR5aCIuKdEUM2835IlbRLWI%2FPVt0T8LQ6uEvSIQF%2BM1Q4mg0NbBbLjfvIMCDSehlNhwXAhmfD3juufygv391CBFGPv7bY0GpnqO1ZswY9P95%2FqtA81X6uduIb51HRG3s2jktajyJSp23wEZiO%2FSSrnfMh%2BdJkq0ABSrlCCtmdsgtSkGyCgZE1F7%2BtBOFljW2v%2FEXCX3kv5b4ioF9962raHJwaTJywbHrntfxbPOxIS7sNG5e1U4%2F1PNR6%2BMIobQ%2FnqpQ1bt7cw6ly30w9AyoQwT31Cg%2BZlWm6b7fHRsFECwmY7W9a%2BeEw3bkUAPZzV4wRN61XFj9%2Bv89eeQmQ1B9k6hlBqXdTKnGllAsyX7D1AUbf1IdVHnirMwms0H%2F9LYMb4PRxpP%2B1N56X9OZ9Dn1qtoo6RfC1DJhlVdf8eADiJLrIizaYcHf9Wg%2B8TzE89E%2BPyJ5hPEQINMWHFv5zOQoRlzPqol81MWmfj47wrpEHuO7TQw7LgsEtAL6PFwYQeF3vp%2Buhe8cKgW3T1iJNZpu%2FfAy8OdtglkqzxYmWuJlYH49FIzv%2BDURxLkFirQEJGZBqCN1X4mYwO0PY2HRB7bS4lps1wFIT1G5YOuMrlDuFaJpn%2Fz%2FCHn6uaqio17hHlpz8phA7fGxXdo8HP3tR1pfwT4dqMUPxKPqFCwjMuwhXR2B7KMRbwlebZA5Bt%2BeMnE4czwOPDovZ0fnqGBHvFQ4BwXKUcBpUKXQo%2B%2Ffioxki54J9MsthVok%2F7MftAJOCvpgzCbG6VTTwAeh9veJ6MjxQcxBsfrqBSpKor2r%2FWf2%2BnDgXfPCblKJlzMnii8WBGogNgIT8nO6%2BZ9S%2FEOnowoezPrgY6sgFcmQ6TVwGlH%2Fa0%2B1t%2BKklpU%2BQhxAUISWBv2OORrvP%2FZFodOpn%2BZb9A1QiEeYD5%2FuMcGWzuj32JwyGGt9NN0L0LUIj2BnJVSVXQyqB%2BQhF7OEJgRxa38rGiQvgjUTLJaXuLzW%2BQITmg1CEGiZ0LRGXl4e65YAvqda6ka17v0OiUMhGK7KRlt9qWhkEfaNfrEQ7X1xUUWNF8gJxCH5XyHpPPY67g%2B8iO2Q8MvVxorim%2Fofpr&amp;X-Amz-Algorithm=AWS4-HMAC-SHA256&amp;X-Amz-Date=20240220T015328Z&amp;X-Amz-SignedHeaders=host&amp;X-Amz-Expires=300&amp;X-Amz-Credential=ASIAQ3PHCVTYRBKPAKVO%2F20240220%2Fus-east-1%2Fs3%2Faws4_request&amp;X-Amz-Signature=09e0cb86f6878bbee2919ded99790f04e1614bd89a967ab9c483abc955ceafbf&amp;hash=ca3b45e3d77f066500f997e6ce1215e8fe5f6cd10e8f312b4e0b474f3f4a6ffe&amp;host=68042c943591013ac2b2430a89b270f6af2c76d8dfd086a07176afe7c76c2c61&amp;pii=S0031320324000621&amp;tid=spdf-4c2205ec-0222-4bee-b198-adf49b9eac3f&amp;sid=fb05b44e57b7004492983aa2c3c587c73068gxrqa&amp;type=client&amp;tsoh=d3d3LnNjaWVuY2VkaXJlY3QuY29t&amp;ua=05085d5a5e545e000558&amp;rr=85831db778dc8511&amp;cc=cn" target="_blank" rel="noopener">
  PDF
</a>



<a href="#" class="btn btn-outline-primary btn-page-header js-cite-modal"
        data-filename="/publication/manual-pr24-chang/cite.bib">
  Cite
</a>













<a class="btn btn-outline-primary btn-page-header" href="https://doi.org/10.1016/j.patcog.2024.110311" target="_blank" rel="noopener">
  DOI
</a>



</div>


  
</div>



  <div class="article-container" align="justify">

    
    <h3>Abstract</h3>
    <p class="pub-abstract">Pose-robust personalized facial expression recognition is rather challenging, as facial expressions are subject-related and pose-dependent. Multi-source domain adaptation tries to leverage knowledge from multiple source domains to boost the performance of the target domain. For pose-robust personalized facial expression recognition, the images of the source domain are from multiple sources since the images are under different poses. Thus, in this paper, we propose a novel unsupervised multi-source domain adaptation framework for pose-robust personalized facial expression recognition. The proposed framework consists of five components: a source encoder, a target encoder, an expression classifier, a view discriminator, and a domain discriminator. The source encoder and target encoder learn facial representations from facial images in the training and testing sets, respectively. The expression classifier recognizes expressions from the learned representations. The view discriminator classifies poses. The domain discriminator distinguishes the learned representations of the source domain from those of the target domain. The source encoder works cooperatively with the expression classifier and adversarially with the view discriminator. The target encoder aims to learn domain robust representations and fool the domain discriminator, while the domain discriminator tries to distinguish between the source and target domains. Through adversarial learning, the distribution of the learned representations from the source domain converges to that from the target domain. Thus, the feature representation extracted by the target encoder is pose-invariant and target subject-specific. Experimental results demonstrate the superiority of the proposed method compared to related works.</p>
    

    
    
    <div class="row">
      <div class="col-md-1"></div>
      <div class="col-md-10">
        <div class="row">
          <div class="col-12 col-md-3 pub-row-heading">Type</div>
          <div class="col-12 col-md-9">
            <a href="/publication/#2">
              Journal article
            </a>
          </div>
        </div>
      </div>
      <div class="col-md-1"></div>
    </div>
    <div class="d-md-none space-below"></div>
    

    
    <div class="row">
      <div class="col-md-1"></div>
      <div class="col-md-10">
        <div class="row">
          <div class="col-12 col-md-3 pub-row-heading">Publication</div>
          <div class="col-12 col-md-9"><em>Pattern Recognition</em></div>
        </div>
      </div>
      <div class="col-md-1"></div>
    </div>
    <div class="d-md-none space-below"></div>
    

    <div class="space-below"></div>

    <div class="article-style"></div>

    




<div class="article-tags">
  
  <a class="badge badge-light" href="/tag/facial-expression-recognition/">Facial Expression Recognition</a>
  
  <a class="badge badge-light" href="/tag/multi-source-domain-adaptation/">Multi-Source Domain Adaptation</a>
  
</div>



<div class="share-box" aria-hidden="true">
  <ul class="share">
    
      
      
      
        
      
      
      
      <li>
        <a href="https://twitter.com/intent/tweet?url=http://localhost:1313/publication/manual-pr24-chang/&amp;text=Pose-robust%20personalized%20facial%20expression%20recognition%20through%20unsupervised%20multi-source%20domain%20adaptation" target="_blank" rel="noopener" class="share-btn-twitter">
          <i class="fab fa-twitter"></i>
        </a>
      </li>
    
      
      
      
        
      
      
      
      <li>
        <a href="https://www.facebook.com/sharer.php?u=http://localhost:1313/publication/manual-pr24-chang/&amp;t=Pose-robust%20personalized%20facial%20expression%20recognition%20through%20unsupervised%20multi-source%20domain%20adaptation" target="_blank" rel="noopener" class="share-btn-facebook">
          <i class="fab fa-facebook"></i>
        </a>
      </li>
    
      
      
      
        
      
      
      
      <li>
        <a href="mailto:?subject=Pose-robust%20personalized%20facial%20expression%20recognition%20through%20unsupervised%20multi-source%20domain%20adaptation&amp;body=http://localhost:1313/publication/manual-pr24-chang/" target="_blank" rel="noopener" class="share-btn-email">
          <i class="fas fa-envelope"></i>
        </a>
      </li>
    
      
      
      
        
      
      
      
      <li>
        <a href="https://www.linkedin.com/shareArticle?url=http://localhost:1313/publication/manual-pr24-chang/&amp;title=Pose-robust%20personalized%20facial%20expression%20recognition%20through%20unsupervised%20multi-source%20domain%20adaptation" target="_blank" rel="noopener" class="share-btn-linkedin">
          <i class="fab fa-linkedin-in"></i>
        </a>
      </li>
    
      
      
      
        
      
      
      
      <li>
        <a href="whatsapp://send?text=Pose-robust%20personalized%20facial%20expression%20recognition%20through%20unsupervised%20multi-source%20domain%20adaptation%20http://localhost:1313/publication/manual-pr24-chang/" target="_blank" rel="noopener" class="share-btn-whatsapp">
          <i class="fab fa-whatsapp"></i>
        </a>
      </li>
    
      
      
      
        
      
      
      
      <li>
        <a href="https://service.weibo.com/share/share.php?url=http://localhost:1313/publication/manual-pr24-chang/&amp;title=Pose-robust%20personalized%20facial%20expression%20recognition%20through%20unsupervised%20multi-source%20domain%20adaptation" target="_blank" rel="noopener" class="share-btn-weibo">
          <i class="fab fa-weibo"></i>
        </a>
      </li>
    
  </ul>
</div>











  
  
    



  
  
  
  
  
  <div class="media author-card content-widget-hr">
    
      
      <a href="/author/shangfei-wang/"><img class="avatar mr-3 avatar-circle" src="/author/shangfei-wang/avatar_hua0dcf0d7d729937dc57147819587ff23_443707_270x270_fill_q75_lanczos_center.jpg" alt="Shangfei Wang"></a>
    

    <div class="media-body">
      <h5 class="card-title"><a href="/author/shangfei-wang/">Shangfei Wang</a></h5>
      <h6 class="card-subtitle">Professor of Artificial Intelligence</h6>
      <p class="card-text">My research interests include Pattern Recognition, Affective Computing, Probabilistic Graphical Models, Computation Intelligence.</p>
      <ul class="network-icon" aria-hidden="true">
  
    
    
    
      
    
    
    
    
    
    <li>
      <a href="mailto:sfwang@ustc.edu.cn" >
        <i class="fas fa-envelope"></i>
      </a>
    </li>
  
    
    
    
    
    
    
    
      
    
    <li>
      <a href="https://scholar.google.com/citations?user=PmDOUfQAAAAJ" target="_blank" rel="noopener">
        <i class="ai ai-google-scholar"></i>
      </a>
    </li>
  
    
    
    
      
    
    
    
    
    
      
    
    <li>
      <a href="http://staff.ustc.edu.cn/~sfwang" target="_blank" rel="noopener">
        <i class="fas fa-home"></i>
      </a>
    </li>
  
</ul>

    </div>
  </div>


  
    



  
  
  
  
  
  <div class="media author-card content-widget-hr">
    
      
      <a href="/author/yanan-chang/"><img class="avatar mr-3 avatar-circle" src="/author/yanan-chang/avatar_hu7fad79f87447af15b28f362037e1a1a0_82453_270x270_fill_q75_lanczos_center.jpg" alt="Ya&#39;nan Chang"></a>
    

    <div class="media-body">
      <h5 class="card-title"><a href="/author/yanan-chang/">Ya&#39;nan Chang</a></h5>
      
      
      <ul class="network-icon" aria-hidden="true">
  
    
    
    
      
    
    
    
    
    
    <li>
      <a href="mailto:cyn123@mail.ustc.edu.cn" >
        <i class="fas fa-envelope"></i>
      </a>
    </li>
  
</ul>

    </div>
  </div>


  
    



  
  
  
  
  
  <div class="media author-card content-widget-hr">
    
      
      <a href="/author/qiong-li/"><img class="avatar mr-3 avatar-circle" src="/author/qiong-li/avatar_huf57b1dedfce8498c6317a36bc3138c51_8831_270x270_fill_q75_lanczos_center.jpg" alt="Qiong Li"></a>
    

    <div class="media-body">
      <h5 class="card-title"><a href="/author/qiong-li/">Qiong Li</a></h5>
      
      
      <ul class="network-icon" aria-hidden="true">
  
    
    
    
      
    
    
    
    
    
    <li>
      <a href="mailto:liqiong7@mail.ustc.edu.cn" >
        <i class="fas fa-envelope"></i>
      </a>
    </li>
  
</ul>

    </div>
  </div>


  
    



  
  
  
  
  
  <div class="media author-card content-widget-hr">
    
      
      <a href="/author/can-wang/"><img class="avatar mr-3 avatar-circle" src="/author/can-wang/avatar_huf57b1dedfce8498c6317a36bc3138c51_8831_270x270_fill_q75_lanczos_center.jpg" alt="Can Wang"></a>
    

    <div class="media-body">
      <h5 class="card-title"><a href="/author/can-wang/">Can Wang</a></h5>
      <h6 class="card-subtitle">PhD</h6>
      
      <ul class="network-icon" aria-hidden="true">
  
    
    
    
      
    
    
    
    
    
      
    
    <li>
      <a href="https://cassiepython.github.io/" target="_blank" rel="noopener">
        <i class="fas fa-home"></i>
      </a>
    </li>
  
    
    
    
      
    
    
    
    
    
      
    
    <li>
      <a href="https://github.com/cassiePython" target="_blank" rel="noopener">
        <i class="fab fa-github"></i>
      </a>
    </li>
  
</ul>

    </div>
  </div>


  














  
  
  <div class="article-widget content-widget-hr">
    <h3>Related</h3>
    <ul>
      
      <li><a href="/publication/manual-icme23-liang/">Privacy-Protected Facial Expression Recognition Augmented by High-Resolution Facial Images</a></li>
      
    </ul>
  </div>
  





  </div>
</div>
  </div>

  <div class="page-footer">
    
    
    <div class="container">
      <footer class="site-footer">

  



  <div class="powered-by d-flex flex-wrap pb-2 justify-content-center">
    <span id="busuanzi_container_site_pv">Page View:<span id="busuanzi_value_site_pv"></span></span>
  </div>
  <div class="powered-by d-flex flex-wrap pb-2 justify-content-center">
    <span id="busuanzi_container_site_uv">Unique Visitor:<span id="busuanzi_value_site_uv"></span></span>
  </div>
  

  

  

  
  






  <p class="powered-by">
    
    
    
      
      
      
      
      
      
      Published with <a href="https://wowchemy.com/?utm_campaign=poweredby" target="_blank" rel="noopener">Wowchemy</a> — the free, <a href="https://github.com/wowchemy/wowchemy-hugo-modules" target="_blank" rel="noopener">open source</a> website builder that empowers creators.
    
  </p>
</footer>

    </div>
    
  </div>

  
<div id="modal" class="modal fade" role="dialog">
  <div class="modal-dialog">
    <div class="modal-content">
      <div class="modal-header">
        <h5 class="modal-title">Cite</h5>
        <button type="button" class="close" data-dismiss="modal" aria-label="Close">
          <span aria-hidden="true">&times;</span>
        </button>
      </div>
      <div class="modal-body">
        <pre><code class="tex hljs"></code></pre>
      </div>
      <div class="modal-footer">
        <a class="btn btn-outline-primary my-1 js-copy-cite" href="#" target="_blank">
          <i class="fas fa-copy"></i> Copy
        </a>
        <a class="btn btn-outline-primary my-1 js-download-cite" href="#" target="_blank">
          <i class="fas fa-download"></i> Download
        </a>
        <div id="modal-error"></div>
      </div>
    </div>
  </div>
</div>

      

    
    
    
      <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.5.1/jquery.min.js" integrity="sha256-9/aliU8dGd2tb6OSsuzixeV4y/faTqgFtohetphbbj0=" crossorigin="anonymous"></script>
      <script src="https://cdnjs.cloudflare.com/ajax/libs/instant.page/5.1.0/instantpage.min.js" integrity="sha512-1+qUtKoh9XZW7j+6LhRMAyOrgSQKenQ4mluTR+cvxXjP1Z54RxZuzstR/H9kgPXQsVB8IW7DMDFUJpzLjvhGSQ==" crossorigin="anonymous"></script>

      
      
        <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery.imagesloaded/4.1.4/imagesloaded.pkgd.min.js" integrity="sha256-lqvxZrPLtfffUl2G/e7szqSvPBILGbwmsGE1MKlOi0Q=" crossorigin="anonymous"></script>
        <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery.isotope/3.0.6/isotope.pkgd.min.js" integrity="sha256-CBrpuqrMhXwcLLUd5tvQ4euBHCdh7wGlDfNz8vbu/iI=" crossorigin="anonymous"></script>
      

      
      

      

      

    

    
    
    
      <script src="https://cdnjs.cloudflare.com/ajax/libs/leaflet/1.7.1/leaflet.min.js" integrity="sha512-SeiQaaDh73yrb56sTW/RgVdi/mMqNeM2oBwubFHagc5BkixSpP1fvqF47mKzPGWYSSy4RwbBunrJBQ4Co8fRWA==" crossorigin="anonymous"></script>
    

    
    

    
    
    
      
      <script id="search-hit-fuse-template" type="text/x-template">
        <div class="search-hit" id="summary-{{key}}">
          <div class="search-hit-content">
            <div class="search-hit-name">
              <a href="{{relpermalink}}">{{title}}</a>
              <div class="article-metadata search-hit-type">{{type}}</div>
              <p class="search-hit-description">{{snippet}}</p>
            </div>
          </div>
        </div>
      </script>
      
        <script src="https://cdnjs.cloudflare.com/ajax/libs/fuse.js/3.2.1/fuse.min.js" integrity="sha256-VzgmKYmhsGNNN4Ph1kMW+BjoYJM2jV5i4IlFoeZA9XI=" crossorigin="anonymous"></script>
        <script src="https://cdnjs.cloudflare.com/ajax/libs/mark.js/8.11.1/jquery.mark.min.js" integrity="sha256-4HLtjeVgH0eIB3aZ9mLYF6E8oU5chNdjU6p6rrXpl9U=" crossorigin="anonymous"></script>
      
    

    
    

    
    
    
    

    
    <script src="/js/bootstrap.bundle.min.js"></script>

    
    
      
      
      
      
      
      
      
    

    
    
    
    
    
    
    
    
      
      
    
    
    <script src="/en/js/wowchemy.min.js"></script>

    






</body>
</html>
