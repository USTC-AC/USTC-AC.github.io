@inproceedings{10.1145/3581783.3612854,
author = {Liang, Cong and Wang, Jiahe and Zhang, Haofan and Tang, Bing and Huang, Junshan and Wang, Shangfei and Chen, Xiaoping},
title = {UniFaRN: Unified Transformer for Facial Reaction Generation},
year = {2023},
isbn = {9798400701085},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3581783.3612854},
doi = {10.1145/3581783.3612854},
abstract = {We propose the Unified Transformer for Facial Reaction GeneratioN (UniFaRN) framework for facial reaction prediction in dyadic interactions. Given the video and audio of one side, the task is to generate facial reactions of the other side. The challenge of the task lies in the fusion of multi-modal inputs and balancing appropriateness and diversity. We adopt the Transformer architecture to tackle the challenge by leveraging its flexibility of handling multi-modal data and ability to control the generation process. By successfully capturing the correlations between multi-modal inputs and outputs with unified layers and balancing the performance with sampling methods, we have won first place in the REACT2023 challenge.},
booktitle = {Proceedings of the 31st ACM International Conference on Multimedia},
pages = {9506â€“9510},
numpages = {5},
keywords = {dyadic human-agent interaction, acial reaction prediction},
location = {, Ottawa ON, Canada, },
series = {MM '23}
}
