@inproceedings{
author = {Wu, Yi and Li, Jingtian and Wang, Shangfei and Li, Guoming and Mao, Meng and Tan, Linxiang},
title = {Learning Knowledge from Textual Descriptions for 3D Human Pose Estimation},
year = {2025},
address = {Singapore EXPO},
url = {},
doi = {},
abstract = {Mainstream 3D human pose estimation methods directly predict 3D coordinates of joints from 2D keypoints, suffering from severe depth ambiguity. Pose textual descriptions contain abundant semantic information, which facilitates the model to learn the spatial relationship among different body parts, partially alleviating this issue. Leveraging this insight, we propose a 3D human pose estimation method assisted by textual descriptions. Specifically, we utilize an automatic captioning pipeline to generate textual descriptions of 3D poses based on spatial relations among joints. These descriptions include details regarding angles, distances, relative positions, pitch&roll and ground-contacts. Subsequently, text features are extracted from these descriptions using a language model, while a 3D human pose estimation model extracts pose features. Aligning the pose features with the text features allows for a more targeted optimization of the estimation model. Therefore, we systematically introduce three alignment approaches to effectively align features extracted by two models operating in entirely different domains. Our method incorporates prior knowledge derived from the textual descriptions into the estimation model and can be seamlessly applied to various existing framework. Experimental results on the Human3.6M and MPI-INF-3DHP datasets demonstrate that our method surpasses state-of-the-art methods.},
pages = {},
numpages = {16},
keywords = {3D Human Pose Estimation, Pose Textual Descriptions, Cross-Modality Feature Alignment}
}