<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Latest News | USTC-AC</title>
    <link>https://ustc-ac.github.io/post/</link>
      <atom:link href="https://ustc-ac.github.io/post/index.xml" rel="self" type="application/rss+xml" />
    <description>Latest News</description>
    <generator>Wowchemy (https://wowchemy.com)</generator><language>en-us</language><lastBuildDate>Sun, 10 Dec 2023 10:26:05 +0000</lastBuildDate>
    <image>
      <url>https://ustc-ac.github.io/media/icon_huc3da470f1a231b6fd15916059be12d5d_66382_512x512_fill_lanczos_center_3.png</url>
      <title>Latest News</title>
      <link>https://ustc-ac.github.io/post/</link>
    </image>
    
    <item>
      <title>One student from our group was awarded Outstanding Master&#39;s Thesis Award</title>
      <link>https://ustc-ac.github.io/post/award-2023-panjicai/</link>
      <pubDate>Sun, 10 Dec 2023 10:26:05 +0000</pubDate>
      <guid>https://ustc-ac.github.io/post/award-2023-panjicai/</guid>
      <description>&lt;p&gt;&lt;a href=&#34;../author/jicai-pan/&#34;&gt;Jicai Pan&lt;/a&gt; was awarded the 2023 Anhui Computer Society Outstanding Master&amp;rsquo;s Thesis Award.&lt;/p&gt;
&lt;figure  &gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img alt=&#34;&#34; srcset=&#34;
               /post/award-2023-panjicai/featured_hu605b07ccb20d836221f8582ae9e78ad6_539913_328da4cb6276c4b9076d97f68b400998.jpg 400w,
               /post/award-2023-panjicai/featured_hu605b07ccb20d836221f8582ae9e78ad6_539913_1a096c0419713ba7ecdf52793a903956.jpg 760w,
               /post/award-2023-panjicai/featured_hu605b07ccb20d836221f8582ae9e78ad6_539913_1200x1200_fit_q75_lanczos.jpg 1200w&#34;
               src=&#34;https://ustc-ac.github.io/post/award-2023-panjicai/featured_hu605b07ccb20d836221f8582ae9e78ad6_539913_328da4cb6276c4b9076d97f68b400998.jpg&#34;
               width=&#34;760&#34;
               height=&#34;541&#34;
               loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;/figure&gt;</description>
    </item>
    
    <item>
      <title>Incoming Talk in Guangdong Rehabilitation Medicine Clinical Research Centre&#39;s Symposium</title>
      <link>https://ustc-ac.github.io/post/talk-2023-guang/</link>
      <pubDate>Sun, 26 Nov 2023 09:25:52 +0000</pubDate>
      <guid>https://ustc-ac.github.io/post/talk-2023-guang/</guid>
      <description>&lt;p&gt;Pro. &lt;a href=&#34;../author/shangfei-wang/&#34;&gt;Shangfei Wang&lt;/a&gt; will give a talk on &lt;em&gt;Expression Recognition&lt;/em&gt; at the Guangdong Rehabilitation Medicine Clinical Research Centre&amp;rsquo;s Symposium on Clinical Application of Intelligent Rehabilitation Aids.&lt;/p&gt;
&lt;figure  &gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img alt=&#34;&#34; srcset=&#34;
               /post/talk-2023-guang/poster_hu0de3debe6e31f3c007365d6b947566eb_390261_0f34a1bc873d3be4cfd8700dc9561fea.jpg 400w,
               /post/talk-2023-guang/poster_hu0de3debe6e31f3c007365d6b947566eb_390261_2c638426807ab80e2cad4d43fec921d5.jpg 760w,
               /post/talk-2023-guang/poster_hu0de3debe6e31f3c007365d6b947566eb_390261_1200x1200_fit_q75_lanczos.jpg 1200w&#34;
               src=&#34;https://ustc-ac.github.io/post/talk-2023-guang/poster_hu0de3debe6e31f3c007365d6b947566eb_390261_0f34a1bc873d3be4cfd8700dc9561fea.jpg&#34;
               width=&#34;304&#34;
               height=&#34;760&#34;
               loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;/figure&gt;</description>
    </item>
    
    <item>
      <title>One student from our group was awarded Doctoral Dissertation Award</title>
      <link>https://ustc-ac.github.io/post/award-2023-yinshi/</link>
      <pubDate>Sun, 30 Jul 2023 10:26:05 +0000</pubDate>
      <guid>https://ustc-ac.github.io/post/award-2023-yinshi/</guid>
      <description>&lt;p&gt;&lt;a href=&#34;../author/yin-shi/&#34;&gt;Shi Yin&lt;/a&gt; was awarded the 2022 ACM CHINA COUNCIL HEFEI CHAPTER DOCTORAL DISSERTATION AWARD.&lt;/p&gt;
&lt;figure  &gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img alt=&#34;&#34; srcset=&#34;
               /post/award-2023-yinshi/yin_hu0c815f353e4a9fcaa5e2dca36bf51048_358300_ff7d47b138cc390664c2e2d03d35c269.jpg 400w,
               /post/award-2023-yinshi/yin_hu0c815f353e4a9fcaa5e2dca36bf51048_358300_b9b2915d9229279fd7270911cde39218.jpg 760w,
               /post/award-2023-yinshi/yin_hu0c815f353e4a9fcaa5e2dca36bf51048_358300_1200x1200_fit_q75_lanczos.jpg 1200w&#34;
               src=&#34;https://ustc-ac.github.io/post/award-2023-yinshi/yin_hu0c815f353e4a9fcaa5e2dca36bf51048_358300_ff7d47b138cc390664c2e2d03d35c269.jpg&#34;
               width=&#34;760&#34;
               height=&#34;570&#34;
               loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;/figure&gt;</description>
    </item>
    
    <item>
      <title>Three papers are accepted by ACMMM2023</title>
      <link>https://ustc-ac.github.io/post/2023-acmmm-threepapers/</link>
      <pubDate>Tue, 25 Jul 2023 09:25:52 +0000</pubDate>
      <guid>https://ustc-ac.github.io/post/2023-acmmm-threepapers/</guid>
      <description>&lt;p&gt;Three papers by &lt;a href=&#34;https://ustc-ac.github.io/author/yi-wu/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Yi Wu&lt;/a&gt;, &lt;a href=&#34;https://ustc-ac.github.io/author/zhouan-zhu/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Zhouan Zhu&lt;/a&gt;, and &lt;a href=&#34;https://ustc-ac.github.io/author/jicai-pan/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Jicai Pan&lt;/a&gt; are accepted by ACMMM2023&lt;/p&gt;
&lt;figure  &gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img alt=&#34;&#34;
           src=&#34;https://ustc-ac.github.io/post/2023-acmmm-threepapers/featured.gif&#34;
           loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;/figure&gt;</description>
    </item>
    
    <item>
      <title>One paper is accepted by IEEE Transactions on Affective Computing</title>
      <link>https://ustc-ac.github.io/post/2023-tac-wu/</link>
      <pubDate>Sun, 16 Jul 2023 09:25:52 +0000</pubDate>
      <guid>https://ustc-ac.github.io/post/2023-tac-wu/</guid>
      <description>&lt;p&gt;“Pose-Aware Facial Expression Recognition Assisted by Expression Descriptions” by &lt;a href=&#34;../author/yi-wu/&#34;&gt;Yi Wu&lt;/a&gt;, has been accepted for publication in a future issue of IEEE Transactions on Affective Computing journal.&lt;/p&gt;
&lt;figure  &gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img alt=&#34;&#34; srcset=&#34;
               /post/2023-tac-wu/featured_huf05967657e7a7c84beaa1d9b073d1d52_29203_04c13bf0700e73fd96d7b1ec873729f1.jpg 400w,
               /post/2023-tac-wu/featured_huf05967657e7a7c84beaa1d9b073d1d52_29203_f73d324e32df4956c9477c7b3018ecce.jpg 760w,
               /post/2023-tac-wu/featured_huf05967657e7a7c84beaa1d9b073d1d52_29203_1200x1200_fit_q75_lanczos.jpg 1200w&#34;
               src=&#34;https://ustc-ac.github.io/post/2023-tac-wu/featured_huf05967657e7a7c84beaa1d9b073d1d52_29203_04c13bf0700e73fd96d7b1ec873729f1.jpg&#34;
               width=&#34;760&#34;
               height=&#34;158&#34;
               loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;/figure&gt;</description>
    </item>
    
    <item>
      <title>One paper is accepted by ICME2023</title>
      <link>https://ustc-ac.github.io/post/2023-icme-liang/</link>
      <pubDate>Sat, 15 Jul 2023 09:25:52 +0000</pubDate>
      <guid>https://ustc-ac.github.io/post/2023-icme-liang/</guid>
      <description>&lt;p&gt;One paper by &lt;a href=&#34;author/cong-liang/&#34;&gt;Cong Liang&lt;/a&gt; is accepted by ICME23&lt;/p&gt;
&lt;figure  &gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img alt=&#34;&#34; srcset=&#34;
               /post/2023-icme-liang/featured_hu344999ba05594e9a9414be4923011afa_23178_d3bf62b0fc38f92ee34ba45354202377.jpg 400w,
               /post/2023-icme-liang/featured_hu344999ba05594e9a9414be4923011afa_23178_52998d0cbcc1f271cf16ad73f4ee2576.jpg 760w,
               /post/2023-icme-liang/featured_hu344999ba05594e9a9414be4923011afa_23178_1200x1200_fit_q75_lanczos.jpg 1200w&#34;
               src=&#34;https://ustc-ac.github.io/post/2023-icme-liang/featured_hu344999ba05594e9a9414be4923011afa_23178_d3bf62b0fc38f92ee34ba45354202377.jpg&#34;
               width=&#34;760&#34;
               height=&#34;208&#34;
               loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;/figure&gt;</description>
    </item>
    
    <item>
      <title>We have gotten the first price of REACT2023</title>
      <link>https://ustc-ac.github.io/post/2023-acmmm-react/</link>
      <pubDate>Tue, 11 Jul 2023 09:25:52 +0000</pubDate>
      <guid>https://ustc-ac.github.io/post/2023-acmmm-react/</guid>
      <description>&lt;p&gt;&lt;a href=&#34;../author/cong-liang/&#34;&gt;Liang Cong&lt;/a&gt;, &lt;a href=&#34;../author/jiahe-wang/&#34;&gt;Wang Jiahe&lt;/a&gt; and &lt;a href=&#34;../author/haofan-zhang/&#34;&gt;Zhang Haofan&lt;/a&gt; participated in the REACT2023 competition (Challenge@ACM-MM23) and won the first place.&lt;/p&gt;
&lt;figure  &gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img alt=&#34;&#34; srcset=&#34;
               /post/2023-acmmm-react/featured_hu48e19ef73d479585da3e065e943cc6fb_24659_9418675e90a7b823de539412ce62b12e.jpg 400w,
               /post/2023-acmmm-react/featured_hu48e19ef73d479585da3e065e943cc6fb_24659_f893843bee4ac8ecec51ce72de3cad02.jpg 760w,
               /post/2023-acmmm-react/featured_hu48e19ef73d479585da3e065e943cc6fb_24659_1200x1200_fit_q75_lanczos.jpg 1200w&#34;
               src=&#34;https://ustc-ac.github.io/post/2023-acmmm-react/featured_hu48e19ef73d479585da3e065e943cc6fb_24659_9418675e90a7b823de539412ce62b12e.jpg&#34;
               width=&#34;760&#34;
               height=&#34;205&#34;
               loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;/figure&gt;</description>
    </item>
    
    <item>
      <title>One paper is awarded the FG2023 Best Paper</title>
      <link>https://ustc-ac.github.io/post/2023-fg-best/</link>
      <pubDate>Fri, 20 Jan 2023 08:00:00 +0000</pubDate>
      <guid>https://ustc-ac.github.io/post/2023-fg-best/</guid>
      <description>&lt;p&gt;Our work &lt;a href=&#34;../publication/manual-fg23-wang&#34;&gt;&lt;em&gt;Low-Resolution Face Recognition Enhanced by High-Resolution Facial Images&lt;/em&gt;&lt;/a&gt; received the FG2023 Best Paper&lt;/p&gt;
&lt;figure  &gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img alt=&#34;&#34; srcset=&#34;
               /post/2023-fg-best/featured_hua1bf0bdc129d60b4e161755a58ae0725_640903_8109cdff835462a949b630452578997f.jpg 400w,
               /post/2023-fg-best/featured_hua1bf0bdc129d60b4e161755a58ae0725_640903_a51b3fd9bc4645d602082b5062d36fbd.jpg 760w,
               /post/2023-fg-best/featured_hua1bf0bdc129d60b4e161755a58ae0725_640903_1200x1200_fit_q75_lanczos.jpg 1200w&#34;
               src=&#34;https://ustc-ac.github.io/post/2023-fg-best/featured_hua1bf0bdc129d60b4e161755a58ae0725_640903_8109cdff835462a949b630452578997f.jpg&#34;
               width=&#34;760&#34;
               height=&#34;424&#34;
               loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;/figure&gt;</description>
    </item>
    
    <item>
      <title>One paper is accepted by ACCV2022</title>
      <link>https://ustc-ac.github.io/post/2022-accv-wang/</link>
      <pubDate>Sun, 02 Oct 2022 08:00:00 +0000</pubDate>
      <guid>https://ustc-ac.github.io/post/2022-accv-wang/</guid>
      <description>&lt;p&gt;One paper by &lt;a href=&#34;author/jiahe-wang/&#34;&gt;Jiahe Wang&lt;/a&gt; and &lt;a href=&#34;author/heyan-ding/&#34;&gt;Heyan Ding&lt;/a&gt; is accepted by ACCV22&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>Two papers are accepted by FG2023</title>
      <link>https://ustc-ac.github.io/post/2023-fg-two/</link>
      <pubDate>Sun, 11 Sep 2022 09:25:52 +0000</pubDate>
      <guid>https://ustc-ac.github.io/post/2023-fg-two/</guid>
      <description>&lt;p&gt;Two papers by &lt;a href=&#34;../author/lin-fang/&#34;&gt;Lin Fang&lt;/a&gt; and &lt;a href=&#34;../author/haihan-wang&#34;&gt;Haihan Wang&lt;/a&gt; are accepted FG 2023&lt;/p&gt;
&lt;figure  &gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img alt=&#34;&#34; srcset=&#34;
               /post/2023-fg-two/featured_hu1ada13ffe5d3ff56922f72f76dc9c526_217126_d12a27a38e19347f6e3b1010f484882c.jpg 400w,
               /post/2023-fg-two/featured_hu1ada13ffe5d3ff56922f72f76dc9c526_217126_dbcb35b5e07f68d9da19c85af88c3b74.jpg 760w,
               /post/2023-fg-two/featured_hu1ada13ffe5d3ff56922f72f76dc9c526_217126_1200x1200_fit_q75_lanczos.jpg 1200w&#34;
               src=&#34;https://ustc-ac.github.io/post/2023-fg-two/featured_hu1ada13ffe5d3ff56922f72f76dc9c526_217126_d12a27a38e19347f6e3b1010f484882c.jpg&#34;
               width=&#34;760&#34;
               height=&#34;371&#34;
               loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;/figure&gt;</description>
    </item>
    
    <item>
      <title>One paper is awarded the ICPR2022 Track 4 Best Scientific Paper</title>
      <link>https://ustc-ac.github.io/post/2022-icpr-best/</link>
      <pubDate>Sun, 28 Aug 2022 08:00:00 +0000</pubDate>
      <guid>https://ustc-ac.github.io/post/2022-icpr-best/</guid>
      <description>&lt;p&gt;Our work &lt;a href=&#34;../publication/manual-icpr22-fang&#34;&gt;&lt;em&gt;Adversarial Stacking Ensemble for Facial Landmark Tracking&lt;/em&gt;&lt;/a&gt; received the ICPR2022 Track 4 Best Scientific Paper&lt;/p&gt;
&lt;figure  &gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img alt=&#34;&#34; srcset=&#34;
               /post/2022-icpr-best/featured_huc064e49a8002d6cf3cc94672b85257f8_167250_104d94e5d5fd0f48b6aa6dc7ae03e669.jpg 400w,
               /post/2022-icpr-best/featured_huc064e49a8002d6cf3cc94672b85257f8_167250_0f471b9602af2ac510ec8496e2b93a51.jpg 760w,
               /post/2022-icpr-best/featured_huc064e49a8002d6cf3cc94672b85257f8_167250_1200x1200_fit_q75_lanczos.jpg 1200w&#34;
               src=&#34;https://ustc-ac.github.io/post/2022-icpr-best/featured_huc064e49a8002d6cf3cc94672b85257f8_167250_104d94e5d5fd0f48b6aa6dc7ae03e669.jpg&#34;
               width=&#34;760&#34;
               height=&#34;576&#34;
               loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;/figure&gt;</description>
    </item>
    
    <item>
      <title>Incoming Talk in VALSE2022</title>
      <link>https://ustc-ac.github.io/post/talk-2022-valse/</link>
      <pubDate>Sat, 20 Aug 2022 09:25:52 +0000</pubDate>
      <guid>https://ustc-ac.github.io/post/talk-2022-valse/</guid>
      <description>&lt;p&gt;Pro. Shangfei Wang will deliver a talk name &lt;em&gt;Video emotional content analysis&lt;/em&gt; in VALSE2022&lt;/p&gt;
&lt;figure  &gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img alt=&#34;&#34; srcset=&#34;
               /post/talk-2022-valse/poster_hu7ceaea2b9cebfd0738f846ec211fc9b8_206066_261870414c99a2da2de5f0632c9ad473.jpg 400w,
               /post/talk-2022-valse/poster_hu7ceaea2b9cebfd0738f846ec211fc9b8_206066_a714fe0bbd4e0942775a6daea18159d7.jpg 760w,
               /post/talk-2022-valse/poster_hu7ceaea2b9cebfd0738f846ec211fc9b8_206066_1200x1200_fit_q75_lanczos.jpg 1200w&#34;
               src=&#34;https://ustc-ac.github.io/post/talk-2022-valse/poster_hu7ceaea2b9cebfd0738f846ec211fc9b8_206066_261870414c99a2da2de5f0632c9ad473.jpg&#34;
               width=&#34;760&#34;
               height=&#34;231&#34;
               loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;/figure&gt;</description>
    </item>
    
    <item>
      <title>Three papers are accepted by ACMMM2022</title>
      <link>https://ustc-ac.github.io/post/2022-mm-three/</link>
      <pubDate>Sun, 10 Jul 2022 08:00:00 +0000</pubDate>
      <guid>https://ustc-ac.github.io/post/2022-mm-three/</guid>
      <description>&lt;p&gt;Three papers by &lt;a href=&#34;https://ustc-ac.github.io/author/haihan-wang/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Wang&lt;/a&gt;, &lt;a href=&#34;https://ustc-ac.github.io/author/xiangyu-miao/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Miao&lt;/a&gt;, and &lt;a href=&#34;https://ustc-ac.github.io/author/jicai-pan/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Pan&lt;/a&gt; are accepted by ACMMM2022&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>One paper is accepted by ICPR2022</title>
      <link>https://ustc-ac.github.io/post/2022-icpr-fang/</link>
      <pubDate>Sat, 02 Apr 2022 08:00:00 +0000</pubDate>
      <guid>https://ustc-ac.github.io/post/2022-icpr-fang/</guid>
      <description>&lt;p&gt;Our work &lt;em&gt;Adversarial Stacking Ensemble for Facial Landmark Tracking&lt;/em&gt; (ID 1281) by &lt;a href=&#34;https://ustc-ac.github.io/author/shi-yin/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Yin&lt;/a&gt; and &lt;a href=&#34;https://ustc-ac.github.io/author/lin-fang/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Fang&lt;/a&gt; is accepted by ICPR22&lt;/p&gt;
&lt;figure  &gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img alt=&#34;&#34; srcset=&#34;
               /post/2022-icpr-fang/accepted_hu5ed295fde70f1be183bce161e169ce61_37359_8c65777bf9b29f7d70bfbd9d2c665d86.jpg 400w,
               /post/2022-icpr-fang/accepted_hu5ed295fde70f1be183bce161e169ce61_37359_e03ad01044d7f4533bcbec8670152207.jpg 760w,
               /post/2022-icpr-fang/accepted_hu5ed295fde70f1be183bce161e169ce61_37359_1200x1200_fit_q75_lanczos.jpg 1200w&#34;
               src=&#34;https://ustc-ac.github.io/post/2022-icpr-fang/accepted_hu5ed295fde70f1be183bce161e169ce61_37359_8c65777bf9b29f7d70bfbd9d2c665d86.jpg&#34;
               width=&#34;548&#34;
               height=&#34;333&#34;
               loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;/figure&gt;</description>
    </item>
    
    <item>
      <title>One of our work is accepted by CVPR2022 !</title>
      <link>https://ustc-ac.github.io/post/2022-cvpr-chang/</link>
      <pubDate>Thu, 03 Mar 2022 08:00:00 +0000</pubDate>
      <guid>https://ustc-ac.github.io/post/2022-cvpr-chang/</guid>
      <description>&lt;p&gt;Our work &lt;em&gt;Knowledge-Driven Self-Supervised Representation Learning for Facial Action Unit Recognition&lt;/em&gt; (ID 6728) by &lt;a href=&#34;https://ustc-ac.github.io/author/yanan-chang/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Chang&lt;/a&gt; is accepted by CVPR2022!&lt;/p&gt;
&lt;figure  &gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img alt=&#34;&#34; srcset=&#34;
               /post/2022-cvpr-chang/accepted_hu79d0e1e93d1cec17731ffea58c3d7948_21562_ec14882abbfde4244df964826fb9923d.jpg 400w,
               /post/2022-cvpr-chang/accepted_hu79d0e1e93d1cec17731ffea58c3d7948_21562_3bb272a26a77f668f40f73b8d4ad9c47.jpg 760w,
               /post/2022-cvpr-chang/accepted_hu79d0e1e93d1cec17731ffea58c3d7948_21562_1200x1200_fit_q75_lanczos.jpg 1200w&#34;
               src=&#34;https://ustc-ac.github.io/post/2022-cvpr-chang/accepted_hu79d0e1e93d1cec17731ffea58c3d7948_21562_ec14882abbfde4244df964826fb9923d.jpg&#34;
               width=&#34;760&#34;
               height=&#34;48&#34;
               loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;/figure&gt;</description>
    </item>
    
    <item>
      <title>Incoming Talk</title>
      <link>https://ustc-ac.github.io/post/talk-2022-ha/</link>
      <pubDate>Sat, 01 Jan 2022 09:25:52 +0000</pubDate>
      <guid>https://ustc-ac.github.io/post/talk-2022-ha/</guid>
      <description>&lt;p&gt;Pro. Shangfei Wang will deliver a talk name &lt;em&gt;Knowing Faces and Hearts-Emotional Human-Computer Interaction Research&lt;/em&gt; in Harbin Institute of Technology Wuhu Research Institute Conference on Artificial Intelligence and Machine Vision (Online)&lt;/p&gt;
&lt;figure  &gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img alt=&#34;&#34; srcset=&#34;
               /post/talk-2022-ha/poster_hu8d53b52848da6b5cee2b33e5b24ed64c_397749_9409ee8e8c9f6b478037b9d7beddf0c8.jpg 400w,
               /post/talk-2022-ha/poster_hu8d53b52848da6b5cee2b33e5b24ed64c_397749_c3526ae1aacd64cb8e1d1b28e20ad59b.jpg 760w,
               /post/talk-2022-ha/poster_hu8d53b52848da6b5cee2b33e5b24ed64c_397749_1200x1200_fit_q75_lanczos.jpg 1200w&#34;
               src=&#34;https://ustc-ac.github.io/post/talk-2022-ha/poster_hu8d53b52848da6b5cee2b33e5b24ed64c_397749_9409ee8e8c9f6b478037b9d7beddf0c8.jpg&#34;
               width=&#34;182&#34;
               height=&#34;760&#34;
               loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;/figure&gt;</description>
    </item>
    
    <item>
      <title>One paper is accepted by FG2021</title>
      <link>https://ustc-ac.github.io/post/2021-fg-liang/</link>
      <pubDate>Sun, 05 Dec 2021 09:25:52 +0000</pubDate>
      <guid>https://ustc-ac.github.io/post/2021-fg-liang/</guid>
      <description>&lt;p&gt;&amp;ldquo;Pose-lnvariant Facial Expression Recognition&amp;rdquo; by Guang Liang has been accepted FG 2021&lt;/p&gt;
&lt;figure  &gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img alt=&#34;&#34; srcset=&#34;
               /post/2021-fg-liang/featured_hu5438825b9b6d1014226d20d231e650c2_1500562_0da4fd5137681798f432a9f075ec136d.jpg 400w,
               /post/2021-fg-liang/featured_hu5438825b9b6d1014226d20d231e650c2_1500562_4da344dce8cf5215b2e6fcdf4fe21595.jpg 760w,
               /post/2021-fg-liang/featured_hu5438825b9b6d1014226d20d231e650c2_1500562_1200x1200_fit_q75_lanczos.jpg 1200w&#34;
               src=&#34;https://ustc-ac.github.io/post/2021-fg-liang/featured_hu5438825b9b6d1014226d20d231e650c2_1500562_0da4fd5137681798f432a9f075ec136d.jpg&#34;
               width=&#34;760&#34;
               height=&#34;428&#34;
               loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;/figure&gt;</description>
    </item>
    
    <item>
      <title>One paper is accepted by IEEE Transactions on Affective Computing</title>
      <link>https://ustc-ac.github.io/post/2021-journal-chang/</link>
      <pubDate>Fri, 24 Sep 2021 08:00:00 +0000</pubDate>
      <guid>https://ustc-ac.github.io/post/2021-journal-chang/</guid>
      <description>&lt;p&gt;&amp;ldquo;Dual Learning for Joint Facial Landmark Detection and Action Unit Recognition&amp;rdquo; by Ya&amp;rsquo;nan Chang, has been accepted for publication in a future issue of &lt;em&gt;IEEE Transactions on Affective Computing&lt;/em&gt; journal. You can find it in &lt;em&gt;&lt;strong&gt;Publications&lt;/strong&gt;&lt;/em&gt; column and &lt;a href=&#34;https://ieeexplore.ieee.org/document/9543559&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Link&lt;/a&gt;&lt;/p&gt;
&lt;figure  &gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img alt=&#34;&#34; srcset=&#34;
               /post/2021-journal-chang/featured_hu02b94926cf1a462548b33acc6dc3b6c0_19043_02579d245c23656688c7338102ddcb9a.jpg 400w,
               /post/2021-journal-chang/featured_hu02b94926cf1a462548b33acc6dc3b6c0_19043_c17c28bccb54fab710b1c2645fb64ded.jpg 760w,
               /post/2021-journal-chang/featured_hu02b94926cf1a462548b33acc6dc3b6c0_19043_1200x1200_fit_q75_lanczos.jpg 1200w&#34;
               src=&#34;https://ustc-ac.github.io/post/2021-journal-chang/featured_hu02b94926cf1a462548b33acc6dc3b6c0_19043_02579d245c23656688c7338102ddcb9a.jpg&#34;
               width=&#34;760&#34;
               height=&#34;227&#34;
               loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;/figure&gt;</description>
    </item>
    
    <item>
      <title>Incoming Talk in MIPR2021</title>
      <link>https://ustc-ac.github.io/post/talk-2022-mipr/</link>
      <pubDate>Mon, 30 Aug 2021 09:25:52 +0000</pubDate>
      <guid>https://ustc-ac.github.io/post/talk-2022-mipr/</guid>
      <description>&lt;p&gt;Pro. Shangfei Wang will deliver a talk name &lt;em&gt;Facial Action Unit Recognition under Non-Full Annotation&lt;/em&gt; in &lt;a href=&#34;https://aiart2021.github.io/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;MIPR2021&lt;/a&gt;&lt;/p&gt;
&lt;figure  &gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img alt=&#34;&#34; srcset=&#34;
               /post/talk-2022-mipr/poster_hub2fbd4f15906ae5c9d91adda5b4611a7_206066_f8b986d45e1ed4a8a7ea26b86d7faf95.jpg 400w,
               /post/talk-2022-mipr/poster_hub2fbd4f15906ae5c9d91adda5b4611a7_206066_b562555b51dcf1507f7fa9a63810008c.jpg 760w,
               /post/talk-2022-mipr/poster_hub2fbd4f15906ae5c9d91adda5b4611a7_206066_1200x1200_fit_q75_lanczos.jpg 1200w&#34;
               src=&#34;https://ustc-ac.github.io/post/talk-2022-mipr/poster_hub2fbd4f15906ae5c9d91adda5b4611a7_206066_f8b986d45e1ed4a8a7ea26b86d7faf95.jpg&#34;
               width=&#34;760&#34;
               height=&#34;340&#34;
               loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;/figure&gt;</description>
    </item>
    
    <item>
      <title>One paper is accepted by IJCAI 2021</title>
      <link>https://ustc-ac.github.io/post/2021-ijcai-bin-xia/</link>
      <pubDate>Tue, 29 Jun 2021 08:57:57 +0000</pubDate>
      <guid>https://ustc-ac.github.io/post/2021-ijcai-bin-xia/</guid>
      <description>&lt;p&gt;Bin Xia&amp;rsquo;s paper “Micro-Expression Recognition Enhanced by Macro-Expression from Spatial-Temporal Domain” is accepted by the 2021 Conference on IJCAI.&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;https://ijcai-21.org/program-main-track/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Link&lt;/a&gt;&lt;/p&gt;
&lt;figure  &gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img alt=&#34;&#34; srcset=&#34;
               /post/2021-ijcai-bin-xia/featured_hu2c69917c91855ca124047f9f7218696c_193835_029ea1dd9d8d3922e8f3fb1957ac27d3.jpg 400w,
               /post/2021-ijcai-bin-xia/featured_hu2c69917c91855ca124047f9f7218696c_193835_758b1e297afccb5136b6560d166bd154.jpg 760w,
               /post/2021-ijcai-bin-xia/featured_hu2c69917c91855ca124047f9f7218696c_193835_1200x1200_fit_q75_lanczos.jpg 1200w&#34;
               src=&#34;https://ustc-ac.github.io/post/2021-ijcai-bin-xia/featured_hu2c69917c91855ca124047f9f7218696c_193835_029ea1dd9d8d3922e8f3fb1957ac27d3.jpg&#34;
               width=&#34;760&#34;
               height=&#34;365&#34;
               loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;/figure&gt;</description>
    </item>
    
    <item>
      <title>Three students from our group were awarded outstanding graduates</title>
      <link>https://ustc-ac.github.io/post/outstanding-graduates/</link>
      <pubDate>Sun, 20 Jun 2021 10:26:05 +0000</pubDate>
      <guid>https://ustc-ac.github.io/post/outstanding-graduates/</guid>
      <description>&lt;p&gt;They accepted the award at the graduation ceremony.&lt;/p&gt;
&lt;p&gt;













&lt;figure  id=&#34;figure-fig1-shi-yin-first-from-the-left&#34;&gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img alt=&#34;Fig1. Shi Yin (first from the left)&#34; srcset=&#34;
               /post/outstanding-graduates/yin_hubac859aa24d48443df50ef54c1c835f4_1229321_148e7ad4a7e1b84c00e4a6416e588a8c.jpg 400w,
               /post/outstanding-graduates/yin_hubac859aa24d48443df50ef54c1c835f4_1229321_9d31d81aa5afbb60fb46f935d7251e54.jpg 760w,
               /post/outstanding-graduates/yin_hubac859aa24d48443df50ef54c1c835f4_1229321_1200x1200_fit_q75_lanczos.jpg 1200w&#34;
               src=&#34;https://ustc-ac.github.io/post/outstanding-graduates/yin_hubac859aa24d48443df50ef54c1c835f4_1229321_148e7ad4a7e1b84c00e4a6416e588a8c.jpg&#34;
               width=&#34;760&#34;
               height=&#34;507&#34;
               loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;figcaption&gt;
      Fig1. Shi Yin (first from the left)
    &lt;/figcaption&gt;&lt;/figure&gt;














&lt;figure  id=&#34;figure-fig2-zhiwei-xu-first-from-the-rightt&#34;&gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img alt=&#34;Fig2. Zhiwei Xu (first from the rightt)&#34; srcset=&#34;
               /post/outstanding-graduates/xu_hu48fff2b7152bbc7bbed649388a630b60_1199446_01152d543151c11640d4b8bffc64257a.jpg 400w,
               /post/outstanding-graduates/xu_hu48fff2b7152bbc7bbed649388a630b60_1199446_2a1df60b248b6a1377d245d4edec1328.jpg 760w,
               /post/outstanding-graduates/xu_hu48fff2b7152bbc7bbed649388a630b60_1199446_1200x1200_fit_q75_lanczos.jpg 1200w&#34;
               src=&#34;https://ustc-ac.github.io/post/outstanding-graduates/xu_hu48fff2b7152bbc7bbed649388a630b60_1199446_01152d543151c11640d4b8bffc64257a.jpg&#34;
               width=&#34;760&#34;
               height=&#34;507&#34;
               loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;figcaption&gt;
      Fig2. Zhiwei Xu (first from the rightt)
    &lt;/figcaption&gt;&lt;/figure&gt;














&lt;figure  id=&#34;figure-fig3-bin-xia-second-from-the-left&#34;&gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img alt=&#34;Fig3. Bin Xia (second from the left)&#34; srcset=&#34;
               /post/outstanding-graduates/xia_hu886e5b37725fd74635920f35135d58ed_1177558_b33e8928cfdedf595ec41b47290ec777.jpg 400w,
               /post/outstanding-graduates/xia_hu886e5b37725fd74635920f35135d58ed_1177558_3976a911b0eae2ce9bd7f0d1a4f1c4fb.jpg 760w,
               /post/outstanding-graduates/xia_hu886e5b37725fd74635920f35135d58ed_1177558_1200x1200_fit_q75_lanczos.jpg 1200w&#34;
               src=&#34;https://ustc-ac.github.io/post/outstanding-graduates/xia_hu886e5b37725fd74635920f35135d58ed_1177558_b33e8928cfdedf595ec41b47290ec777.jpg&#34;
               width=&#34;760&#34;
               height=&#34;507&#34;
               loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;figcaption&gt;
      Fig3. Bin Xia (second from the left)
    &lt;/figcaption&gt;&lt;/figure&gt;&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>Our facial expression recognition job is filmed in a CCTV-9 documentary</title>
      <link>https://ustc-ac.github.io/post/our-facial-expression-recognition-job-is-filmed-in-a-cctv-9-documentary/</link>
      <pubDate>Fri, 01 Nov 2019 09:25:52 +0000</pubDate>
      <guid>https://ustc-ac.github.io/post/our-facial-expression-recognition-job-is-filmed-in-a-cctv-9-documentary/</guid>
      <description>&lt;p&gt;A CCTV-9 documentary called &amp;ldquo;The Power of Science&amp;rdquo;, which filmed the facial expression recognition function of our robot Jiajia is online now!&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;http://tv.cctv.com/2019/11/04/VIDEtJcLKelpZLWUd2TrHZIZ191104.shtml?spm=C55924871139.PKgX4CXWWE68.0.0&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Link&lt;/a&gt; (17:44)&lt;/p&gt;
&lt;p&gt;On the 70th anniversary of the founding of New China, in order to showcase the achievements of China&amp;rsquo;s scientific and technological development and tell the story of Chinese science, the Chinese Academy of Sciences and China Central Television have cooperated in-depth to create a series of documentary &amp;ldquo;The Power of Science&amp;rdquo;. In the seventh episode, &amp;ldquo;The Biggest Temptation of the Intelligent Era-Bionic Robots&amp;rdquo;, a story of Jiajia is told, a service robot developed by the University of Science and Technology.&lt;/p&gt;
&lt;p&gt;















&lt;figure  &gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img alt=&#34;featured&#34; srcset=&#34;
               /post/our-facial-expression-recognition-job-is-filmed-in-a-cctv-9-documentary/featured_hud42958405cab0abbd778f69f7e22b2bd_119843_f28d4bac54ca9cae83a31f2a830d1483.jpg 400w,
               /post/our-facial-expression-recognition-job-is-filmed-in-a-cctv-9-documentary/featured_hud42958405cab0abbd778f69f7e22b2bd_119843_eabe97cccb517d3efdd8c01303f391b1.jpg 760w,
               /post/our-facial-expression-recognition-job-is-filmed-in-a-cctv-9-documentary/featured_hud42958405cab0abbd778f69f7e22b2bd_119843_1200x1200_fit_q75_lanczos.jpg 1200w&#34;
               src=&#34;https://ustc-ac.github.io/post/our-facial-expression-recognition-job-is-filmed-in-a-cctv-9-documentary/featured_hud42958405cab0abbd778f69f7e22b2bd_119843_f28d4bac54ca9cae83a31f2a830d1483.jpg&#34;
               width=&#34;760&#34;
               height=&#34;428&#34;
               loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;/figure&gt;
&lt;/p&gt;
&lt;h2 id=&#34;more-------------&#34;&gt;More &amp;mdash;&amp;mdash;&amp;mdash;&amp;mdash;&lt;/h2&gt;
&lt;h4 id=&#34;robot-kejia&#34;&gt;Robot KeJia&lt;/h4&gt;
&lt;p&gt;Able to respond to facial expressions with her own micro-level movements during conversations in both Chinese and English, Jiajia is the result of integrating technologies for cognitive modelling, semantic understanding, automated reasoning and planning, knowledge acquisition, kinematics and cloud robotics. These technologies have been studied in the Kejia Robotics project by Chen Xiaoping’s laboratory at USTC. The award-winning development team has received many international accolades, including first places at the popular RoboCup world championship, and the IJCAI Robotics competition.&lt;/p&gt;
&lt;p&gt;















&lt;figure  &gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img alt=&#34;kejia&#34; srcset=&#34;
               /post/our-facial-expression-recognition-job-is-filmed-in-a-cctv-9-documentary/kejia_huc644d72809a34ad6e17e51e82fce8e17_133855_350a41798f23d2ebf6618ee94fb2259b.jpg 400w,
               /post/our-facial-expression-recognition-job-is-filmed-in-a-cctv-9-documentary/kejia_huc644d72809a34ad6e17e51e82fce8e17_133855_d25d82f42118cbb02d496142bd0f1b3e.jpg 760w,
               /post/our-facial-expression-recognition-job-is-filmed-in-a-cctv-9-documentary/kejia_huc644d72809a34ad6e17e51e82fce8e17_133855_1200x1200_fit_q75_lanczos.jpg 1200w&#34;
               src=&#34;https://ustc-ac.github.io/post/our-facial-expression-recognition-job-is-filmed-in-a-cctv-9-documentary/kejia_huc644d72809a34ad6e17e51e82fce8e17_133855_350a41798f23d2ebf6618ee94fb2259b.jpg&#34;
               width=&#34;760&#34;
               height=&#34;508&#34;
               loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;/figure&gt;
&lt;/p&gt;
&lt;h4 id=&#34;reported-by-nature2016&#34;&gt;Reported by Nature(2016)&lt;/h4&gt;
&lt;p&gt;Jiajia was reported by Nature during the 60th anniversary of the University of Science and Technology of China&lt;/p&gt;
&lt;p&gt;















&lt;figure  &gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img alt=&#34;nature&#34; srcset=&#34;
               /post/our-facial-expression-recognition-job-is-filmed-in-a-cctv-9-documentary/nature_hu78fee13afca7fa82f62c2e75c78da747_77253_9b0b338214ebfcb9b3122f0e104e8aad.jpg 400w,
               /post/our-facial-expression-recognition-job-is-filmed-in-a-cctv-9-documentary/nature_hu78fee13afca7fa82f62c2e75c78da747_77253_433eddcea31eac45c7388d202c34aaa6.jpg 760w,
               /post/our-facial-expression-recognition-job-is-filmed-in-a-cctv-9-documentary/nature_hu78fee13afca7fa82f62c2e75c78da747_77253_1200x1200_fit_q75_lanczos.jpg 1200w&#34;
               src=&#34;https://ustc-ac.github.io/post/our-facial-expression-recognition-job-is-filmed-in-a-cctv-9-documentary/nature_hu78fee13afca7fa82f62c2e75c78da747_77253_9b0b338214ebfcb9b3122f0e104e8aad.jpg&#34;
               width=&#34;760&#34;
               height=&#34;489&#34;
               loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;/figure&gt;
&lt;/p&gt;
&lt;h4 id=&#34;praised-by-the-minister-of-foreign-affairs2017&#34;&gt;Praised by the Minister of Foreign Affairs(2017)&lt;/h4&gt;
&lt;p&gt;On the afternoon of April 11, the Anhui Global Promotion Event of the Ministry of Foreign Affairs with the theme of &amp;ldquo;An Open China: Splendid Anhui Welcomes the World&amp;rdquo; was held in the Blue Hall of the Ministry of Foreign Affairs. At 16:20 in the afternoon, Foreign Minister Yi Wang and the invited envoys to China, accompanied by Secretary of the Provincial Party Committee Jinbin Li and Governor Guoying Li, walked into the comprehensive exhibition area. Jiajia, taking the lead in the  exhibition area, was praised by Minister Wang Yi.&lt;/p&gt;
&lt;p&gt;















&lt;figure  &gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img alt=&#34;minister&#34; srcset=&#34;
               /post/our-facial-expression-recognition-job-is-filmed-in-a-cctv-9-documentary/minister_hu14b22c93fc41905bddf176c6e7d8a93e_52235_fe5980a90ecef727d9dc611314e3fb0d.jpg 400w,
               /post/our-facial-expression-recognition-job-is-filmed-in-a-cctv-9-documentary/minister_hu14b22c93fc41905bddf176c6e7d8a93e_52235_610c58d3e327c2159ee896405bc84c2d.jpg 760w,
               /post/our-facial-expression-recognition-job-is-filmed-in-a-cctv-9-documentary/minister_hu14b22c93fc41905bddf176c6e7d8a93e_52235_1200x1200_fit_q75_lanczos.jpg 1200w&#34;
               src=&#34;https://ustc-ac.github.io/post/our-facial-expression-recognition-job-is-filmed-in-a-cctv-9-documentary/minister_hu14b22c93fc41905bddf176c6e7d8a93e_52235_fe5980a90ecef727d9dc611314e3fb0d.jpg&#34;
               width=&#34;554&#34;
               height=&#34;369&#34;
               loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;/figure&gt;
&lt;/p&gt;
&lt;h4 id=&#34;cameron-and-jia-jia2017&#34;&gt;Cameron and Jia Jia(2017)&lt;/h4&gt;
&lt;p&gt;At the opening of the 17th United Bank of Switzerland (UBS) Greater China Conference, held in Shanghai, Jiajia had a spectacular conversation with former British Prime Minister Cameron.&lt;/p&gt;
&lt;p&gt;















&lt;figure  &gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img alt=&#34;cameron&#34; srcset=&#34;
               /post/our-facial-expression-recognition-job-is-filmed-in-a-cctv-9-documentary/cameron_hu69778a0b52edfba6cb4adb4f245ac760_167699_9ed4fb74fef830520db002ebb34fe552.jpg 400w,
               /post/our-facial-expression-recognition-job-is-filmed-in-a-cctv-9-documentary/cameron_hu69778a0b52edfba6cb4adb4f245ac760_167699_975bf43d91400451678f74c2c680ab52.jpg 760w,
               /post/our-facial-expression-recognition-job-is-filmed-in-a-cctv-9-documentary/cameron_hu69778a0b52edfba6cb4adb4f245ac760_167699_1200x1200_fit_q75_lanczos.jpg 1200w&#34;
               src=&#34;https://ustc-ac.github.io/post/our-facial-expression-recognition-job-is-filmed-in-a-cctv-9-documentary/cameron_hu69778a0b52edfba6cb4adb4f245ac760_167699_9ed4fb74fef830520db002ebb34fe552.jpg&#34;
               width=&#34;640&#34;
               height=&#34;427&#34;
               loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;/figure&gt;
&lt;/p&gt;</description>
    </item>
    
  </channel>
</rss>
