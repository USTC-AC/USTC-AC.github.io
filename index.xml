<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>USTC-AC</title>
    <link>https://ustc-ac.github.io/</link>
      <atom:link href="https://ustc-ac.github.io/index.xml" rel="self" type="application/rss+xml" />
    <description>USTC-AC</description>
    <generator>Wowchemy (https://wowchemy.com)</generator><language>en-us</language><lastBuildDate>Mon, 04 Dec 2023 09:53:00 +0000</lastBuildDate>
    <image>
      <url>https://ustc-ac.github.io/media/icon_huc3da470f1a231b6fd15916059be12d5d_66382_512x512_fill_lanczos_center_3.png</url>
      <title>USTC-AC</title>
      <link>https://ustc-ac.github.io/</link>
    </image>
    
    <item>
      <title>A Multi-Modal Hierarchical Recurrent Neural Network for Depression Detection</title>
      <link>https://ustc-ac.github.io/publication/dblp-confmm-yin-ldw-19/</link>
      <pubDate>Mon, 21 Oct 2019 00:00:00 +0000</pubDate>
      <guid>https://ustc-ac.github.io/publication/dblp-confmm-yin-ldw-19/</guid>
      <description>&lt;p&gt;Depression has a severe effect on people’s life. The artificial therapy of depression is facing a shortage of expert therapists. The automatic detection of depression can be an auxiliary means of artificial therapy. As a delicate mental symptom, depression cannot be accurately distinguished via single modal observation. To address this, our work utilizes vision, audio and text features. For vision features, gaze direction, 3D position, the orientation of the head and 17 facial action units are considered. For audio, the hidden layers of pre-trained deep models are used. For text, we build features from two aspects. The first one is the semantic embedding of the whole sentence. The second one is the emotional distribution of several words with obvious emotional tendencies. A subject engaging in a multi-turns conversation may produce several video clips sharing a similar theme. Facing the hierarchical characteristic of such data, we design a framework consisting of two hierarchies of bidirectional long short term memories (LSTM) for the depression detection task. The first hierarchy of bidirectional LSTM extracts vision and audio features for every video clip. The second hierarchy of bidirectional LSTM fuses the visual, audio and textual features and regresses the degree of depression. The indicator in concern in the DDS challenge is the PHQ-8 Score, while the proposed method jointly learns the PTSD Severity to facilitate the prediction of the PHQ-8 Score under a multi-task learning schema.&lt;/p&gt;














&lt;figure  id=&#34;figure-the-framework-of-the-proposed-hierarchical-recurrent-model-the-first-hierarchy-of-bi-lstm-fuses-multi-modal-features-of-a-single-video-clip-and-the-second-hierarchy-fuses-all-clips-of-a-conversation-to-predict-the-result&#34;&gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img alt=&#34;The framework of the proposed hierarchical recurrent model. The first hierarchy of Bi-LSTM fuses multi-modal features of a single video clip. And the second hierarchy fuses all clips of a conversation to predict the result.&#34; srcset=&#34;
               /publication/dblp-confmm-yin-ldw-19/featured_hu7dee86ac700df39881feb7af9e18242c_138487_aee75ebbef059b6f84c61be574151c85.png 400w,
               /publication/dblp-confmm-yin-ldw-19/featured_hu7dee86ac700df39881feb7af9e18242c_138487_5630924c619272899f07b90643de324d.png 760w,
               /publication/dblp-confmm-yin-ldw-19/featured_hu7dee86ac700df39881feb7af9e18242c_138487_1200x1200_fit_lanczos_3.png 1200w&#34;
               src=&#34;https://ustc-ac.github.io/publication/dblp-confmm-yin-ldw-19/featured_hu7dee86ac700df39881feb7af9e18242c_138487_aee75ebbef059b6f84c61be574151c85.png&#34;
               width=&#34;760&#34;
               height=&#34;467&#34;
               loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;figcaption&gt;
      The framework of the proposed hierarchical recurrent model. The first hierarchy of Bi-LSTM fuses multi-modal features of a single video clip. And the second hierarchy fuses all clips of a conversation to predict the result.
    &lt;/figcaption&gt;&lt;/figure&gt;
&lt;p&gt;We conduct training on the official training set and test it on the official testing set of the challenge. Compared to the optimal results in baseline methods, our method increases Concordance Correlation Coefficients (CCC) by 19.64% and decreases Root Mean Square Error (RMSE) by 1.79% on the development set, and also increases CCC by 268.33% and decreases RMSE by 13.66% on the testing set, which means a significant performance compared to the baseline methods.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>MEDIC: A Multimodal Empathy Dataset in Counseling</title>
      <link>https://ustc-ac.github.io/publication/manual-mm23-zhu/</link>
      <pubDate>Mon, 04 Dec 2023 09:53:00 +0000</pubDate>
      <guid>https://ustc-ac.github.io/publication/manual-mm23-zhu/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Patch-Aware Representation Learning for Facial Expression Recognition</title>
      <link>https://ustc-ac.github.io/publication/manual-mm23-wu/</link>
      <pubDate>Mon, 04 Dec 2023 09:53:00 +0000</pubDate>
      <guid>https://ustc-ac.github.io/publication/manual-mm23-wu/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Progressive Visual Content Understanding Network for Image Emotion Classification</title>
      <link>https://ustc-ac.github.io/publication/manual-mm23-pan/</link>
      <pubDate>Mon, 04 Dec 2023 09:53:00 +0000</pubDate>
      <guid>https://ustc-ac.github.io/publication/manual-mm23-pan/</guid>
      <description></description>
    </item>
    
    <item>
      <title>MEDIC</title>
      <link>https://ustc-ac.github.io/datasets/medic/</link>
      <pubDate>Thu, 03 Aug 2023 08:57:57 +0000</pubDate>
      <guid>https://ustc-ac.github.io/datasets/medic/</guid>
      <description></description>
    </item>
    
    <item>
      <title>One student from our group was awarded Doctoral Dissertation Award</title>
      <link>https://ustc-ac.github.io/post/award-2023-yinshi/</link>
      <pubDate>Sun, 30 Jul 2023 10:26:05 +0000</pubDate>
      <guid>https://ustc-ac.github.io/post/award-2023-yinshi/</guid>
      <description>&lt;p&gt;&lt;a href=&#34;../author/yin-shi/&#34;&gt;Shi Yin&lt;/a&gt; was awarded the 2022 ACM CHINA COUNCIL HEFEI CHAPTER DOCTORAL DISSERTATION AWARD.&lt;/p&gt;
&lt;figure  &gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img alt=&#34;&#34; srcset=&#34;
               /post/award-2023-yinshi/yin_hu0c815f353e4a9fcaa5e2dca36bf51048_358300_ff7d47b138cc390664c2e2d03d35c269.jpg 400w,
               /post/award-2023-yinshi/yin_hu0c815f353e4a9fcaa5e2dca36bf51048_358300_b9b2915d9229279fd7270911cde39218.jpg 760w,
               /post/award-2023-yinshi/yin_hu0c815f353e4a9fcaa5e2dca36bf51048_358300_1200x1200_fit_q75_lanczos.jpg 1200w&#34;
               src=&#34;https://ustc-ac.github.io/post/award-2023-yinshi/yin_hu0c815f353e4a9fcaa5e2dca36bf51048_358300_ff7d47b138cc390664c2e2d03d35c269.jpg&#34;
               width=&#34;760&#34;
               height=&#34;570&#34;
               loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;/figure&gt;</description>
    </item>
    
    <item>
      <title>Three papers are accepted by ACMMM2023</title>
      <link>https://ustc-ac.github.io/post/2023-acmmm-threepapers/</link>
      <pubDate>Tue, 25 Jul 2023 09:25:52 +0000</pubDate>
      <guid>https://ustc-ac.github.io/post/2023-acmmm-threepapers/</guid>
      <description>&lt;p&gt;Three papers by &lt;a href=&#34;https://ustc-ac.github.io/author/yi-wu/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Yi Wu&lt;/a&gt;, &lt;a href=&#34;https://ustc-ac.github.io/author/zhouan-zhu/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Zhouan Zhu&lt;/a&gt;, and &lt;a href=&#34;https://ustc-ac.github.io/author/jicai-pan/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Jicai Pan&lt;/a&gt; are accepted by ACMMM2023&lt;/p&gt;
&lt;figure  &gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img alt=&#34;&#34;
           src=&#34;https://ustc-ac.github.io/post/2023-acmmm-threepapers/featured.gif&#34;
           loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;/figure&gt;</description>
    </item>
    
    <item>
      <title>Pose-Aware Facial Expression Recognition Assisted by Expression Descriptions</title>
      <link>https://ustc-ac.github.io/publication/manual-tac23-wu/</link>
      <pubDate>Sun, 16 Jul 2023 09:53:00 +0000</pubDate>
      <guid>https://ustc-ac.github.io/publication/manual-tac23-wu/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Privacy-Protected Facial Expression Recognition Augmented by High-Resolution Facial Images</title>
      <link>https://ustc-ac.github.io/publication/manual-icme23-liang/</link>
      <pubDate>Sun, 16 Jul 2023 09:53:00 +0000</pubDate>
      <guid>https://ustc-ac.github.io/publication/manual-icme23-liang/</guid>
      <description></description>
    </item>
    
    <item>
      <title>UniFaRN: Unified Transformer for Facial Reaction Generation</title>
      <link>https://ustc-ac.github.io/publication/manual-mm23-react/</link>
      <pubDate>Sun, 16 Jul 2023 09:53:00 +0000</pubDate>
      <guid>https://ustc-ac.github.io/publication/manual-mm23-react/</guid>
      <description></description>
    </item>
    
    <item>
      <title>One paper is accepted by IEEE Transactions on Affective Computing</title>
      <link>https://ustc-ac.github.io/post/2023-tac-wu/</link>
      <pubDate>Sun, 16 Jul 2023 09:25:52 +0000</pubDate>
      <guid>https://ustc-ac.github.io/post/2023-tac-wu/</guid>
      <description>&lt;p&gt;“Pose-Aware Facial Expression Recognition Assisted by Expression Descriptions” by &lt;a href=&#34;../author/yi-wu/&#34;&gt;Yi Wu&lt;/a&gt;, has been accepted for publication in a future issue of IEEE Transactions on Affective Computing journal.&lt;/p&gt;
&lt;figure  &gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img alt=&#34;&#34; srcset=&#34;
               /post/2023-tac-wu/featured_huf05967657e7a7c84beaa1d9b073d1d52_29203_04c13bf0700e73fd96d7b1ec873729f1.jpg 400w,
               /post/2023-tac-wu/featured_huf05967657e7a7c84beaa1d9b073d1d52_29203_f73d324e32df4956c9477c7b3018ecce.jpg 760w,
               /post/2023-tac-wu/featured_huf05967657e7a7c84beaa1d9b073d1d52_29203_1200x1200_fit_q75_lanczos.jpg 1200w&#34;
               src=&#34;https://ustc-ac.github.io/post/2023-tac-wu/featured_huf05967657e7a7c84beaa1d9b073d1d52_29203_04c13bf0700e73fd96d7b1ec873729f1.jpg&#34;
               width=&#34;760&#34;
               height=&#34;158&#34;
               loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;/figure&gt;</description>
    </item>
    
    <item>
      <title>One paper is accepted by ICME2023</title>
      <link>https://ustc-ac.github.io/post/2023-icme-liang/</link>
      <pubDate>Sat, 15 Jul 2023 09:25:52 +0000</pubDate>
      <guid>https://ustc-ac.github.io/post/2023-icme-liang/</guid>
      <description>&lt;p&gt;One paper by &lt;a href=&#34;author/cong-liang/&#34;&gt;Cong Liang&lt;/a&gt; is accepted by ICME23&lt;/p&gt;
&lt;figure  &gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img alt=&#34;&#34; srcset=&#34;
               /post/2023-icme-liang/featured_hu344999ba05594e9a9414be4923011afa_23178_d3bf62b0fc38f92ee34ba45354202377.jpg 400w,
               /post/2023-icme-liang/featured_hu344999ba05594e9a9414be4923011afa_23178_52998d0cbcc1f271cf16ad73f4ee2576.jpg 760w,
               /post/2023-icme-liang/featured_hu344999ba05594e9a9414be4923011afa_23178_1200x1200_fit_q75_lanczos.jpg 1200w&#34;
               src=&#34;https://ustc-ac.github.io/post/2023-icme-liang/featured_hu344999ba05594e9a9414be4923011afa_23178_d3bf62b0fc38f92ee34ba45354202377.jpg&#34;
               width=&#34;760&#34;
               height=&#34;208&#34;
               loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;/figure&gt;</description>
    </item>
    
    <item>
      <title>We have gotten the first price of REACT2023</title>
      <link>https://ustc-ac.github.io/post/2023-acmmm-react/</link>
      <pubDate>Tue, 11 Jul 2023 09:25:52 +0000</pubDate>
      <guid>https://ustc-ac.github.io/post/2023-acmmm-react/</guid>
      <description>&lt;p&gt;&lt;a href=&#34;../author/cong-liang/&#34;&gt;Liang Cong&lt;/a&gt;, &lt;a href=&#34;../author/jiahe-wang/&#34;&gt;Wang Jiahe&lt;/a&gt; and &lt;a href=&#34;../author/haofan-zhang/&#34;&gt;Zhang Haofan&lt;/a&gt; participated in the REACT2023 competition (Challenge@ACM-MM23) and won the first place.&lt;/p&gt;
&lt;figure  &gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img alt=&#34;&#34; srcset=&#34;
               /post/2023-acmmm-react/featured_hu48e19ef73d479585da3e065e943cc6fb_24659_9418675e90a7b823de539412ce62b12e.jpg 400w,
               /post/2023-acmmm-react/featured_hu48e19ef73d479585da3e065e943cc6fb_24659_f893843bee4ac8ecec51ce72de3cad02.jpg 760w,
               /post/2023-acmmm-react/featured_hu48e19ef73d479585da3e065e943cc6fb_24659_1200x1200_fit_q75_lanczos.jpg 1200w&#34;
               src=&#34;https://ustc-ac.github.io/post/2023-acmmm-react/featured_hu48e19ef73d479585da3e065e943cc6fb_24659_9418675e90a7b823de539412ce62b12e.jpg&#34;
               width=&#34;760&#34;
               height=&#34;205&#34;
               loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;/figure&gt;</description>
    </item>
    
    <item>
      <title>One paper is awarded the FG2023 Best Paper</title>
      <link>https://ustc-ac.github.io/post/2023-fg-best/</link>
      <pubDate>Fri, 20 Jan 2023 08:00:00 +0000</pubDate>
      <guid>https://ustc-ac.github.io/post/2023-fg-best/</guid>
      <description>&lt;p&gt;Our work &lt;a href=&#34;../publication/manual-fg23-wang&#34;&gt;&lt;em&gt;Low-Resolution Face Recognition Enhanced by High-Resolution Facial Images&lt;/em&gt;&lt;/a&gt; received the FG2023 Best Paper&lt;/p&gt;
&lt;figure  &gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img alt=&#34;&#34; srcset=&#34;
               /post/2023-fg-best/featured_hua1bf0bdc129d60b4e161755a58ae0725_640903_8109cdff835462a949b630452578997f.jpg 400w,
               /post/2023-fg-best/featured_hua1bf0bdc129d60b4e161755a58ae0725_640903_a51b3fd9bc4645d602082b5062d36fbd.jpg 760w,
               /post/2023-fg-best/featured_hua1bf0bdc129d60b4e161755a58ae0725_640903_1200x1200_fit_q75_lanczos.jpg 1200w&#34;
               src=&#34;https://ustc-ac.github.io/post/2023-fg-best/featured_hua1bf0bdc129d60b4e161755a58ae0725_640903_8109cdff835462a949b630452578997f.jpg&#34;
               width=&#34;760&#34;
               height=&#34;424&#34;
               loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;/figure&gt;</description>
    </item>
    
    <item>
      <title>Occluded Facial Expression Recognition using Self-supervised Learning</title>
      <link>https://ustc-ac.github.io/publication/manual-accv22-wang/</link>
      <pubDate>Thu, 08 Dec 2022 09:53:00 +0000</pubDate>
      <guid>https://ustc-ac.github.io/publication/manual-accv22-wang/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Human Pose Estimation with Shape Aware Loss</title>
      <link>https://ustc-ac.github.io/publication/manual-fg23-fang/</link>
      <pubDate>Wed, 07 Dec 2022 09:53:00 +0000</pubDate>
      <guid>https://ustc-ac.github.io/publication/manual-fg23-fang/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Low-Resolution Face Recognition Enhanced by High-Resolution Facial Images</title>
      <link>https://ustc-ac.github.io/publication/manual-fg23-wang/</link>
      <pubDate>Wed, 07 Dec 2022 09:53:00 +0000</pubDate>
      <guid>https://ustc-ac.github.io/publication/manual-fg23-wang/</guid>
      <description></description>
    </item>
    
    <item>
      <title>One paper is accepted by ACCV2022</title>
      <link>https://ustc-ac.github.io/post/2022-accv-wang/</link>
      <pubDate>Sun, 02 Oct 2022 08:00:00 +0000</pubDate>
      <guid>https://ustc-ac.github.io/post/2022-accv-wang/</guid>
      <description>&lt;p&gt;One paper by &lt;a href=&#34;author/jiahe-wang/&#34;&gt;Jiahe Wang&lt;/a&gt; and &lt;a href=&#34;author/heyan-ding/&#34;&gt;Heyan Ding&lt;/a&gt; is accepted by ACCV22&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>Two papers are accepted by FG2023</title>
      <link>https://ustc-ac.github.io/post/2023-fg-two/</link>
      <pubDate>Sun, 11 Sep 2022 09:25:52 +0000</pubDate>
      <guid>https://ustc-ac.github.io/post/2023-fg-two/</guid>
      <description>&lt;p&gt;Two papers by &lt;a href=&#34;../author/lin-fang/&#34;&gt;Lin Fang&lt;/a&gt; and &lt;a href=&#34;../author/haihan-wang&#34;&gt;Haihan Wang&lt;/a&gt; are accepted FG 2023&lt;/p&gt;
&lt;figure  &gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img alt=&#34;&#34; srcset=&#34;
               /post/2023-fg-two/featured_hu1ada13ffe5d3ff56922f72f76dc9c526_217126_d12a27a38e19347f6e3b1010f484882c.jpg 400w,
               /post/2023-fg-two/featured_hu1ada13ffe5d3ff56922f72f76dc9c526_217126_dbcb35b5e07f68d9da19c85af88c3b74.jpg 760w,
               /post/2023-fg-two/featured_hu1ada13ffe5d3ff56922f72f76dc9c526_217126_1200x1200_fit_q75_lanczos.jpg 1200w&#34;
               src=&#34;https://ustc-ac.github.io/post/2023-fg-two/featured_hu1ada13ffe5d3ff56922f72f76dc9c526_217126_d12a27a38e19347f6e3b1010f484882c.jpg&#34;
               width=&#34;760&#34;
               height=&#34;371&#34;
               loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;/figure&gt;</description>
    </item>
    
    <item>
      <title>One paper is awarded the ICPR2022 Track 4 Best Scientific Paper</title>
      <link>https://ustc-ac.github.io/post/2022-icpr-best/</link>
      <pubDate>Sun, 28 Aug 2022 08:00:00 +0000</pubDate>
      <guid>https://ustc-ac.github.io/post/2022-icpr-best/</guid>
      <description>&lt;p&gt;Our work &lt;a href=&#34;../publication/manual-icpr22-fang&#34;&gt;&lt;em&gt;Adversarial Stacking Ensemble for Facial Landmark Tracking&lt;/em&gt;&lt;/a&gt; received the ICPR2022 Track 4 Best Scientific Paper&lt;/p&gt;
&lt;figure  &gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img alt=&#34;&#34; srcset=&#34;
               /post/2022-icpr-best/featured_huc064e49a8002d6cf3cc94672b85257f8_167250_104d94e5d5fd0f48b6aa6dc7ae03e669.jpg 400w,
               /post/2022-icpr-best/featured_huc064e49a8002d6cf3cc94672b85257f8_167250_0f471b9602af2ac510ec8496e2b93a51.jpg 760w,
               /post/2022-icpr-best/featured_huc064e49a8002d6cf3cc94672b85257f8_167250_1200x1200_fit_q75_lanczos.jpg 1200w&#34;
               src=&#34;https://ustc-ac.github.io/post/2022-icpr-best/featured_huc064e49a8002d6cf3cc94672b85257f8_167250_104d94e5d5fd0f48b6aa6dc7ae03e669.jpg&#34;
               width=&#34;760&#34;
               height=&#34;576&#34;
               loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;/figure&gt;</description>
    </item>
    
    <item>
      <title>Incoming Talk in VALSE2022</title>
      <link>https://ustc-ac.github.io/post/talk-2022-valse/</link>
      <pubDate>Sat, 20 Aug 2022 09:25:52 +0000</pubDate>
      <guid>https://ustc-ac.github.io/post/talk-2022-valse/</guid>
      <description>&lt;p&gt;Pro. Shangfei Wang will deliver a talk name &lt;em&gt;Video emotional content analysis&lt;/em&gt; in VALSE2022&lt;/p&gt;
&lt;figure  &gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img alt=&#34;&#34; srcset=&#34;
               /post/talk-2022-valse/poster_hu7ceaea2b9cebfd0738f846ec211fc9b8_206066_261870414c99a2da2de5f0632c9ad473.jpg 400w,
               /post/talk-2022-valse/poster_hu7ceaea2b9cebfd0738f846ec211fc9b8_206066_a714fe0bbd4e0942775a6daea18159d7.jpg 760w,
               /post/talk-2022-valse/poster_hu7ceaea2b9cebfd0738f846ec211fc9b8_206066_1200x1200_fit_q75_lanczos.jpg 1200w&#34;
               src=&#34;https://ustc-ac.github.io/post/talk-2022-valse/poster_hu7ceaea2b9cebfd0738f846ec211fc9b8_206066_261870414c99a2da2de5f0632c9ad473.jpg&#34;
               width=&#34;760&#34;
               height=&#34;231&#34;
               loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;/figure&gt;</description>
    </item>
    
    <item>
      <title>Three papers are accepted by ACMMM2022</title>
      <link>https://ustc-ac.github.io/post/2022-mm-three/</link>
      <pubDate>Sun, 10 Jul 2022 08:00:00 +0000</pubDate>
      <guid>https://ustc-ac.github.io/post/2022-mm-three/</guid>
      <description>&lt;p&gt;Three papers by &lt;a href=&#34;https://ustc-ac.github.io/author/haihan-wang/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Wang&lt;/a&gt;, &lt;a href=&#34;https://ustc-ac.github.io/author/xiangyu-miao/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Miao&lt;/a&gt;, and &lt;a href=&#34;https://ustc-ac.github.io/author/jicai-pan/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Pan&lt;/a&gt; are accepted by ACMMM2022&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>Knowledge Guided Representation Disentanglement for Face Recognition from Low Illumination Images</title>
      <link>https://ustc-ac.github.io/publication/manual-mm22-miao/</link>
      <pubDate>Tue, 28 Jun 2022 09:53:00 +0000</pubDate>
      <guid>https://ustc-ac.github.io/publication/manual-mm22-miao/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Representation Learning through Multimodal Attention and Time-sync Comments for Video Affective Content Analysis</title>
      <link>https://ustc-ac.github.io/publication/manual-mm22-pan/</link>
      <pubDate>Tue, 28 Jun 2022 09:53:00 +0000</pubDate>
      <guid>https://ustc-ac.github.io/publication/manual-mm22-pan/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Two-Stage Multi-Scale Resolution-Adaptive Network for Low-Resolution Face Recognition</title>
      <link>https://ustc-ac.github.io/publication/manual-mm22-wang/</link>
      <pubDate>Tue, 28 Jun 2022 09:53:00 +0000</pubDate>
      <guid>https://ustc-ac.github.io/publication/manual-mm22-wang/</guid>
      <description></description>
    </item>
    
    <item>
      <title>One paper is accepted by ICPR2022</title>
      <link>https://ustc-ac.github.io/post/2022-icpr-fang/</link>
      <pubDate>Sat, 02 Apr 2022 08:00:00 +0000</pubDate>
      <guid>https://ustc-ac.github.io/post/2022-icpr-fang/</guid>
      <description>&lt;p&gt;Our work &lt;em&gt;Adversarial Stacking Ensemble for Facial Landmark Tracking&lt;/em&gt; (ID 1281) by &lt;a href=&#34;https://ustc-ac.github.io/author/shi-yin/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Yin&lt;/a&gt; and &lt;a href=&#34;https://ustc-ac.github.io/author/lin-fang/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Fang&lt;/a&gt; is accepted by ICPR22&lt;/p&gt;
&lt;figure  &gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img alt=&#34;&#34; srcset=&#34;
               /post/2022-icpr-fang/accepted_hu5ed295fde70f1be183bce161e169ce61_37359_8c65777bf9b29f7d70bfbd9d2c665d86.jpg 400w,
               /post/2022-icpr-fang/accepted_hu5ed295fde70f1be183bce161e169ce61_37359_e03ad01044d7f4533bcbec8670152207.jpg 760w,
               /post/2022-icpr-fang/accepted_hu5ed295fde70f1be183bce161e169ce61_37359_1200x1200_fit_q75_lanczos.jpg 1200w&#34;
               src=&#34;https://ustc-ac.github.io/post/2022-icpr-fang/accepted_hu5ed295fde70f1be183bce161e169ce61_37359_8c65777bf9b29f7d70bfbd9d2c665d86.jpg&#34;
               width=&#34;548&#34;
               height=&#34;333&#34;
               loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;/figure&gt;</description>
    </item>
    
    <item>
      <title>Adversarial Stacking Ensemble for Facial Landmark Tracking</title>
      <link>https://ustc-ac.github.io/publication/manual-icpr22-fang/</link>
      <pubDate>Fri, 01 Apr 2022 09:53:00 +0000</pubDate>
      <guid>https://ustc-ac.github.io/publication/manual-icpr22-fang/</guid>
      <description></description>
    </item>
    
    <item>
      <title>One of our work is accepted by CVPR2022 !</title>
      <link>https://ustc-ac.github.io/post/2022-cvpr-chang/</link>
      <pubDate>Thu, 03 Mar 2022 08:00:00 +0000</pubDate>
      <guid>https://ustc-ac.github.io/post/2022-cvpr-chang/</guid>
      <description>&lt;p&gt;Our work &lt;em&gt;Knowledge-Driven Self-Supervised Representation Learning for Facial Action Unit Recognition&lt;/em&gt; (ID 6728) by &lt;a href=&#34;https://ustc-ac.github.io/author/yanan-chang/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Chang&lt;/a&gt; is accepted by CVPR2022!&lt;/p&gt;
&lt;figure  &gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img alt=&#34;&#34; srcset=&#34;
               /post/2022-cvpr-chang/accepted_hu79d0e1e93d1cec17731ffea58c3d7948_21562_ec14882abbfde4244df964826fb9923d.jpg 400w,
               /post/2022-cvpr-chang/accepted_hu79d0e1e93d1cec17731ffea58c3d7948_21562_3bb272a26a77f668f40f73b8d4ad9c47.jpg 760w,
               /post/2022-cvpr-chang/accepted_hu79d0e1e93d1cec17731ffea58c3d7948_21562_1200x1200_fit_q75_lanczos.jpg 1200w&#34;
               src=&#34;https://ustc-ac.github.io/post/2022-cvpr-chang/accepted_hu79d0e1e93d1cec17731ffea58c3d7948_21562_ec14882abbfde4244df964826fb9923d.jpg&#34;
               width=&#34;760&#34;
               height=&#34;48&#34;
               loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;/figure&gt;</description>
    </item>
    
    <item>
      <title>Knowledge-Driven Self-Supervised Representation Learning for Facial Action Unit Recognition</title>
      <link>https://ustc-ac.github.io/publication/manual-cvpr22-chang/</link>
      <pubDate>Tue, 01 Mar 2022 09:53:00 +0000</pubDate>
      <guid>https://ustc-ac.github.io/publication/manual-cvpr22-chang/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Incoming Talk</title>
      <link>https://ustc-ac.github.io/post/talk-2022-ha/</link>
      <pubDate>Sat, 01 Jan 2022 09:25:52 +0000</pubDate>
      <guid>https://ustc-ac.github.io/post/talk-2022-ha/</guid>
      <description>&lt;p&gt;Pro. Shangfei Wang will deliver a talk name &lt;em&gt;Knowing Faces and Hearts-Emotional Human-Computer Interaction Research&lt;/em&gt; in Harbin Institute of Technology Wuhu Research Institute Conference on Artificial Intelligence and Machine Vision (Online)&lt;/p&gt;
&lt;figure  &gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img alt=&#34;&#34; srcset=&#34;
               /post/talk-2022-ha/poster_hu8d53b52848da6b5cee2b33e5b24ed64c_397749_9409ee8e8c9f6b478037b9d7beddf0c8.jpg 400w,
               /post/talk-2022-ha/poster_hu8d53b52848da6b5cee2b33e5b24ed64c_397749_c3526ae1aacd64cb8e1d1b28e20ad59b.jpg 760w,
               /post/talk-2022-ha/poster_hu8d53b52848da6b5cee2b33e5b24ed64c_397749_1200x1200_fit_q75_lanczos.jpg 1200w&#34;
               src=&#34;https://ustc-ac.github.io/post/talk-2022-ha/poster_hu8d53b52848da6b5cee2b33e5b24ed64c_397749_9409ee8e8c9f6b478037b9d7beddf0c8.jpg&#34;
               width=&#34;182&#34;
               height=&#34;760&#34;
               loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;/figure&gt;</description>
    </item>
    
    <item>
      <title>Pose-Invariant Facial Expression Recognition</title>
      <link>https://ustc-ac.github.io/publication/manual-fg21-liang/</link>
      <pubDate>Sun, 05 Dec 2021 09:53:00 +0000</pubDate>
      <guid>https://ustc-ac.github.io/publication/manual-fg21-liang/</guid>
      <description></description>
    </item>
    
    <item>
      <title>One paper is accepted by FG2021</title>
      <link>https://ustc-ac.github.io/post/2021-fg-liang/</link>
      <pubDate>Sun, 05 Dec 2021 09:25:52 +0000</pubDate>
      <guid>https://ustc-ac.github.io/post/2021-fg-liang/</guid>
      <description>&lt;p&gt;&amp;ldquo;Pose-lnvariant Facial Expression Recognition&amp;rdquo; by Guang Liang has been accepted FG 2021&lt;/p&gt;
&lt;figure  &gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img alt=&#34;&#34; srcset=&#34;
               /post/2021-fg-liang/featured_hu5438825b9b6d1014226d20d231e650c2_1500562_0da4fd5137681798f432a9f075ec136d.jpg 400w,
               /post/2021-fg-liang/featured_hu5438825b9b6d1014226d20d231e650c2_1500562_4da344dce8cf5215b2e6fcdf4fe21595.jpg 760w,
               /post/2021-fg-liang/featured_hu5438825b9b6d1014226d20d231e650c2_1500562_1200x1200_fit_q75_lanczos.jpg 1200w&#34;
               src=&#34;https://ustc-ac.github.io/post/2021-fg-liang/featured_hu5438825b9b6d1014226d20d231e650c2_1500562_0da4fd5137681798f432a9f075ec136d.jpg&#34;
               width=&#34;760&#34;
               height=&#34;428&#34;
               loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;/figure&gt;</description>
    </item>
    
    <item>
      <title>One paper is accepted by IEEE Transactions on Affective Computing</title>
      <link>https://ustc-ac.github.io/post/2021-journal-chang/</link>
      <pubDate>Fri, 24 Sep 2021 08:00:00 +0000</pubDate>
      <guid>https://ustc-ac.github.io/post/2021-journal-chang/</guid>
      <description>&lt;p&gt;&amp;ldquo;Dual Learning for Joint Facial Landmark Detection and Action Unit Recognition&amp;rdquo; by Ya&amp;rsquo;nan Chang, has been accepted for publication in a future issue of &lt;em&gt;IEEE Transactions on Affective Computing&lt;/em&gt; journal. You can find it in &lt;em&gt;&lt;strong&gt;Publications&lt;/strong&gt;&lt;/em&gt; column and &lt;a href=&#34;https://ieeexplore.ieee.org/document/9543559&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Link&lt;/a&gt;&lt;/p&gt;
&lt;figure  &gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img alt=&#34;&#34; srcset=&#34;
               /post/2021-journal-chang/featured_hu02b94926cf1a462548b33acc6dc3b6c0_19043_02579d245c23656688c7338102ddcb9a.jpg 400w,
               /post/2021-journal-chang/featured_hu02b94926cf1a462548b33acc6dc3b6c0_19043_c17c28bccb54fab710b1c2645fb64ded.jpg 760w,
               /post/2021-journal-chang/featured_hu02b94926cf1a462548b33acc6dc3b6c0_19043_1200x1200_fit_q75_lanczos.jpg 1200w&#34;
               src=&#34;https://ustc-ac.github.io/post/2021-journal-chang/featured_hu02b94926cf1a462548b33acc6dc3b6c0_19043_02579d245c23656688c7338102ddcb9a.jpg&#34;
               width=&#34;760&#34;
               height=&#34;227&#34;
               loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;/figure&gt;</description>
    </item>
    
    <item>
      <title>Incoming Talk in MIPR2021</title>
      <link>https://ustc-ac.github.io/post/talk-2022-mipr/</link>
      <pubDate>Mon, 30 Aug 2021 09:25:52 +0000</pubDate>
      <guid>https://ustc-ac.github.io/post/talk-2022-mipr/</guid>
      <description>&lt;p&gt;Pro. Shangfei Wang will deliver a talk name &lt;em&gt;Facial Action Unit Recognition under Non-Full Annotation&lt;/em&gt; in &lt;a href=&#34;https://aiart2021.github.io/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;MIPR2021&lt;/a&gt;&lt;/p&gt;
&lt;figure  &gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img alt=&#34;&#34; srcset=&#34;
               /post/talk-2022-mipr/poster_hub2fbd4f15906ae5c9d91adda5b4611a7_206066_f8b986d45e1ed4a8a7ea26b86d7faf95.jpg 400w,
               /post/talk-2022-mipr/poster_hub2fbd4f15906ae5c9d91adda5b4611a7_206066_b562555b51dcf1507f7fa9a63810008c.jpg 760w,
               /post/talk-2022-mipr/poster_hub2fbd4f15906ae5c9d91adda5b4611a7_206066_1200x1200_fit_q75_lanczos.jpg 1200w&#34;
               src=&#34;https://ustc-ac.github.io/post/talk-2022-mipr/poster_hub2fbd4f15906ae5c9d91adda5b4611a7_206066_f8b986d45e1ed4a8a7ea26b86d7faf95.jpg&#34;
               width=&#34;760&#34;
               height=&#34;340&#34;
               loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;/figure&gt;</description>
    </item>
    
    <item>
      <title>One paper is accepted by IJCAI 2021</title>
      <link>https://ustc-ac.github.io/post/2021-ijcai-bin-xia/</link>
      <pubDate>Tue, 29 Jun 2021 08:57:57 +0000</pubDate>
      <guid>https://ustc-ac.github.io/post/2021-ijcai-bin-xia/</guid>
      <description>&lt;p&gt;Bin Xia&amp;rsquo;s paper “Micro-Expression Recognition Enhanced by Macro-Expression from Spatial-Temporal Domain” is accepted by the 2021 Conference on IJCAI.&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;https://ijcai-21.org/program-main-track/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Link&lt;/a&gt;&lt;/p&gt;
&lt;figure  &gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img alt=&#34;&#34; srcset=&#34;
               /post/2021-ijcai-bin-xia/featured_hu2c69917c91855ca124047f9f7218696c_193835_029ea1dd9d8d3922e8f3fb1957ac27d3.jpg 400w,
               /post/2021-ijcai-bin-xia/featured_hu2c69917c91855ca124047f9f7218696c_193835_758b1e297afccb5136b6560d166bd154.jpg 760w,
               /post/2021-ijcai-bin-xia/featured_hu2c69917c91855ca124047f9f7218696c_193835_1200x1200_fit_q75_lanczos.jpg 1200w&#34;
               src=&#34;https://ustc-ac.github.io/post/2021-ijcai-bin-xia/featured_hu2c69917c91855ca124047f9f7218696c_193835_029ea1dd9d8d3922e8f3fb1957ac27d3.jpg&#34;
               width=&#34;760&#34;
               height=&#34;365&#34;
               loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;/figure&gt;</description>
    </item>
    
    <item>
      <title>Three students from our group were awarded outstanding graduates</title>
      <link>https://ustc-ac.github.io/post/outstanding-graduates/</link>
      <pubDate>Sun, 20 Jun 2021 10:26:05 +0000</pubDate>
      <guid>https://ustc-ac.github.io/post/outstanding-graduates/</guid>
      <description>&lt;p&gt;They accepted the award at the graduation ceremony.&lt;/p&gt;
&lt;p&gt;













&lt;figure  id=&#34;figure-fig1-shi-yin-first-from-the-left&#34;&gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img alt=&#34;Fig1. Shi Yin (first from the left)&#34; srcset=&#34;
               /post/outstanding-graduates/yin_hubac859aa24d48443df50ef54c1c835f4_1229321_148e7ad4a7e1b84c00e4a6416e588a8c.jpg 400w,
               /post/outstanding-graduates/yin_hubac859aa24d48443df50ef54c1c835f4_1229321_9d31d81aa5afbb60fb46f935d7251e54.jpg 760w,
               /post/outstanding-graduates/yin_hubac859aa24d48443df50ef54c1c835f4_1229321_1200x1200_fit_q75_lanczos.jpg 1200w&#34;
               src=&#34;https://ustc-ac.github.io/post/outstanding-graduates/yin_hubac859aa24d48443df50ef54c1c835f4_1229321_148e7ad4a7e1b84c00e4a6416e588a8c.jpg&#34;
               width=&#34;760&#34;
               height=&#34;507&#34;
               loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;figcaption&gt;
      Fig1. Shi Yin (first from the left)
    &lt;/figcaption&gt;&lt;/figure&gt;














&lt;figure  id=&#34;figure-fig2-zhiwei-xu-first-from-the-rightt&#34;&gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img alt=&#34;Fig2. Zhiwei Xu (first from the rightt)&#34; srcset=&#34;
               /post/outstanding-graduates/xu_hu48fff2b7152bbc7bbed649388a630b60_1199446_01152d543151c11640d4b8bffc64257a.jpg 400w,
               /post/outstanding-graduates/xu_hu48fff2b7152bbc7bbed649388a630b60_1199446_2a1df60b248b6a1377d245d4edec1328.jpg 760w,
               /post/outstanding-graduates/xu_hu48fff2b7152bbc7bbed649388a630b60_1199446_1200x1200_fit_q75_lanczos.jpg 1200w&#34;
               src=&#34;https://ustc-ac.github.io/post/outstanding-graduates/xu_hu48fff2b7152bbc7bbed649388a630b60_1199446_01152d543151c11640d4b8bffc64257a.jpg&#34;
               width=&#34;760&#34;
               height=&#34;507&#34;
               loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;figcaption&gt;
      Fig2. Zhiwei Xu (first from the rightt)
    &lt;/figcaption&gt;&lt;/figure&gt;














&lt;figure  id=&#34;figure-fig3-bin-xia-second-from-the-left&#34;&gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img alt=&#34;Fig3. Bin Xia (second from the left)&#34; srcset=&#34;
               /post/outstanding-graduates/xia_hu886e5b37725fd74635920f35135d58ed_1177558_b33e8928cfdedf595ec41b47290ec777.jpg 400w,
               /post/outstanding-graduates/xia_hu886e5b37725fd74635920f35135d58ed_1177558_3976a911b0eae2ce9bd7f0d1a4f1c4fb.jpg 760w,
               /post/outstanding-graduates/xia_hu886e5b37725fd74635920f35135d58ed_1177558_1200x1200_fit_q75_lanczos.jpg 1200w&#34;
               src=&#34;https://ustc-ac.github.io/post/outstanding-graduates/xia_hu886e5b37725fd74635920f35135d58ed_1177558_b33e8928cfdedf595ec41b47290ec777.jpg&#34;
               width=&#34;760&#34;
               height=&#34;507&#34;
               loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;figcaption&gt;
      Fig3. Bin Xia (second from the left)
    &lt;/figcaption&gt;&lt;/figure&gt;&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>Micro-Expression Recognition Enhanced by Macro-Expression from Spatial-Temporal Domain</title>
      <link>https://ustc-ac.github.io/publication/manual-ijcai21-xia/</link>
      <pubDate>Thu, 29 Apr 2021 10:03:01 +0000</pubDate>
      <guid>https://ustc-ac.github.io/publication/manual-ijcai21-xia/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Emotional Attention Detection and Correlation Exploration for Image Emotion Distribution Learning</title>
      <link>https://ustc-ac.github.io/publication/emotional-attention-detection-and-correlation-exploration-for-image-emotion-distribution-learning/</link>
      <pubDate>Tue, 06 Apr 2021 11:54:51 +0000</pubDate>
      <guid>https://ustc-ac.github.io/publication/emotional-attention-detection-and-correlation-exploration-for-image-emotion-distribution-learning/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Capturing Emotion Distribution for Multimedia Emotion Tagging</title>
      <link>https://ustc-ac.github.io/publication/capturing-emotion-distribution-for-multimedia-emotion-tagging/</link>
      <pubDate>Thu, 18 Feb 2021 11:58:00 +0000</pubDate>
      <guid>https://ustc-ac.github.io/publication/capturing-emotion-distribution-for-multimedia-emotion-tagging/</guid>
      <description>&lt;p&gt;Multimedia collections usually induce multiple emotions in audiences. The data distribution of multiple emotions can be leveraged to facilitate the learning process of emotion tagging, yet has not been thoroughly explored. To address this, we propose adversarial learning to fully capture emotion distributions for emotion tagging of multimedia data. The proposed multimedia emotion tagging approach includes an emotion classifier and a discriminator. The emotion classifier predicts emotion labels of multimedia data from their content. The discriminator distinguishes the predicted emotion labels from the ground truth labels. The emotion classifier and the discriminator are trained simultaneously in competition with each other. By jointly minimizing the traditional supervised loss and maximizing the distribution similarity between the predicted emotion labels and the ground truth emotion labels, the proposed multimedia emotion tagging approach successfully captures both the mapping function between multimedia content and emotion labels as well as prior distribution in emotion labels, and thus achieves state-of-the-art performance for multiple emotion tagging, as demonstrated by the experimental results on four benchmark databases.&lt;/p&gt;














&lt;figure  id=&#34;figure-fig-the-framework-of-the-proposed-multi-emotion-tagging-model-c-is-the-multiple-emotion-tag-classifier-and-d-is-the-multiple-emotion-tag-discriminator-cx-is-the-multiple-emotion-prediction-for-feature-vector-x-y-is-the-ground-truth-multiple-emotion-label-of-x-y-0-is-the-real-multiple-emotion-label-from-ground-truth-label-set-the-dotted-line-indicates-that-there-is-an-one-to-one-correspondence-between-cx-and-y-see-text-for-details&#34;&gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img alt=&#34;Fig. The framework of the proposed multi-emotion tagging model. C is the multiple emotion tag classifier, and D is the multiple emotion tag discriminator. C(x) is the multiple emotion prediction for feature vector x. y is the ground truth multiple emotion label of x. y 0 is the real multiple emotion label from ground truth label set. The dotted line indicates that there is an one-to-one correspondence between C(x) and y. See text for details.&#34; srcset=&#34;
               /publication/capturing-emotion-distribution-for-multimedia-emotion-tagging/featured_hufb81d44ede70ae46d4815dea23d83fa1_71767_247e857c1e6d823735aaa3289daa1c24.jpg 400w,
               /publication/capturing-emotion-distribution-for-multimedia-emotion-tagging/featured_hufb81d44ede70ae46d4815dea23d83fa1_71767_9d15f47da7b78c49ba5849d19a69178e.jpg 760w,
               /publication/capturing-emotion-distribution-for-multimedia-emotion-tagging/featured_hufb81d44ede70ae46d4815dea23d83fa1_71767_1200x1200_fit_q75_lanczos.jpg 1200w&#34;
               src=&#34;https://ustc-ac.github.io/publication/capturing-emotion-distribution-for-multimedia-emotion-tagging/featured_hufb81d44ede70ae46d4815dea23d83fa1_71767_247e857c1e6d823735aaa3289daa1c24.jpg&#34;
               width=&#34;760&#34;
               height=&#34;324&#34;
               loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;figcaption&gt;
      Fig. The framework of the proposed multi-emotion tagging model. C is the multiple emotion tag classifier, and D is the multiple emotion tag discriminator. C(x) is the multiple emotion prediction for feature vector x. y is the ground truth multiple emotion label of x. y 0 is the real multiple emotion label from ground truth label set. The dotted line indicates that there is an one-to-one correspondence between C(x) and y. See text for details.
    &lt;/figcaption&gt;&lt;/figure&gt;
</description>
    </item>
    
    <item>
      <title>Exploring Adversarial Learning for Deep Semi-Supervised Facial Action Unit Recognition</title>
      <link>https://ustc-ac.github.io/publication/dblp-journalscorrabs-2106-02258/</link>
      <pubDate>Fri, 01 Jan 2021 00:00:00 +0000</pubDate>
      <guid>https://ustc-ac.github.io/publication/dblp-journalscorrabs-2106-02258/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Multi-task face analyses through adversarial learning</title>
      <link>https://ustc-ac.github.io/publication/dblp-journalspr-wang-yhl-21/</link>
      <pubDate>Fri, 01 Jan 2021 00:00:00 +0000</pubDate>
      <guid>https://ustc-ac.github.io/publication/dblp-journalspr-wang-yhl-21/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Attentive One-Dimensional Heatmap Regression for Facial Landmark Detection and Tracking</title>
      <link>https://ustc-ac.github.io/publication/dblp-confmm-yin-wccl-20/</link>
      <pubDate>Thu, 15 Oct 2020 00:00:00 +0000</pubDate>
      <guid>https://ustc-ac.github.io/publication/dblp-confmm-yin-wccl-20/</guid>
      <description>&lt;p&gt;Although heatmap regression is considered a state-of-the-art method to locate facial landmarks, it suffers from huge spatial complexity and is prone to quantization error. To address this, we propose a novel attentive one-dimensional heatmap regression method for facial landmark localization. First, we predict two groups of 1D heatmaps to represent the marginal distributions of the 𝑥 and 𝑦 coordinates. These 1D heatmaps reduce spatial complexity significantly compared to current heatmap regression methods, which use 2D heatmaps to represent the joint distributions of 𝑥 and 𝑦 coordinates. With much lower spatial complexity, the proposed method can output high-resolution 1D heatmaps despite limited GPU memory, significantly alleviating the quantization error. Second, a co-attention mechanism is adopted to model the inherent spatial patterns existing in 𝑥 and 𝑦 coordinates, and therefore the joint distributions on the 𝑥 and 𝑦 axes are also captured. Third, based on the 1D heatmap structures, we propose a facial landmark detector capturing spatial patterns for landmark detection on an image; and a tracker further capturing temporal patterns with a temporal refinement mechanism for landmark tracking. Experimental results on four benchmark databases demonstrate the superiority of our method.&lt;/p&gt;














&lt;figure  id=&#34;figure-fig-the-proposed-detector-a-and-tracker-b&#34;&gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img alt=&#34;Fig. The proposed detector (a) and tracker (b).&#34; srcset=&#34;
               /publication/dblp-confmm-yin-wccl-20/featured_hu22c09b2bbf7a4fd412912d6cba403c45_869902_b2b2b5e4846adc9bf0e43146ebc4a648.png 400w,
               /publication/dblp-confmm-yin-wccl-20/featured_hu22c09b2bbf7a4fd412912d6cba403c45_869902_8c3a8280da7eec8db4b8d933907c5c70.png 760w,
               /publication/dblp-confmm-yin-wccl-20/featured_hu22c09b2bbf7a4fd412912d6cba403c45_869902_1200x1200_fit_lanczos_3.png 1200w&#34;
               src=&#34;https://ustc-ac.github.io/publication/dblp-confmm-yin-wccl-20/featured_hu22c09b2bbf7a4fd412912d6cba403c45_869902_b2b2b5e4846adc9bf0e43146ebc4a648.png&#34;
               width=&#34;760&#34;
               height=&#34;498&#34;
               loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;figcaption&gt;
      Fig. The proposed detector (a) and tracker (b).
    &lt;/figcaption&gt;&lt;/figure&gt;
</description>
    </item>
    
    <item>
      <title>Dual Learning for Facial Action Unit Detection Under Nonfull Annotation</title>
      <link>https://ustc-ac.github.io/publication/dual-learning-for-facial-action-unit-detection-under-nonfull-annotation/</link>
      <pubDate>Mon, 13 Jul 2020 09:53:00 +0000</pubDate>
      <guid>https://ustc-ac.github.io/publication/dual-learning-for-facial-action-unit-detection-under-nonfull-annotation/</guid>
      <description></description>
    </item>
    
    <item>
      <title>NVIE</title>
      <link>https://ustc-ac.github.io/datasets/nvie/</link>
      <pubDate>Mon, 29 Jun 2020 08:57:57 +0000</pubDate>
      <guid>https://ustc-ac.github.io/datasets/nvie/</guid>
      <description></description>
    </item>
    
    <item>
      <title>A Novel Dynamic Model Capturing Spatial and Temporal Patterns for Facial Expression Analysis</title>
      <link>https://ustc-ac.github.io/publication/dblp-journalspami-wang-zyyj-20/</link>
      <pubDate>Wed, 01 Jan 2020 00:00:00 +0000</pubDate>
      <guid>https://ustc-ac.github.io/publication/dblp-journalspami-wang-zyyj-20/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Capturing Joint Label Distribution for Multi-Label Classification Through Adversarial Learning</title>
      <link>https://ustc-ac.github.io/publication/dblp-journalstkde-wang-pz-20/</link>
      <pubDate>Wed, 01 Jan 2020 00:00:00 +0000</pubDate>
      <guid>https://ustc-ac.github.io/publication/dblp-journalstkde-wang-pz-20/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Exploiting Multi-Emotion Relations at Feature and Label Levels for Emotion Tagging</title>
      <link>https://ustc-ac.github.io/publication/dblp-confmm-xu-ww-20/</link>
      <pubDate>Wed, 01 Jan 2020 00:00:00 +0000</pubDate>
      <guid>https://ustc-ac.github.io/publication/dblp-confmm-xu-ww-20/</guid>
      <description>&lt;p&gt;The dependence among emotions is crucial to boost emotion tagging. In this paper, we propose a novel emotion tagging method, that thoroughly explores emotion relations from both the feature and label levels. Specifically, a graph convolutional network is introduced to inject local dependence among emotions into the model at the feature level, while an adversarial learning strategy is applied to constrain the joint distribution of multiple emotions at the label level. In addition, a new balanced loss function that mitigates the adverse effects of intra-class and inter-class imbalance is introduced to deal with the imbalance of emotion labels. Experimental results on several benchmark databases demonstrate the superiority of the proposed method compared to state-of-the-art works.&lt;/p&gt;














&lt;figure  id=&#34;figure-fig-the-architecture-of-the-proposed-emotion-tagging-method-it-consists-of-an-emotional-gcn-φg-an-encoder-φe--a-classifier-φc--and-a-discriminator-d-dgcn-denotes-the-product-of-the-output-of-emotional-gcn-and-the-encoder-feature-and-d-denotes-the-concatenation-of-dgcn-and-the-output-of-the-encoder-y-and-ŷrepresent-the-real-label-and-the-predicted-label-respectively&#34;&gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img alt=&#34;Fig. The architecture of the proposed emotion tagging method. It consists of an emotional GCN Φg, an encoder Φe , a classifier Φc , and a discriminator D. dgcn denotes the product of the output of emotional GCN and the encoder feature, and d denotes the concatenation of dgcn and the output of the encoder. y and ŷ represent the real label and the predicted label, respectively.&#34; srcset=&#34;
               /publication/dblp-confmm-xu-ww-20/featured_hu3761510e6e147991b8afa8272c353f87_120835_ea8d120282a84a3099ac9e43758706b0.jpg 400w,
               /publication/dblp-confmm-xu-ww-20/featured_hu3761510e6e147991b8afa8272c353f87_120835_3e2c15bb1a0f5c524a41632e86e8c53a.jpg 760w,
               /publication/dblp-confmm-xu-ww-20/featured_hu3761510e6e147991b8afa8272c353f87_120835_1200x1200_fit_q75_lanczos.jpg 1200w&#34;
               src=&#34;https://ustc-ac.github.io/publication/dblp-confmm-xu-ww-20/featured_hu3761510e6e147991b8afa8272c353f87_120835_ea8d120282a84a3099ac9e43758706b0.jpg&#34;
               width=&#34;760&#34;
               height=&#34;305&#34;
               loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;figcaption&gt;
      Fig. The architecture of the proposed emotion tagging method. It consists of an emotional GCN Φ&lt;sub&gt;g&lt;/sub&gt;, an encoder Φ&lt;sub&gt;e&lt;/sub&gt; , a classifier Φ&lt;sub&gt;c&lt;/sub&gt; , and a discriminator D. d&lt;sub&gt;gcn&lt;/sub&gt; denotes the product of the output of emotional GCN and the encoder feature, and d denotes the concatenation of d&lt;sub&gt;gcn&lt;/sub&gt; and the output of the encoder. y and ŷ represent the real label and the predicted label, respectively.
    &lt;/figcaption&gt;&lt;/figure&gt;
</description>
    </item>
    
    <item>
      <title>Exploiting Self-Supervised and Semi-Supervised Learning for Facial Landmark Tracking with Unlabeled Data</title>
      <link>https://ustc-ac.github.io/publication/dblp-confmm-yin-wcc-20/</link>
      <pubDate>Wed, 01 Jan 2020 00:00:00 +0000</pubDate>
      <guid>https://ustc-ac.github.io/publication/dblp-confmm-yin-wcc-20/</guid>
      <description>&lt;p&gt;Current work of facial landmark tracking usually requires large amounts of fully annotated facial videos to train a landmark tracker. To relieve the burden of manual annotations, we propose a novel facial landmark tracking method that makes full use of unlabeled facial videos by exploiting both self-supervised and semi-supervised learning mechanisms. First, self-supervised learning is adopted for representation learning from unlabeled facial videos. Specifically, a facial video and its shuffled version are fed into a feature encoder and a classifier. The feature encoder is used to learn visual representations, and the classifier distinguishes the input videos as the original or the shuffled ones. The feature encoder and the classifier are trained jointly. Through self-supervised learning, the spatial and temporal patterns of a facial video are captured at representation level. After that, the facial landmark tracker, consisting of the pre-trained feature encoder and a regressor, is trained semi-supervisedly. The consistencies among the tracking results of the original, the inverse and the disturbed facial sequences are exploited as the constraints on the unlabeled facial videos, and the supervised loss is adopted for the labeled videos. Through semi-supervised end to-end training, the tracker captures sequential patterns inherent in facial videos despite small amount of manual annotations. Experiments on two benchmark datasets show that the proposed framework outperforms state-of-the art semi-supervised facial landmark tracking methods, and also achieves advanced performance compared to fully supervised facial landmark tracking methods.&lt;/p&gt;














&lt;figure  id=&#34;figure-fig-the-proposed-two-stage-learning-framework-the-first-stage-is-self-supervised-learning-stage-as-shown-in-the-upper-part-of-the-fig-the-second-stage-is-semi-supervised-learning-stage-as-shown-in-the-lower-part-of-the-figure&#34;&gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img alt=&#34;Fig. The proposed two-stage learning framework. The first stage is self-supervised learning stage, as shown in the upper part of the Fig. The second stage is semi-supervised learning stage, as shown in the lower part of the figure.&#34; srcset=&#34;
               /publication/dblp-confmm-yin-wcc-20/featured_hub08f116c5b40a4b0bfaddc695f5ffa21_256954_579755cb69722ad3b53926d39f603b7a.jpg 400w,
               /publication/dblp-confmm-yin-wcc-20/featured_hub08f116c5b40a4b0bfaddc695f5ffa21_256954_b96c3a48c425c65acb4fcbbfa2599202.jpg 760w,
               /publication/dblp-confmm-yin-wcc-20/featured_hub08f116c5b40a4b0bfaddc695f5ffa21_256954_1200x1200_fit_q75_lanczos.jpg 1200w&#34;
               src=&#34;https://ustc-ac.github.io/publication/dblp-confmm-yin-wcc-20/featured_hub08f116c5b40a4b0bfaddc695f5ffa21_256954_579755cb69722ad3b53926d39f603b7a.jpg&#34;
               width=&#34;760&#34;
               height=&#34;372&#34;
               loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;figcaption&gt;
      Fig. The proposed two-stage learning framework. The first stage is self-supervised learning stage, as shown in the upper part of the Fig. The second stage is semi-supervised learning stage, as shown in the lower part of the figure.
    &lt;/figcaption&gt;&lt;/figure&gt;
</description>
    </item>
    
    <item>
      <title>Exploring Domain Knowledge for Facial Expression-Assisted Action Unit Activation Recognition</title>
      <link>https://ustc-ac.github.io/publication/dblp-journalstaffco-wang-pj-20/</link>
      <pubDate>Wed, 01 Jan 2020 00:00:00 +0000</pubDate>
      <guid>https://ustc-ac.github.io/publication/dblp-journalstaffco-wang-pj-20/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Knowledge-Augmented Multimodal Deep Regression Bayesian Networks for Emotion Video Tagging</title>
      <link>https://ustc-ac.github.io/publication/dblp-journalstmm-wang-hj-20/</link>
      <pubDate>Wed, 01 Jan 2020 00:00:00 +0000</pubDate>
      <guid>https://ustc-ac.github.io/publication/dblp-journalstmm-wang-hj-20/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Learning from Macro-expression: a Micro-expression Recognition Framework</title>
      <link>https://ustc-ac.github.io/publication/dblp-confmm-0012-wwc-20/</link>
      <pubDate>Wed, 01 Jan 2020 00:00:00 +0000</pubDate>
      <guid>https://ustc-ac.github.io/publication/dblp-confmm-0012-wwc-20/</guid>
      <description>&lt;p&gt;As one of the most important forms of psychological behaviors, micro-expression can reveal the real emotion. However, the existing labeled micro-expression samples are limited to train a high performance micro-expression classifier. Since micro-expression and macro-expression share some similarities in facial muscle movements and texture changes, in this paper we propose a micro-expression recognition framework that leverages macro-expression samples as guidance. Specifically, we first introduce two Expression Identity Disentangle Network, named MicroNet and MacroNet, as the feature extractor to disentangle expression-related features for micro and macro expression samples. Then MacroNet is fixed and used to guide the fine-tuning of MicroNet from both label and feature space. Adversarial learning strategy and triplet loss are added upon feature level between the MicroNet and MacroNet, so the MicroNet can efficiently capture the shared features of micro-expression and macro-expression samples. Loss inequality regularization is imposed to the label space to make the output of MicroNet converge to that of MicroNet. Comprehensive experiments on three public spontaneous micro-expression databases, i.e., SMIC, CASME2 and SAMM demonstrate the superiority of the proposed method.&lt;/p&gt;














&lt;figure  id=&#34;figure-fig-the-framework-of-our-micro-expression-recognition-model-first-we-pretrain-two-eidnets-with-micro-expression-and-macro-expression-databases-separately-named-micronet-and-macronet-secondly-macronet-is-fixed-and-used-to-guide-the-fine-tuning-of-micronet-from-both-label-and-feature-space-named-mtmnet&#34;&gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img alt=&#34;Fig. The framework of our micro-expression recognition model. First we pretrain two EIDNets with micro-expression and macro-expression databases separately, named MicroNet and MacroNet. Secondly, MacroNet is fixed and used to guide the fine-tuning of MicroNet from both label and feature space, named MTMNet.&#34; srcset=&#34;
               /publication/dblp-confmm-0012-wwc-20/featured_hucbe6b30a302d6c912ad5aaa64ac11de4_85027_f2b0e8b8233ae160b6b4596c5cec04fa.jpg 400w,
               /publication/dblp-confmm-0012-wwc-20/featured_hucbe6b30a302d6c912ad5aaa64ac11de4_85027_0b1bee0764bd1981a1b49b81caca145c.jpg 760w,
               /publication/dblp-confmm-0012-wwc-20/featured_hucbe6b30a302d6c912ad5aaa64ac11de4_85027_1200x1200_fit_q75_lanczos.jpg 1200w&#34;
               src=&#34;https://ustc-ac.github.io/publication/dblp-confmm-0012-wwc-20/featured_hucbe6b30a302d6c912ad5aaa64ac11de4_85027_f2b0e8b8233ae160b6b4596c5cec04fa.jpg&#34;
               width=&#34;760&#34;
               height=&#34;405&#34;
               loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;figcaption&gt;
      Fig. The framework of our micro-expression recognition model. First we pretrain two EIDNets with micro-expression and macro-expression databases separately, named MicroNet and MacroNet. Secondly, MacroNet is fixed and used to guide the fine-tuning of MicroNet from both label and feature space, named MTMNet.
    &lt;/figcaption&gt;&lt;/figure&gt;
</description>
    </item>
    
    <item>
      <title>Occluded Facial Expression Recognition with Step-Wise Assistance from Unpaired Non-Occluded Images</title>
      <link>https://ustc-ac.github.io/publication/dblp-confmm-0012-w-20/</link>
      <pubDate>Wed, 01 Jan 2020 00:00:00 +0000</pubDate>
      <guid>https://ustc-ac.github.io/publication/dblp-confmm-0012-w-20/</guid>
      <description>&lt;p&gt;Although facial expression recognition has improved in recent years, it is still very challenging to recognize expressions from occluded facial images in the wild. Due to the lack of large-scale facial expression datasets with diversity of the type and position of occlusions, it is very difficult to learn robust occluded expression classifier directly from limited occluded images. Considering facial images without occlusions usually provide more information for facial expression recognition compared to occluded facial images, we propose a step-wise learning strategy for occluded facial expression recognition that utilizes unpaired non-occluded images as guidance in the feature and label space. Specifically, we first measure the complexity of non-occluded data using distribution density in a feature space and split data into three subsets. In this way, the occluded expression classifier can be guided by basic samples first, and subsequently leverage more meaningful and discriminative samples. Complementary adversarial learning techniques are applied in the global-level and local-level feature space throughout, forcing the distribution of the occluded features to be close to the distribution of the non-occluded features. We also take the variability of the different images&amp;rsquo; transferability into account via adaptive classification loss. Loss inequality regularization is imposed in the label space to calibrate the output values of the occluded network. Experimental results show that our method improves performance on both synthesized occluded databases and realistic occluded databases.&lt;/p&gt;














&lt;figure  id=&#34;figure-fig-the-framework-of-the-proposed-approach-consists-of-an-occluded-network-fo-a-non-occluded-network-fc-k-local-level-feature-discriminator-dlk-k--1-2--k-and-a-global-feature-discriminator-dg&#34;&gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img alt=&#34;Fig. The framework of the proposed approach consists of an occluded network fo, a non-occluded network ​fc, K​ local-level feature discriminator ​Dlk (k = 1, 2, ..., K)​, and a global feature discriminator ​Dg&#34; srcset=&#34;
               /publication/dblp-confmm-0012-w-20/featured_hu82f0b636e3d7f43c2e4e1586bc541f2e_109475_a39a576ac3712e5dd318ceca017099cc.jpg 400w,
               /publication/dblp-confmm-0012-w-20/featured_hu82f0b636e3d7f43c2e4e1586bc541f2e_109475_9c4a277e5a6e2fde4662887abbf83019.jpg 760w,
               /publication/dblp-confmm-0012-w-20/featured_hu82f0b636e3d7f43c2e4e1586bc541f2e_109475_1200x1200_fit_q75_lanczos.jpg 1200w&#34;
               src=&#34;https://ustc-ac.github.io/publication/dblp-confmm-0012-w-20/featured_hu82f0b636e3d7f43c2e4e1586bc541f2e_109475_a39a576ac3712e5dd318ceca017099cc.jpg&#34;
               width=&#34;760&#34;
               height=&#34;274&#34;
               loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;figcaption&gt;
      Fig. The framework of the proposed approach consists of an occluded network f&lt;sub&gt;o&lt;/sub&gt;, a non-occluded network ​f&lt;sub&gt;c&lt;/sub&gt;, K​ local-level feature discriminator ​D&lt;sub&gt;l&lt;/sub&gt;&lt;sup&gt;k&lt;/sup&gt; (k = 1, 2, &amp;hellip;, K)​, and a global feature discriminator ​D&lt;sub&gt;g&lt;/sub&gt;
    &lt;/figcaption&gt;&lt;/figure&gt;
</description>
    </item>
    
    <item>
      <title>Pose-aware Adversarial Domain Adaptation for Personalized Facial Expression Recognition</title>
      <link>https://ustc-ac.github.io/publication/dblp-journalscorrabs-2007-05932/</link>
      <pubDate>Wed, 01 Jan 2020 00:00:00 +0000</pubDate>
      <guid>https://ustc-ac.github.io/publication/dblp-journalscorrabs-2007-05932/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Posed and Spontaneous Expression Distinction Using Latent Regression Bayesian Networks</title>
      <link>https://ustc-ac.github.io/publication/dblp-journalstomccap-wang-hj-20/</link>
      <pubDate>Wed, 01 Jan 2020 00:00:00 +0000</pubDate>
      <guid>https://ustc-ac.github.io/publication/dblp-journalstomccap-wang-hj-20/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Unpaired Multimodal Facial Expression Recognition</title>
      <link>https://ustc-ac.github.io/publication/dblp-confaccv-0012-w-20/</link>
      <pubDate>Wed, 01 Jan 2020 00:00:00 +0000</pubDate>
      <guid>https://ustc-ac.github.io/publication/dblp-confaccv-0012-w-20/</guid>
      <description>&lt;p&gt;Current works on multimodal facial expression recognition typically require paired visible and thermal facial images. Although visible cameras are readily available in our daily life, thermal cameras are expensive and less prevalent. It is costly to collect a large quantity of synchronous visible and thermal facial images. To tackle this paired training data bottleneck, we propose an unpaired multimodal facial expression recognition method, which makes full use of the massive number of unpaired visible and thermal images by utilizing thermal images to construct better image representations and classifiers for visible images during training. Specifically, two deep neural networks are trained from visible and thermal images to learn image representations and expression classifiers for two modalities. Then, an adversarial strategy is adopted to force statistical similarity between the learned visible and thermal representations, and to minimize the distribution mismatch between the predictions of the visible and thermal images. Through adversarial learning, the proposed method leverages thermal images to construct better image representations and classifiers for visible images during training, without the requirement of paired data. A decoder network is built upon the visible hidden features in order to preserve some inherent features of the visible view. We also take the variability of the different images’ transferability into account via adaptive classification loss. During testing, only visible images are required and the visible network is used. Thus, the proposed method is appropriate for real-world scenarios, since thermal imaging is rare in these instances. Experiments on two benchmark multimodal expression databases and three visible facial expression databases demonstrate the superiority of the proposed method compared to state-of-the-art methods.&lt;/p&gt;














&lt;figure  id=&#34;figure-fig-the-framework-of-the-proposed-unpaired-facial-expression-recognition-method&#34;&gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img alt=&#34;Fig. The framework of the proposed unpaired facial expression recognition method.&#34; srcset=&#34;
               /publication/dblp-confaccv-0012-w-20/featured_hu3699aaa94be3003441bff909318fbaba_230095_46950f1829ab083feeea48c0f692f8f7.png 400w,
               /publication/dblp-confaccv-0012-w-20/featured_hu3699aaa94be3003441bff909318fbaba_230095_32e70429dd06375f2df8c780e0a4351c.png 760w,
               /publication/dblp-confaccv-0012-w-20/featured_hu3699aaa94be3003441bff909318fbaba_230095_1200x1200_fit_lanczos_3.png 1200w&#34;
               src=&#34;https://ustc-ac.github.io/publication/dblp-confaccv-0012-w-20/featured_hu3699aaa94be3003441bff909318fbaba_230095_46950f1829ab083feeea48c0f692f8f7.png&#34;
               width=&#34;760&#34;
               height=&#34;360&#34;
               loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;figcaption&gt;
      Fig. The framework of the proposed unpaired facial expression recognition method.
    &lt;/figcaption&gt;&lt;/figure&gt;
</description>
    </item>
    
    <item>
      <title>Our facial expression recognition job is filmed in a CCTV-9 documentary</title>
      <link>https://ustc-ac.github.io/post/our-facial-expression-recognition-job-is-filmed-in-a-cctv-9-documentary/</link>
      <pubDate>Fri, 01 Nov 2019 09:25:52 +0000</pubDate>
      <guid>https://ustc-ac.github.io/post/our-facial-expression-recognition-job-is-filmed-in-a-cctv-9-documentary/</guid>
      <description>&lt;p&gt;A CCTV-9 documentary called &amp;ldquo;The Power of Science&amp;rdquo;, which filmed the facial expression recognition function of our robot Jiajia is online now!&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;http://tv.cctv.com/2019/11/04/VIDEtJcLKelpZLWUd2TrHZIZ191104.shtml?spm=C55924871139.PKgX4CXWWE68.0.0&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Link&lt;/a&gt; (17:44)&lt;/p&gt;
&lt;p&gt;On the 70th anniversary of the founding of New China, in order to showcase the achievements of China&amp;rsquo;s scientific and technological development and tell the story of Chinese science, the Chinese Academy of Sciences and China Central Television have cooperated in-depth to create a series of documentary &amp;ldquo;The Power of Science&amp;rdquo;. In the seventh episode, &amp;ldquo;The Biggest Temptation of the Intelligent Era-Bionic Robots&amp;rdquo;, a story of Jiajia is told, a service robot developed by the University of Science and Technology.&lt;/p&gt;
&lt;p&gt;















&lt;figure  &gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img alt=&#34;featured&#34; srcset=&#34;
               /post/our-facial-expression-recognition-job-is-filmed-in-a-cctv-9-documentary/featured_hud42958405cab0abbd778f69f7e22b2bd_119843_f28d4bac54ca9cae83a31f2a830d1483.jpg 400w,
               /post/our-facial-expression-recognition-job-is-filmed-in-a-cctv-9-documentary/featured_hud42958405cab0abbd778f69f7e22b2bd_119843_eabe97cccb517d3efdd8c01303f391b1.jpg 760w,
               /post/our-facial-expression-recognition-job-is-filmed-in-a-cctv-9-documentary/featured_hud42958405cab0abbd778f69f7e22b2bd_119843_1200x1200_fit_q75_lanczos.jpg 1200w&#34;
               src=&#34;https://ustc-ac.github.io/post/our-facial-expression-recognition-job-is-filmed-in-a-cctv-9-documentary/featured_hud42958405cab0abbd778f69f7e22b2bd_119843_f28d4bac54ca9cae83a31f2a830d1483.jpg&#34;
               width=&#34;760&#34;
               height=&#34;428&#34;
               loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;/figure&gt;
&lt;/p&gt;
&lt;h2 id=&#34;more-------------&#34;&gt;More &amp;mdash;&amp;mdash;&amp;mdash;&amp;mdash;&lt;/h2&gt;
&lt;h4 id=&#34;robot-kejia&#34;&gt;Robot KeJia&lt;/h4&gt;
&lt;p&gt;Able to respond to facial expressions with her own micro-level movements during conversations in both Chinese and English, Jiajia is the result of integrating technologies for cognitive modelling, semantic understanding, automated reasoning and planning, knowledge acquisition, kinematics and cloud robotics. These technologies have been studied in the Kejia Robotics project by Chen Xiaoping’s laboratory at USTC. The award-winning development team has received many international accolades, including first places at the popular RoboCup world championship, and the IJCAI Robotics competition.&lt;/p&gt;
&lt;p&gt;















&lt;figure  &gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img alt=&#34;kejia&#34; srcset=&#34;
               /post/our-facial-expression-recognition-job-is-filmed-in-a-cctv-9-documentary/kejia_huc644d72809a34ad6e17e51e82fce8e17_133855_350a41798f23d2ebf6618ee94fb2259b.jpg 400w,
               /post/our-facial-expression-recognition-job-is-filmed-in-a-cctv-9-documentary/kejia_huc644d72809a34ad6e17e51e82fce8e17_133855_d25d82f42118cbb02d496142bd0f1b3e.jpg 760w,
               /post/our-facial-expression-recognition-job-is-filmed-in-a-cctv-9-documentary/kejia_huc644d72809a34ad6e17e51e82fce8e17_133855_1200x1200_fit_q75_lanczos.jpg 1200w&#34;
               src=&#34;https://ustc-ac.github.io/post/our-facial-expression-recognition-job-is-filmed-in-a-cctv-9-documentary/kejia_huc644d72809a34ad6e17e51e82fce8e17_133855_350a41798f23d2ebf6618ee94fb2259b.jpg&#34;
               width=&#34;760&#34;
               height=&#34;508&#34;
               loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;/figure&gt;
&lt;/p&gt;
&lt;h4 id=&#34;reported-by-nature2016&#34;&gt;Reported by Nature(2016)&lt;/h4&gt;
&lt;p&gt;Jiajia was reported by Nature during the 60th anniversary of the University of Science and Technology of China&lt;/p&gt;
&lt;p&gt;















&lt;figure  &gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img alt=&#34;nature&#34; srcset=&#34;
               /post/our-facial-expression-recognition-job-is-filmed-in-a-cctv-9-documentary/nature_hu78fee13afca7fa82f62c2e75c78da747_77253_9b0b338214ebfcb9b3122f0e104e8aad.jpg 400w,
               /post/our-facial-expression-recognition-job-is-filmed-in-a-cctv-9-documentary/nature_hu78fee13afca7fa82f62c2e75c78da747_77253_433eddcea31eac45c7388d202c34aaa6.jpg 760w,
               /post/our-facial-expression-recognition-job-is-filmed-in-a-cctv-9-documentary/nature_hu78fee13afca7fa82f62c2e75c78da747_77253_1200x1200_fit_q75_lanczos.jpg 1200w&#34;
               src=&#34;https://ustc-ac.github.io/post/our-facial-expression-recognition-job-is-filmed-in-a-cctv-9-documentary/nature_hu78fee13afca7fa82f62c2e75c78da747_77253_9b0b338214ebfcb9b3122f0e104e8aad.jpg&#34;
               width=&#34;760&#34;
               height=&#34;489&#34;
               loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;/figure&gt;
&lt;/p&gt;
&lt;h4 id=&#34;praised-by-the-minister-of-foreign-affairs2017&#34;&gt;Praised by the Minister of Foreign Affairs(2017)&lt;/h4&gt;
&lt;p&gt;On the afternoon of April 11, the Anhui Global Promotion Event of the Ministry of Foreign Affairs with the theme of &amp;ldquo;An Open China: Splendid Anhui Welcomes the World&amp;rdquo; was held in the Blue Hall of the Ministry of Foreign Affairs. At 16:20 in the afternoon, Foreign Minister Yi Wang and the invited envoys to China, accompanied by Secretary of the Provincial Party Committee Jinbin Li and Governor Guoying Li, walked into the comprehensive exhibition area. Jiajia, taking the lead in the  exhibition area, was praised by Minister Wang Yi.&lt;/p&gt;
&lt;p&gt;















&lt;figure  &gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img alt=&#34;minister&#34; srcset=&#34;
               /post/our-facial-expression-recognition-job-is-filmed-in-a-cctv-9-documentary/minister_hu14b22c93fc41905bddf176c6e7d8a93e_52235_fe5980a90ecef727d9dc611314e3fb0d.jpg 400w,
               /post/our-facial-expression-recognition-job-is-filmed-in-a-cctv-9-documentary/minister_hu14b22c93fc41905bddf176c6e7d8a93e_52235_610c58d3e327c2159ee896405bc84c2d.jpg 760w,
               /post/our-facial-expression-recognition-job-is-filmed-in-a-cctv-9-documentary/minister_hu14b22c93fc41905bddf176c6e7d8a93e_52235_1200x1200_fit_q75_lanczos.jpg 1200w&#34;
               src=&#34;https://ustc-ac.github.io/post/our-facial-expression-recognition-job-is-filmed-in-a-cctv-9-documentary/minister_hu14b22c93fc41905bddf176c6e7d8a93e_52235_fe5980a90ecef727d9dc611314e3fb0d.jpg&#34;
               width=&#34;554&#34;
               height=&#34;369&#34;
               loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;/figure&gt;
&lt;/p&gt;
&lt;h4 id=&#34;cameron-and-jia-jia2017&#34;&gt;Cameron and Jia Jia(2017)&lt;/h4&gt;
&lt;p&gt;At the opening of the 17th United Bank of Switzerland (UBS) Greater China Conference, held in Shanghai, Jiajia had a spectacular conversation with former British Prime Minister Cameron.&lt;/p&gt;
&lt;p&gt;















&lt;figure  &gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img alt=&#34;cameron&#34; srcset=&#34;
               /post/our-facial-expression-recognition-job-is-filmed-in-a-cctv-9-documentary/cameron_hu69778a0b52edfba6cb4adb4f245ac760_167699_9ed4fb74fef830520db002ebb34fe552.jpg 400w,
               /post/our-facial-expression-recognition-job-is-filmed-in-a-cctv-9-documentary/cameron_hu69778a0b52edfba6cb4adb4f245ac760_167699_975bf43d91400451678f74c2c680ab52.jpg 760w,
               /post/our-facial-expression-recognition-job-is-filmed-in-a-cctv-9-documentary/cameron_hu69778a0b52edfba6cb4adb4f245ac760_167699_1200x1200_fit_q75_lanczos.jpg 1200w&#34;
               src=&#34;https://ustc-ac.github.io/post/our-facial-expression-recognition-job-is-filmed-in-a-cctv-9-documentary/cameron_hu69778a0b52edfba6cb4adb4f245ac760_167699_9ed4fb74fef830520db002ebb34fe552.jpg&#34;
               width=&#34;640&#34;
               height=&#34;427&#34;
               loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;/figure&gt;
&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>Affective Computing for Large-Scale Heterogeneous Multimedia Data: A Survey</title>
      <link>https://ustc-ac.github.io/publication/dblp-journalscorrabs-1911-05609/</link>
      <pubDate>Tue, 01 Jan 2019 00:00:00 +0000</pubDate>
      <guid>https://ustc-ac.github.io/publication/dblp-journalscorrabs-1911-05609/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Capturing Feature and Label Relations Simultaneously for Multiple Facial Action Unit Recognition</title>
      <link>https://ustc-ac.github.io/publication/dblp-journalstaffco-wang-wpj-19/</link>
      <pubDate>Tue, 01 Jan 2019 00:00:00 +0000</pubDate>
      <guid>https://ustc-ac.github.io/publication/dblp-journalstaffco-wang-wpj-19/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Capturing Spatial and Temporal Patterns for Facial Landmark Tracking through Adversarial Learning</title>
      <link>https://ustc-ac.github.io/publication/dblp-confijcai-yin-wpcp-19/</link>
      <pubDate>Tue, 01 Jan 2019 00:00:00 +0000</pubDate>
      <guid>https://ustc-ac.github.io/publication/dblp-confijcai-yin-wpcp-19/</guid>
      <description>&lt;p&gt;The spatial and temporal patterns inherent in facial feature points are crucial for facial landmark tracking, but have not been thoroughly explored yet. In this paper, we propose a novel deep adversarial framework to explore the shape and temporal dependencies from both appearance level and target label level. The proposed deep adversarial framework consists of a deep landmark tracker and a discriminator. The deep landmark tracker is composed of a stacked Hourglass network as well as a convolutional neural network and a long short-term memory network, and thus implicitly capture spatial and temporal patterns from facial appearance for facial landmark tracking. The discriminator is adopted to distinguish the tracked facial landmarks from ground truth ones. It explicitly models shape and temporal dependencies existing in ground truth facial landmarks through another convolutional neural network and another long short-term memory network. The deep landmark tracker and the discriminator compete with each other. Through adversarial learning, the proposed deep adversarial landmark tracking approach leverages inherent spatial and temporal patterns to facilitate facial landmark tracking from both appearance level and target label level. Experimental results on two benchmark databases demonstrate the superiority of the proposed approach to state-of-the-art work.&lt;/p&gt;














&lt;figure  id=&#34;figure-fig-the-framework-of-the-proposed-approach-consists-of-two-deep-neural-networks-ie-a-tracker-and-a-discriminator-the-tracker-is-used-to-track-landmarks-from-a-facial-video-the-discriminator-is-introduced-to-distinguish-the-predicted-landmark-positions-from-the-ground-truth-ones-the-tracker-tries-to-confuse-the-discriminator-by-predicting-landmark-positions-with-joint-distributions-that-are-close-to-the-ground-truth-ones-through-adversarial-learning-the-inherent-spatial-and-temporal-dependencies-of-a-facial-sequence-are-captured-from-both-appearance-level-and-target-level-for-landmark-tracking-see-text-for-details&#34;&gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img alt=&#34;Fig. The framework of the proposed approach consists of two deep neural networks, i.e., a tracker and a discriminator. The tracker is used to track landmarks from a facial video. The discriminator is introduced to distinguish the predicted landmark positions from the ground truth ones. The tracker tries to confuse the discriminator by predicting landmark positions with joint distributions that are close to the ground truth ones. Through adversarial learning, the inherent spatial and temporal dependencies of a facial sequence are captured from both appearance level and target level for landmark tracking. See text for details.&#34; srcset=&#34;
               /publication/dblp-confijcai-yin-wpcp-19/featured_hu83191d9c4948d0db84b838d1cd056b88_246371_a1a25f4b20008f3a0b9b4cfdd267d247.jpg 400w,
               /publication/dblp-confijcai-yin-wpcp-19/featured_hu83191d9c4948d0db84b838d1cd056b88_246371_a56dd2a610ca26e0e1f5ae708692b7f9.jpg 760w,
               /publication/dblp-confijcai-yin-wpcp-19/featured_hu83191d9c4948d0db84b838d1cd056b88_246371_1200x1200_fit_q75_lanczos.jpg 1200w&#34;
               src=&#34;https://ustc-ac.github.io/publication/dblp-confijcai-yin-wpcp-19/featured_hu83191d9c4948d0db84b838d1cd056b88_246371_a1a25f4b20008f3a0b9b4cfdd267d247.jpg&#34;
               width=&#34;760&#34;
               height=&#34;268&#34;
               loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;figcaption&gt;
      Fig. The framework of the proposed approach consists of two deep neural networks, i.e., a tracker and a discriminator. The tracker is used to track landmarks from a facial video. The discriminator is introduced to distinguish the predicted landmark positions from the ground truth ones. The tracker tries to confuse the discriminator by predicting landmark positions with joint distributions that are close to the ground truth ones. Through adversarial learning, the inherent spatial and temporal dependencies of a facial sequence are captured from both appearance level and target level for landmark tracking. See text for details.
    &lt;/figcaption&gt;&lt;/figure&gt;
</description>
    </item>
    
    <item>
      <title>Content-Based Video Emotion Tagging Augmented by Users&#39; Multiple Physiological Responses</title>
      <link>https://ustc-ac.github.io/publication/dblp-journalstaffco-wang-cj-19/</link>
      <pubDate>Tue, 01 Jan 2019 00:00:00 +0000</pubDate>
      <guid>https://ustc-ac.github.io/publication/dblp-journalstaffco-wang-cj-19/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Dual Semi-Supervised Learning for Facial Action Unit Recognition</title>
      <link>https://ustc-ac.github.io/publication/dblp-confaaai-peng-w-19/</link>
      <pubDate>Tue, 01 Jan 2019 00:00:00 +0000</pubDate>
      <guid>https://ustc-ac.github.io/publication/dblp-confaaai-peng-w-19/</guid>
      <description>&lt;p&gt;Current works on facial action unit (AU) recognition typically require fully AU-labeled training samples. To reduce the reliance on time-consuming manual AU annotations, we propose a novel semi-supervised AU recognition method leveraging two kinds of readily available auxiliary information.The method leverages the dependencies between AUs and expressions as well as the dependencies among AUs, which are caused by facial anatomy and therefore embedded in all facial images, independent on their AU annotation status. The other auxiliary information is facial image synthesis given AUs, the dual task of AU recognition from facial images, and therefore has intrinsic probabilistic connections with AU recognition, regardless of AU annotations. Specifically, we propose a dual semi-supervised generative adversarial network for AU recognition from partially AU-labeled and fully expression-labeled facial images. The proposed network consists of an AU classifier C, an image generator G, and a discriminator D. In addition to minimize the supervised losses of the AU classifier and the face generator for labeled training data,we explore the probabilistic duality between the tasks using adversary learning to force the convergence of the face-AU-expression tuples generated from the AU classifier and the face generator, and the ground-truth distribution in labeled data for all training data. This joint distribution also includes the inherent AU dependencies. Furthermore, we reconstruct the facial image using the output of the AU classifier as the input of the face generator, and create AU labels by feeding the output of the face generator to the AU classifier. We minimize reconstruction losses for all training data, thus exploiting the informative feedback provided by the dual tasks. Within-database and cross-database experiments on three benchmark databases demonstrate the superiority of our method in both AU recognition and face synthesis compared to state-of-the-art works.&lt;/p&gt;














&lt;figure  id=&#34;figure-fig-the-framework-of-the-proposed-dual-semi-supervised-gan-consisting-of-three-modules-a-discriminator-d-a-classifier-c-and-a-generator-g&#34;&gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img alt=&#34;Fig. The framework of the proposed dual semi-supervised GAN, consisting of three modules: a discriminator D, a classifier C, and a generator G.&#34; srcset=&#34;
               /publication/dblp-confaaai-peng-w-19/featured_hudd7daa1dca088bf83220ca908efacfc0_71773_ab3401311a2c9492477c955c1bac484b.JPG 400w,
               /publication/dblp-confaaai-peng-w-19/featured_hudd7daa1dca088bf83220ca908efacfc0_71773_c2fe2348982c07e727263e1acbc9ab88.JPG 760w,
               /publication/dblp-confaaai-peng-w-19/featured_hudd7daa1dca088bf83220ca908efacfc0_71773_1200x1200_fit_q75_lanczos.JPG 1200w&#34;
               src=&#34;https://ustc-ac.github.io/publication/dblp-confaaai-peng-w-19/featured_hudd7daa1dca088bf83220ca908efacfc0_71773_ab3401311a2c9492477c955c1bac484b.JPG&#34;
               width=&#34;760&#34;
               height=&#34;314&#34;
               loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;figcaption&gt;
      Fig. The framework of the proposed dual semi-supervised GAN, consisting of three modules: a discriminator D, a classifier C, and a generator G.
    &lt;/figcaption&gt;&lt;/figure&gt;
</description>
    </item>
    
    <item>
      <title>Facial Action Unit Recognition and Intensity Estimation Enhanced Through Label Dependencies</title>
      <link>https://ustc-ac.github.io/publication/dblp-journalstip-wang-hj-19/</link>
      <pubDate>Tue, 01 Jan 2019 00:00:00 +0000</pubDate>
      <guid>https://ustc-ac.github.io/publication/dblp-journalstip-wang-hj-19/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Identity- and Pose-Robust Facial Expression Recognition through Adversarial Feature Learning</title>
      <link>https://ustc-ac.github.io/publication/dblp-confmm-wang-wl-19/</link>
      <pubDate>Tue, 01 Jan 2019 00:00:00 +0000</pubDate>
      <guid>https://ustc-ac.github.io/publication/dblp-confmm-wang-wl-19/</guid>
      <description>&lt;p&gt;Existing facial expression recognition methods either focus on pose variations or identity bias, but not both simultaneously. This paper proposes an adversarial feature learning method to address both of these issues. Specifically, the proposed method consists of five components: an encoder, an expression classifier, a pose discriminator, a subject discriminator, and a generator. An encoder extracts feature representations, and an expression classifier tries to perform facial expression recognition using the extracted feature representations. The encoder and the expression classifier are trained collaboratively, so that the extracted feature representations are discriminative for expression recognition. A pose discriminator and a subject discriminator classify the pose and the subject from the extracted feature representations respectively. They are trained adversarially with the encoder. Thus, the extracted feature representations are robust to poses and subjects. A generator reconstructs facial images to further favor the feature representations. Experiments on five benchmark databases demonstrate the superiority of the proposed method to state-of-the-art work.&lt;/p&gt;














&lt;figure  id=&#34;figure-fig-the-structure-of-the-proposed-method-it-consists-of-an-encoder-e--an-expression-classifier-dc--a-pose-discriminator-dp--a-subject-discriminator-dsand-a-generator-g-&#34;&gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img alt=&#34;Fig. The structure of the proposed method. It consists of an encoder E , an expression classifier Dc , a pose discriminator Dp , a subject discriminator Ds ,and a generator G .&#34; srcset=&#34;
               /publication/dblp-confmm-wang-wl-19/featured_hu6a8143bd29530d43f3d1f85b0d638a7e_147884_35f4bf217e4a06d8325c371a0ddf8b16.JPG 400w,
               /publication/dblp-confmm-wang-wl-19/featured_hu6a8143bd29530d43f3d1f85b0d638a7e_147884_4df95c27df35fe431c16fd0386aebc1a.JPG 760w,
               /publication/dblp-confmm-wang-wl-19/featured_hu6a8143bd29530d43f3d1f85b0d638a7e_147884_1200x1200_fit_q75_lanczos.JPG 1200w&#34;
               src=&#34;https://ustc-ac.github.io/publication/dblp-confmm-wang-wl-19/featured_hu6a8143bd29530d43f3d1f85b0d638a7e_147884_35f4bf217e4a06d8325c371a0ddf8b16.JPG&#34;
               width=&#34;760&#34;
               height=&#34;469&#34;
               loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;figcaption&gt;
      Fig. The structure of the proposed method. It consists of an encoder &lt;i&gt;E&lt;/i&gt; , an expression classifier &lt;i&gt;D&lt;sub&gt;c&lt;/sub&gt;&lt;/i&gt; , a pose discriminator &lt;i&gt;D&lt;sub&gt;p&lt;/sub&gt;&lt;/i&gt; , a subject discriminator &lt;i&gt;D&lt;sub&gt;s&lt;/sub&gt;&lt;/i&gt; ,and a generator &lt;i&gt;G&lt;/i&gt; .
    &lt;/figcaption&gt;&lt;/figure&gt;
</description>
    </item>
    
    <item>
      <title>Image Aesthetic Assessment Assisted by Attributes through Adversarial Learning</title>
      <link>https://ustc-ac.github.io/publication/dblp-confaaai-pan-wj-19/</link>
      <pubDate>Tue, 01 Jan 2019 00:00:00 +0000</pubDate>
      <guid>https://ustc-ac.github.io/publication/dblp-confaaai-pan-wj-19/</guid>
      <description>&lt;p&gt;The inherent connections among aesthetic attributes and aesthetics are crucial for image aesthetic assessment, but have not been thoroughly explored yet. In this paper, we propose a novel image aesthetic assessment assisted by attributes through both representation-level and label-level. The attributes are used as privileged information, which is only required during training. Specifically, we first propose a multitask deep convolutional rating network to learn the aesthetic score and attributes simultaneously. The attributes are explored to construct better feature representations for aesthetic assessment through multi-task learning. After that, we introduce a discriminator to distinguish the predicted attributes and aesthetics of the multi-task deep network from the ground truth label distribution embedded in the training data. The multi-task deep network wants to output aesthetic score and attributes as close to the ground truth labels as possible. Thus the deep network and the discriminator compete with each other. Through adversarial learning, the attributes are explored to enforce the distribution of the predicted attributes and aesthetics to converge to the ground truth label distribution. Experimental results on two benchmark databases demonstrate the superiority of the proposed method to state of the art work.&lt;/p&gt;














&lt;figure  id=&#34;figure-fig-the-framework-of-the-proposed-method&#34;&gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img alt=&#34;Fig. The framework of the proposed method&#34; srcset=&#34;
               /publication/dblp-confaaai-pan-wj-19/featured_hu2a714477b00e9d9445d21b555b800bff_206782_4e387916ae9a39d9b4f079e98792d7fa.PNG 400w,
               /publication/dblp-confaaai-pan-wj-19/featured_hu2a714477b00e9d9445d21b555b800bff_206782_59f5b33a19657e3ab31ea7ffc0c860ae.PNG 760w,
               /publication/dblp-confaaai-pan-wj-19/featured_hu2a714477b00e9d9445d21b555b800bff_206782_1200x1200_fit_lanczos_3.PNG 1200w&#34;
               src=&#34;https://ustc-ac.github.io/publication/dblp-confaaai-pan-wj-19/featured_hu2a714477b00e9d9445d21b555b800bff_206782_4e387916ae9a39d9b4f079e98792d7fa.PNG&#34;
               width=&#34;760&#34;
               height=&#34;475&#34;
               loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;figcaption&gt;
      Fig. The framework of the proposed method
    &lt;/figcaption&gt;&lt;/figure&gt;
</description>
    </item>
    
    <item>
      <title>Integrating Facial Images, Speeches and Time for Empathy Prediction</title>
      <link>https://ustc-ac.github.io/publication/dblp-conffgr-yin-fwwdw-19/</link>
      <pubDate>Tue, 01 Jan 2019 00:00:00 +0000</pubDate>
      <guid>https://ustc-ac.github.io/publication/dblp-conffgr-yin-fwwdw-19/</guid>
      <description>&lt;p&gt;We propose a multi-modal method for the OneMinute Empathy Prediction competition. First, we use bottleneck residual and fully-connected network to encode facial images and speeches of the listener. Second, we propose to use the current time stage as a temporal feature and encoded it into the proposed multi-modal network. Third, we select a subset training data based on its performance of empathy prediction on the validation data. Experimental results on the testing set show that the proposed method outperforms the baseline methods significantly according to the CCC metric (0.14 vs 0.06).














&lt;figure  id=&#34;figure-fig-model-architecturethe-inputs-of-the-proposed-multi-modal-deep-network-are-facial-images-audio-signals-and-time-stamps-specifically-we-extract-facial-images-of-the-listener-in-each-frame-through-opencv-and-then-reshape-the-size-of-facial-images-to-120-120-the-preprocessed-facial-images-are-fed-into-a-network-with-one-convolution-layer-and-six-sequential-bottleneck-residual-modulessee-text-for-details&#34;&gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img alt=&#34;Fig. Model architecture.The inputs of the proposed multi-modal deep network are facial images, audio signals and time stamps. Specifically, we extract facial images of the listener in each frame through opencv, and then reshape the size of facial images to (120, 120). The preprocessed facial images are fed into a network with one convolution layer and six sequential bottleneck residual modules.See text for details.&#34; srcset=&#34;
               /publication/dblp-conffgr-yin-fwwdw-19/network_hu9c2d1df41939a3017ee182bef8e8ca87_57891_3ed075402035858ba13ed0aef6a9a279.jpg 400w,
               /publication/dblp-conffgr-yin-fwwdw-19/network_hu9c2d1df41939a3017ee182bef8e8ca87_57891_56529168d08d4d1cfdc7834a79a338ec.jpg 760w,
               /publication/dblp-conffgr-yin-fwwdw-19/network_hu9c2d1df41939a3017ee182bef8e8ca87_57891_1200x1200_fit_q75_lanczos.jpg 1200w&#34;
               src=&#34;https://ustc-ac.github.io/publication/dblp-conffgr-yin-fwwdw-19/network_hu9c2d1df41939a3017ee182bef8e8ca87_57891_3ed075402035858ba13ed0aef6a9a279.jpg&#34;
               width=&#34;485&#34;
               height=&#34;755&#34;
               loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;figcaption&gt;
      Fig. Model architecture.The inputs of the proposed multi-modal deep network are facial images, audio signals and time stamps. Specifically, we extract facial images of the listener in each frame through opencv, and then reshape the size of facial images to (120, 120). The preprocessed facial images are fed into a network with one convolution layer and six sequential bottleneck residual modules.See text for details.
    &lt;/figcaption&gt;&lt;/figure&gt;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>KDSL: a Knowledge-Driven Supervised Learning Framework for Word Sense Disambiguation</title>
      <link>https://ustc-ac.github.io/publication/dblp-confijcnn-yin-zlwjcw-19/</link>
      <pubDate>Tue, 01 Jan 2019 00:00:00 +0000</pubDate>
      <guid>https://ustc-ac.github.io/publication/dblp-confijcnn-yin-zlwjcw-19/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Multiple Face Analyses through Adversarial Learning</title>
      <link>https://ustc-ac.github.io/publication/dblp-journalscorrabs-1911-07846/</link>
      <pubDate>Tue, 01 Jan 2019 00:00:00 +0000</pubDate>
      <guid>https://ustc-ac.github.io/publication/dblp-journalscorrabs-1911-07846/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Occluded Facial Expression Recognition Enhanced through Privileged Information</title>
      <link>https://ustc-ac.github.io/publication/dblp-confmm-pan-wx-19/</link>
      <pubDate>Tue, 01 Jan 2019 00:00:00 +0000</pubDate>
      <guid>https://ustc-ac.github.io/publication/dblp-confmm-pan-wx-19/</guid>
      <description>&lt;p&gt;In this paper, we propose a novel approach of occluded facial expression recognition under the help of non-occluded facial images. The non-occluded facial images are used as privileged information, which is only required during training, but not required during testing. Specifically, two deep neural networks are first trained from occluded and non-occluded facial images respectively. Then the non-occluded network is fixed and is used to guide the fine-tuning of the occluded network from both label space and feature space. Similarity constraint and loss inequality regularization are imposed to the label space to make the output of occluded network converge to that of the non-occluded network. Adversarial leaning is adopted to force the distribution of the learned features from occluded facial images to be close to that from non-occluded facial images. Furthermore, a decoder network is employed to reconstruct the non-occluded facial images from occluded features. Under the guidance of non-occluded facial images, the occluded network is expected to learn better features and classifier during training. Experiments on the benchmark databases with both synthesized and realistic occluded facial images demonstrate the superiority of the proposed method to state-of-the-art.&lt;/p&gt;














&lt;figure  id=&#34;figure-fig-the-framework-of-proposed-facial-expression-recognition-with-occlusions&#34;&gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img alt=&#34;Fig. The framework of proposed facial expression recognition with occlusions&#34; srcset=&#34;
               /publication/dblp-confmm-pan-wx-19/featured_hucf65a7887d8ff00a6d7a1a8fcf59d04f_218950_804330664a43426320a8d1cc0818d0a2.png 400w,
               /publication/dblp-confmm-pan-wx-19/featured_hucf65a7887d8ff00a6d7a1a8fcf59d04f_218950_9bc960654f17587aae4a7e9a80742eb0.png 760w,
               /publication/dblp-confmm-pan-wx-19/featured_hucf65a7887d8ff00a6d7a1a8fcf59d04f_218950_1200x1200_fit_lanczos_3.png 1200w&#34;
               src=&#34;https://ustc-ac.github.io/publication/dblp-confmm-pan-wx-19/featured_hucf65a7887d8ff00a6d7a1a8fcf59d04f_218950_804330664a43426320a8d1cc0818d0a2.png&#34;
               width=&#34;760&#34;
               height=&#34;491&#34;
               loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;figcaption&gt;
      Fig. The framework of proposed facial expression recognition with occlusions
    &lt;/figcaption&gt;&lt;/figure&gt;
</description>
    </item>
    
    <item>
      <title>Weakly Supervised Dual Learning for Facial Action Unit Recognition</title>
      <link>https://ustc-ac.github.io/publication/dblp-journalstmm-wang-p-19/</link>
      <pubDate>Tue, 01 Jan 2019 00:00:00 +0000</pubDate>
      <guid>https://ustc-ac.github.io/publication/dblp-journalstmm-wang-p-19/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Facial Action Unit Recognition Augmented by Their Dependencies</title>
      <link>https://ustc-ac.github.io/publication/dblp-conffgr-hao-wpj-18/</link>
      <pubDate>Mon, 01 Jan 2018 00:00:00 +0000</pubDate>
      <guid>https://ustc-ac.github.io/publication/dblp-conffgr-hao-wpj-18/</guid>
      <description>&lt;p&gt;Due to the underlying anatomic mechanism that govern facial muscular interactions, there exist inherent de-pendencies between facial action units (AU). Such dependen-cies carry crucial information for AU recognition, yet have not been thoroughly exploited. Therefore, in this paper, we propose a novel AU recognition method with a three-layer hybrid Bayesian network, whose top two layers consist of a latent regression Bayesian network (LRBN), and the bottom two layers are Bayesian networks. The LRBN is a directed graphical model consisting of one latent layer and one visible layer. Specifically, the visible nodes of LRBN represent the ground-truth AU labels. Due to the “explaining away” effect in Bayesian networks, LRBN is able to capture both the depen-dencies among the latent variables given the observation and the dependencies among visible variables. Such dependencie ssuccessfully and faithfully represent relations among multiple AUs. The bottom two layers are two node Bayesian networks, connecting the ground truth AU labels and their measurements.Efficient learning and inference algorithms are also proposed.Furthermore, we extend the proposed hybrid Bayesian network model for facial expression-assisted AU recognition, since AUrelations are influenced by expressions. By introducing facial expression nodes in the middle visible layer, facial expressions,which are only required during training, facilitate the estima-tion of label dependencies among AUs. Experimental results on three benchmark databases, i.e. the CK+ database, the SEMAINE database, and the BP4D database, demonstrate that the proposed approaches can successfully capture complex AU relationships, and the expression labels available only during training are benefit for AU recognition during testing.&lt;/p&gt;














&lt;figure  id=&#34;figure-fig-the-proposed-au-recognition-through-au-relation-modeling&#34;&gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img alt=&#34;Fig. The proposed AU recognition through AU-relation modeling&#34; srcset=&#34;
               /publication/dblp-conffgr-hao-wpj-18/featured_hu186a5b8e8dbef9b8cf16ed6628856e29_401689_33d9177604e731c05ae08715c5f01605.jpg 400w,
               /publication/dblp-conffgr-hao-wpj-18/featured_hu186a5b8e8dbef9b8cf16ed6628856e29_401689_ba6243d78c78efbb864395b00b0b72ca.jpg 760w,
               /publication/dblp-conffgr-hao-wpj-18/featured_hu186a5b8e8dbef9b8cf16ed6628856e29_401689_1200x1200_fit_q75_lanczos.jpg 1200w&#34;
               src=&#34;https://ustc-ac.github.io/publication/dblp-conffgr-hao-wpj-18/featured_hu186a5b8e8dbef9b8cf16ed6628856e29_401689_33d9177604e731c05ae08715c5f01605.jpg&#34;
               width=&#34;760&#34;
               height=&#34;380&#34;
               loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;figcaption&gt;
      Fig. The proposed AU recognition through AU-relation modeling
    &lt;/figcaption&gt;&lt;/figure&gt;
</description>
    </item>
    
    <item>
      <title>Facial Expression Recognition Enhanced by Thermal Images through Adversarial Learning</title>
      <link>https://ustc-ac.github.io/publication/dblp-confmm-pan-w-18/</link>
      <pubDate>Mon, 01 Jan 2018 00:00:00 +0000</pubDate>
      <guid>https://ustc-ac.github.io/publication/dblp-confmm-pan-w-18/</guid>
      <description>&lt;p&gt;Currently, fusing visible and thermal images for facial expression recognition requires two modalities during both training and testing. Visible cameras are commonly used in real-life applications, and thermal cameras are typically only available in lab situations due to their high price. Thermal imaging for facial expression recognition is not frequently used in real-world situations. To address this, we propose a novel thermally enhanced facial expression recognition method which uses thermal images as privileged information to construct better visible feature representation and improved classifiers by incorporating adversarial learning and similarity constraints during training. Specifically, we train two deep neural networks from visible images and thermal images. We impose adversarial loss to enforce statistical similarity between the learned representations of two modalities, and a similarity constraint to regulate the mapping functions from visible and thermal representation to expressions. Thus, thermal images are leveraged to simultaneously improve visible feature representation and classification during training. To mimic real-world scenarios, only visible images are available during testing. We further extend the proposed expression recognition method for partially unpaired data to explore thermal images&amp;rsquo; supplementary role in visible facial expression recognition when visible images and thermal images are not synchronously recorded. Experimental results on the MAHNOB Laughter database demonstrate that our proposed method can effectively regularize visible representation and expression classifiers with the help of thermal images, achieving state-of-the-art recognition performance.&lt;/p&gt;














&lt;figure  id=&#34;figure-fig-the-framework-of-our-proposed-method-for-facial-expression-recognition&#34;&gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img alt=&#34;Fig. The framework of our proposed method for facial expression recognition.&#34; srcset=&#34;
               /publication/dblp-confmm-pan-w-18/featured_hubbc45ea9bf79dd9eb975f82036f90e3d_46195_a23dec5de33bab404eca82a58b7c5a2b.jpg 400w,
               /publication/dblp-confmm-pan-w-18/featured_hubbc45ea9bf79dd9eb975f82036f90e3d_46195_4c350ae5a8ecb1aca8ab669937a70122.jpg 760w,
               /publication/dblp-confmm-pan-w-18/featured_hubbc45ea9bf79dd9eb975f82036f90e3d_46195_1200x1200_fit_q75_lanczos.jpg 1200w&#34;
               src=&#34;https://ustc-ac.github.io/publication/dblp-confmm-pan-w-18/featured_hubbc45ea9bf79dd9eb975f82036f90e3d_46195_a23dec5de33bab404eca82a58b7c5a2b.jpg&#34;
               width=&#34;760&#34;
               height=&#34;496&#34;
               loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;figcaption&gt;
      Fig. The framework of our proposed method for facial expression recognition.
    &lt;/figcaption&gt;&lt;/figure&gt;
</description>
    </item>
    
    <item>
      <title>Learning with privileged information for multi-Label classification</title>
      <link>https://ustc-ac.github.io/publication/dblp-journalspr-wang-ccs-18/</link>
      <pubDate>Mon, 01 Jan 2018 00:00:00 +0000</pubDate>
      <guid>https://ustc-ac.github.io/publication/dblp-journalspr-wang-ccs-18/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Personalized Multiple Facial Action Unit Recognition through Generative Adversarial Recognition Network</title>
      <link>https://ustc-ac.github.io/publication/dblp-confmm-wang-w-18/</link>
      <pubDate>Mon, 01 Jan 2018 00:00:00 +0000</pubDate>
      <guid>https://ustc-ac.github.io/publication/dblp-confmm-wang-w-18/</guid>
      <description>&lt;p&gt;Personalized facial action unit (AU) recognition is challenging due to subject-dependent facial behavior. This paper proposes a method to recognize personalized multiple facial AUs through a novel generative adversarial network, which adapts the distribution of source domain facial images to that of target domain facial images and detects multiple AUs by leveraging AU dependencies. Specifically, we use a generative adversarial network to generate synthetic images from source domain; the synthetic images have a similar appearance to the target subject and retain the AU patterns of the source images. We simultaneously leverage AU dependencies to train a multiple AU classifier. Experimental results on three benchmark databases demonstrate that the proposed method can successfully realize unsupervised domain adaptation for individual AU detection, and thus outperforms state-of-the-art AU detection methods.&lt;/p&gt;














&lt;figure  id=&#34;figure-fig-our-proposed-architecture-includes-a-generator-a-discriminator-and-a-classifier-the-generator-g-generates-an-image-conditioned-on-a-source-image-the-discriminator-d-discriminates-between-generated-and-target-images-the-classifier-r-assigns-au-labels-to-an-image&#34;&gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img alt=&#34;Fig. Our proposed architecture includes a generator, a discriminator and a classifier. The generator G generates an image conditioned on a source image. The discriminator D discriminates between generated and target images. The classifier R assigns AU labels to an image.&#34; srcset=&#34;
               /publication/dblp-confmm-wang-w-18/featured_hu2b619861a230c980126dcce3eefea7b7_166103_ccd5082ef9a156d22656b435b79e6c77.png 400w,
               /publication/dblp-confmm-wang-w-18/featured_hu2b619861a230c980126dcce3eefea7b7_166103_3e4acf7eae93e6a0a1220ced031d9066.png 760w,
               /publication/dblp-confmm-wang-w-18/featured_hu2b619861a230c980126dcce3eefea7b7_166103_1200x1200_fit_lanczos_3.png 1200w&#34;
               src=&#34;https://ustc-ac.github.io/publication/dblp-confmm-wang-w-18/featured_hu2b619861a230c980126dcce3eefea7b7_166103_ccd5082ef9a156d22656b435b79e6c77.png&#34;
               width=&#34;760&#34;
               height=&#34;332&#34;
               loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;figcaption&gt;
      Fig. Our proposed architecture includes a generator, a discriminator and a classifier. The generator G generates an image conditioned on a source image. The discriminator D discriminates between generated and target images. The classifier R assigns AU labels to an image.
    &lt;/figcaption&gt;&lt;/figure&gt;
</description>
    </item>
    
    <item>
      <title>Thermal Augmented Expression Recognition</title>
      <link>https://ustc-ac.github.io/publication/dblp-journalstcyb-wang-pcj-18/</link>
      <pubDate>Mon, 01 Jan 2018 00:00:00 +0000</pubDate>
      <guid>https://ustc-ac.github.io/publication/dblp-journalstcyb-wang-pcj-18/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Weakly Supervised Facial Action Unit Recognition Through Adversarial Training</title>
      <link>https://ustc-ac.github.io/publication/dblp-confcvpr-peng-w-18/</link>
      <pubDate>Mon, 01 Jan 2018 00:00:00 +0000</pubDate>
      <guid>https://ustc-ac.github.io/publication/dblp-confcvpr-peng-w-18/</guid>
      <description>&lt;p&gt;Current works on facial action unit (AU) recognition typically require fully AU-annotated facial images for supervised AU classifier training. AU annotation is a timeconsuming, expensive, and error-prone process. While AUs are hard to annotate, facial expression is relatively easy to label. Furthermore, there exist strong probabilistic dependencies between expressions and AUs as well as dependencies among AUs. Such dependencies are referred to as domain knowledge. In this paper, we propose a novel AU recognition method that learns AU classifiers from domain knowledge and expression-annotated facial images through adversarial training. Specifically, we first generate pseudo AU labels according to the probabilistic dependencies between expressions and AUs as well as correlations among AUs summarized from domain knowledge. Then we propose a weakly supervised AU recognition method via an adversarial process, in which we simultaneously train two models: a recognition model R, which learns AU classifiers, and a discrimination model D, which estimates the probability that AU labels generated from domain knowledge rather than the recognized AU labels from R. The training procedure for R maximizes the probability of D making a mistake. By leveraging the adversarial mechanism, the distribution of recognized AUs is closed to AU prior distribution from domain knowledge. Furthermore, the proposed weakly supervised AU recognition can be extended to semisupervised learning scenarios with partially AU-annotated images. Experimental results on three benchmark databases demonstrate that the proposed method successfully leverages the summarized domain knowledge to weakly supervised AU classifier learning through an adversarial process, and thus achieves state-of-the-art performance.&lt;/p&gt;














&lt;figure  id=&#34;figure-fig-the-framework-of-ran-in-part-1-the-facial-feature-x-is-inputted-into-recognizer-r-and-get-the-fake-au-vector-the-real-au-data-generated-in-section-22-are-in-part-2-in-part-3-p-discriminators-are-trained-real-or-fake-au-data-are-inputted-to-corresponding-discriminator-with-the-same-expression-label-see-text-for-details&#34;&gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img alt=&#34;Fig. The framework of RAN. In Part 1, the facial feature X is inputted into recognizer R and get the “fake” AU vector, the “real” AU data generated in section 2.2 are in Part 2. In part 3, P discriminators are trained, “real” or ”fake” AU data are inputted to corresponding discriminator with the same expression label. See text for details.&#34; srcset=&#34;
               /publication/dblp-confcvpr-peng-w-18/featured_hu1005fd87fac0ca886d0287f946d2eb97_137194_c02a8747c28f4e2f45f1231add499913.jpg 400w,
               /publication/dblp-confcvpr-peng-w-18/featured_hu1005fd87fac0ca886d0287f946d2eb97_137194_4eb4480d88985757ef87bf0ffea1b798.jpg 760w,
               /publication/dblp-confcvpr-peng-w-18/featured_hu1005fd87fac0ca886d0287f946d2eb97_137194_1200x1200_fit_q75_lanczos.jpg 1200w&#34;
               src=&#34;https://ustc-ac.github.io/publication/dblp-confcvpr-peng-w-18/featured_hu1005fd87fac0ca886d0287f946d2eb97_137194_c02a8747c28f4e2f45f1231add499913.jpg&#34;
               width=&#34;760&#34;
               height=&#34;378&#34;
               loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;figcaption&gt;
      Fig. The framework of RAN. In Part 1, the facial feature X is inputted into recognizer R and get the “fake” AU vector, the “real” AU data generated in section 2.2 are in Part 2. In part 3, P discriminators are trained, “real” or ”fake” AU data are inputted to corresponding discriminator with the same expression label. See text for details.
    &lt;/figcaption&gt;&lt;/figure&gt;
</description>
    </item>
    
    <item>
      <title>Weakly Supervised Facial Action Unit Recognition With Domain Knowledge</title>
      <link>https://ustc-ac.github.io/publication/dblp-journalstcyb-wang-pcj-18-a/</link>
      <pubDate>Mon, 01 Jan 2018 00:00:00 +0000</pubDate>
      <guid>https://ustc-ac.github.io/publication/dblp-journalstcyb-wang-pcj-18-a/</guid>
      <description></description>
    </item>
    
    <item>
      <title>A Multimodal Deep Regression Bayesian Network for Affective Video Content Analyses</title>
      <link>https://ustc-ac.github.io/publication/dblp-conficcv-gan-whj-17/</link>
      <pubDate>Sun, 01 Jan 2017 00:00:00 +0000</pubDate>
      <guid>https://ustc-ac.github.io/publication/dblp-conficcv-gan-whj-17/</guid>
      <description>&lt;p&gt;The inherent dependencies between visual elements and aural elements are crucial for affective video content analyses, yet have not been successfully exploited. Therefore, we propose a multimodal deep regression Bayesian network (MMDRBN) to capture the dependencies between visual elements and aural elements for affective video content analyses. The regression Bayesian network (RBN) is a directed graphical model consisting of one latent layer and one visible layer. Due to the explaining away effect in Bayesian networks (BN), RBN is able to capture both the dependencies among the latent variables given the observation and the dependencies among visible variables. We propose a fast learning algorithm to learn the RBN. For the MMDRBN, first, we learn several RBNs layer-wisely from visual modality and audio modality respectively. Then we stack these RBNs and obtain two deep networks. After that, a joint representation is extracted from the top layers of the two deep networks, and thus captures the high order dependencies between visual modality and audio modality. In order to predict the valence or arousal score of video contents, we initialize a feed-forward inference network from the MMDRBN whose inference is intractable by minimizing the KullbackCLeibler (KL)divergence between the two networks. The back propagation algorithm is adopted for finetuning the inference network. Experimental results on the LIRIS-ACCEDE database demonstrate that the proposed MMDRBN successfully captures the dependencies between visual and audio elements, and thus achieves better performance compared with state-of-the-art work.














&lt;figure  id=&#34;figure-fig-the-framework-of-our-proposed-method-first-we-train-a-multimodal-generative-network-it-consists-of-two-stacked-rbns-that-are-created-for-visual-and-audio-modalities-respectively&#34;&gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img alt=&#34;Fig. The framework of our proposed method. First, we train a multimodal generative network. It consists of two stacked RBNs that are created for visual and audio modalities respectively.&#34; srcset=&#34;
               /publication/dblp-conficcv-gan-whj-17/featured_hu0ff0903a4051f103a27ffc3b5d24d0b2_144232_33de511fe787d72c7424d929317c4505.jpg 400w,
               /publication/dblp-conficcv-gan-whj-17/featured_hu0ff0903a4051f103a27ffc3b5d24d0b2_144232_70c0a87a200289b1d89f4c6a966a6481.jpg 760w,
               /publication/dblp-conficcv-gan-whj-17/featured_hu0ff0903a4051f103a27ffc3b5d24d0b2_144232_1200x1200_fit_q75_lanczos.jpg 1200w&#34;
               src=&#34;https://ustc-ac.github.io/publication/dblp-conficcv-gan-whj-17/featured_hu0ff0903a4051f103a27ffc3b5d24d0b2_144232_33de511fe787d72c7424d929317c4505.jpg&#34;
               width=&#34;760&#34;
               height=&#34;411&#34;
               loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;figcaption&gt;
      Fig. The framework of our proposed method. First, we train a multimodal generative network. It consists of two stacked RBNs that are created for visual and audio modalities respectively.
    &lt;/figcaption&gt;&lt;/figure&gt;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Capturing Dependencies among Labels and Features for Multiple Emotion Tagging of Multimedia Data</title>
      <link>https://ustc-ac.github.io/publication/dblp-confaaai-wu-wj-17/</link>
      <pubDate>Sun, 01 Jan 2017 00:00:00 +0000</pubDate>
      <guid>https://ustc-ac.github.io/publication/dblp-confaaai-wu-wj-17/</guid>
      <description>&lt;p&gt;In this paper,we tackle the problem of emotion tagging of multimedia data by modeling the dependencies among multiple emotions in both the feature and label spaces. These dependencies,which carry crucial top-down and bottom-up evidence for improving multimedia affective content analysis,have not been thoroughly exploited yet. To this end, we propose two hierarchical models that independently and dependently learn the shared features and global semantic relationships among emotion labels to jointly tag multiple emotion labels of multimedia data. Efficient learning and inference algorithms of the proposed models are also developed. Experiments on three benchmark emotion databases demonstrate the superior performance of our methods to existing methods.&lt;/p&gt;














&lt;figure  id=&#34;figure-fig-two-proposed-methodsa-combining-a-multi-task-rbm-with-a-three-layer-rbm-to-capture-dependencies-among-features-and-labels-independentlyb-capturing-dependencies-among-features-and-labels-dependently&#34;&gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img alt=&#34;Fig. Two proposed methods.(a) Combining a multi-task RBM with a three-layer RBM to capture dependencies among features and labels independently.(b) Capturing dependencies among features and labels dependently.&#34; srcset=&#34;
               /publication/dblp-confaaai-wu-wj-17/featured_hu703fb9dff318081c89de1de4167dd036_98867_f9dbd9aa622db9e1b1e017a870c9ff02.JPG 400w,
               /publication/dblp-confaaai-wu-wj-17/featured_hu703fb9dff318081c89de1de4167dd036_98867_dfd10bd9528c4784cc86e20ff1f3a372.JPG 760w,
               /publication/dblp-confaaai-wu-wj-17/featured_hu703fb9dff318081c89de1de4167dd036_98867_1200x1200_fit_q75_lanczos.JPG 1200w&#34;
               src=&#34;https://ustc-ac.github.io/publication/dblp-confaaai-wu-wj-17/featured_hu703fb9dff318081c89de1de4167dd036_98867_f9dbd9aa622db9e1b1e017a870c9ff02.JPG&#34;
               width=&#34;760&#34;
               height=&#34;439&#34;
               loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;figcaption&gt;
      Fig. Two proposed methods.(a) Combining a multi-task RBM with a three-layer RBM to capture dependencies among features and labels independently.(b) Capturing dependencies among features and labels dependently.
    &lt;/figcaption&gt;&lt;/figure&gt;
</description>
    </item>
    
    <item>
      <title>Capturing Spatial and Temporal Patterns for Distinguishing between Posed and Spontaneous Expressions</title>
      <link>https://ustc-ac.github.io/publication/dblp-confmm-yang-w-17/</link>
      <pubDate>Sun, 01 Jan 2017 00:00:00 +0000</pubDate>
      <guid>https://ustc-ac.github.io/publication/dblp-confmm-yang-w-17/</guid>
      <description>&lt;p&gt;Spatial and temporal patterns inherent in facial behavior carry crucial information for posed and spontaneous expressions distinction, but have not been thoroughly exploited yet. To address this issue, we propose a novel dynamic model, termed as interval temporal restricted Boltzmann machine (IT-RBM), to jointly capture global spatial patterns and complex temporal patterns embedded in posed and spontaneous expressions respectively for distinguishing between posed and spontaneous expressions. Specifically, we consider a facial expression as a complex activity that consists of temporally overlapping or sequential primitive facial events, which are defined as the motion of feature points. We propose using the Allen s Interval Algebra to represent the complex temporal patterns existing in facial events through a two-layer Bayesian network. Furthermore, we propose employing multi-value restricted Boltzmann machine to capture intrinsic global spatial patterns among facial events. Experimental results on three benchmark databases, the UvA-NEMO smile database, the DISFA+ database, and theSPOS database, demonstrate the proposed interval temporal restricted Boltzmann machine can successfully capture the intrinsic spatial-temporal patterns in facial behavior, and thus outperform state-of-the art work of posed and spontaneous expressions distinction.&lt;/p&gt;














&lt;figure  id=&#34;figure-fig-the-outline-of-recognition-system&#34;&gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img alt=&#34;Fig. The outline of recognition system&#34; srcset=&#34;
               /publication/dblp-confmm-yang-w-17/featured_hu0bd183b9574ce216c435855d547551be_121258_0768ee6974650f9c803706a12511ded7.jpg 400w,
               /publication/dblp-confmm-yang-w-17/featured_hu0bd183b9574ce216c435855d547551be_121258_3e53e99adac07c31e2ef80d5e9dd326f.jpg 760w,
               /publication/dblp-confmm-yang-w-17/featured_hu0bd183b9574ce216c435855d547551be_121258_1200x1200_fit_q75_lanczos.jpg 1200w&#34;
               src=&#34;https://ustc-ac.github.io/publication/dblp-confmm-yang-w-17/featured_hu0bd183b9574ce216c435855d547551be_121258_0768ee6974650f9c803706a12511ded7.jpg&#34;
               width=&#34;760&#34;
               height=&#34;458&#34;
               loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;figcaption&gt;
      Fig. The outline of recognition system
    &lt;/figcaption&gt;&lt;/figure&gt;
</description>
    </item>
    
    <item>
      <title>Deep Facial Action Unit Recognition from Partially Labeled Data</title>
      <link>https://ustc-ac.github.io/publication/dblp-conficcv-wu-wpj-17/</link>
      <pubDate>Sun, 01 Jan 2017 00:00:00 +0000</pubDate>
      <guid>https://ustc-ac.github.io/publication/dblp-conficcv-wu-wpj-17/</guid>
      <description>&lt;p&gt;Current work on facial action unit (AU) recognition requires AU-labeled facial images. Although large amounts of facial images are readily available, AU annotation is expensive and time consuming. To address this, we propose a deep facial action unit recognition approach learning from partially AU-labeled data. The proposed approach makes full use of both partly available ground-truth AU labels and the readily available large scale facial images without annotation. Specifically, we propose to learn label distribution from the ground-truth AU labels, and then train the AU classifiers from the large-scale facial images by maximizing the log likelihood of the mapping functions of AUs with regard to the learnt label distribution for all training data and minimizing the error between predicted AUs and ground-truth AUs for labeled data simultaneously. A restricted Boltzmann machine is adopted to model AU label distribution, a deep neural network is used to learn facial representation from facial images, and the support vector machine is employed as the classifier. Experiments on two benchmark databases demonstrate the effectiveness of the proposed approach.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Deep multimodal network for multi-label classification</title>
      <link>https://ustc-ac.github.io/publication/dblp-conficmcs-chen-wc-17/</link>
      <pubDate>Sun, 01 Jan 2017 00:00:00 +0000</pubDate>
      <guid>https://ustc-ac.github.io/publication/dblp-conficmcs-chen-wc-17/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Differentiating Between Posed and Spontaneous Expressions with Latent Regression Bayesian Network</title>
      <link>https://ustc-ac.github.io/publication/dblp-confaaai-gan-nwj-17/</link>
      <pubDate>Sun, 01 Jan 2017 00:00:00 +0000</pubDate>
      <guid>https://ustc-ac.github.io/publication/dblp-confaaai-gan-nwj-17/</guid>
      <description>&lt;p&gt;Spatial patterns embedded in human faces are crucial for differentiating posed expressions from spontaneous ones, yet they have not been thoroughly exploited in the literature. To tackle this problem, we present a generative model, i.e., Latent Regression Bayesian Network (LRBN), to effectively capture the spatial patterns embedded in facial landmark points to differentiate between posed and spontaneous facial expressions. The LRBN is a directed graphical model consisting of one latent layer and one visible layer. Due to the “explaining away” effect in Bayesian networks, LRBN is able to capture both the dependencies among the latent variables given the observation and the dependencies among visible variables. We believe that such dependencies are crucial for faithful data representation. Specifically, during training, we construct two LRBNs to capture spatial patterns inherent in displacements of landmark points from spontaneous facial expressions and posed facial expressions respectively. During testing, the samples are classified into posed or spontaneous expressions according to their likelihoods on two models. Efficient learning and inference algorithms are proposed. Experimental results on two benchmark databases demonstrate the advantages of the proposed approach in modeling spatial patterns as well as its superior performance to the existing methods in differentiating between posed and spontaneous expressions.&lt;/p&gt;














&lt;figure  id=&#34;figure-fig-the-framework-of-capturing-spatial-patterns&#34;&gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img alt=&#34;Fig. The framework of capturing spatial patterns&#34; srcset=&#34;
               /publication/dblp-confaaai-gan-nwj-17/featured_hu5eafe8c2628672eb14e0c65744466ffa_86294_efc2b202e0496b643e7469c436369d59.jpg 400w,
               /publication/dblp-confaaai-gan-nwj-17/featured_hu5eafe8c2628672eb14e0c65744466ffa_86294_b9cbd7521ce6355a25d7e9d9350ea896.jpg 760w,
               /publication/dblp-confaaai-gan-nwj-17/featured_hu5eafe8c2628672eb14e0c65744466ffa_86294_1200x1200_fit_q75_lanczos.jpg 1200w&#34;
               src=&#34;https://ustc-ac.github.io/publication/dblp-confaaai-gan-nwj-17/featured_hu5eafe8c2628672eb14e0c65744466ffa_86294_efc2b202e0496b643e7469c436369d59.jpg&#34;
               width=&#34;760&#34;
               height=&#34;377&#34;
               loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;figcaption&gt;
      Fig. The framework of capturing spatial patterns
    &lt;/figcaption&gt;&lt;/figure&gt;
</description>
    </item>
    
    <item>
      <title>Emotion recognition through integrating EEG and peripheral signals</title>
      <link>https://ustc-ac.github.io/publication/dblp-conficassp-shu-w-17/</link>
      <pubDate>Sun, 01 Jan 2017 00:00:00 +0000</pubDate>
      <guid>https://ustc-ac.github.io/publication/dblp-conficassp-shu-w-17/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Exploring Domain Knowledge for Affective Video Content Analyses</title>
      <link>https://ustc-ac.github.io/publication/dblp-confmm-chen-wwc-17/</link>
      <pubDate>Sun, 01 Jan 2017 00:00:00 +0000</pubDate>
      <guid>https://ustc-ac.github.io/publication/dblp-confmm-chen-wwc-17/</guid>
      <description>&lt;p&gt;The well-established film grammar is often used to change visual and audio elements of videos to invoke audiences’ emotional experience. Such film grammar, referred to as domain knowledge, is crucial for affective video content analyses, but has not been thoroughly explored yet. In this paper, we propose a novel method to analyze video affective content through exploring domain knowledge. Specifically, take visual elements as an example, we first infer probabilistic dependencies between visual elements and emotions from the summarized film grammar. Then, we transfer the domain knowledge as constraints, and formulate affective video content analyses as a constrained optimization problem. Experiments on the LIRIS-ACCEDE database and the DEAP database demonstrate that the proposed affective content analyses method can successfully leverage well-established film grammar for better emotion classification from video content.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Expression-assisted facial action unit recognition under incomplete AU annotation</title>
      <link>https://ustc-ac.github.io/publication/dblp-journalspr-wang-gj-17/</link>
      <pubDate>Sun, 01 Jan 2017 00:00:00 +0000</pubDate>
      <guid>https://ustc-ac.github.io/publication/dblp-journalspr-wang-gj-17/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Feature and label relation modeling for multiple-facial action unit classification and intensity estimation</title>
      <link>https://ustc-ac.github.io/publication/dblp-journalspr-wang-ygj-17/</link>
      <pubDate>Sun, 01 Jan 2017 00:00:00 +0000</pubDate>
      <guid>https://ustc-ac.github.io/publication/dblp-journalspr-wang-ygj-17/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Learning with Privileged Information for Multi-Label Classification</title>
      <link>https://ustc-ac.github.io/publication/dblp-journalscorr-chen-wcs-17/</link>
      <pubDate>Sun, 01 Jan 2017 00:00:00 +0000</pubDate>
      <guid>https://ustc-ac.github.io/publication/dblp-journalscorr-chen-wcs-17/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Personalized video emotion tagging through a topic model</title>
      <link>https://ustc-ac.github.io/publication/dblp-conficassp-wu-wg-17/</link>
      <pubDate>Sun, 01 Jan 2017 00:00:00 +0000</pubDate>
      <guid>https://ustc-ac.github.io/publication/dblp-conficassp-wu-wg-17/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Capturing global spatial patterns for distinguishing posed and spontaneous expressions</title>
      <link>https://ustc-ac.github.io/publication/dblp-journalscviu-wang-wj-16/</link>
      <pubDate>Fri, 01 Jan 2016 00:00:00 +0000</pubDate>
      <guid>https://ustc-ac.github.io/publication/dblp-journalscviu-wang-wj-16/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Emotion Recognition from EEG Signals Enhanced by User&#39;s Profile</title>
      <link>https://ustc-ac.github.io/publication/dblp-confmir-chen-wgw-16/</link>
      <pubDate>Fri, 01 Jan 2016 00:00:00 +0000</pubDate>
      <guid>https://ustc-ac.github.io/publication/dblp-confmir-chen-wgw-16/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Emotion recognition from peripheral physiological signals enhanced by EEG</title>
      <link>https://ustc-ac.github.io/publication/dblp-conficassp-chen-gw-16/</link>
      <pubDate>Fri, 01 Jan 2016 00:00:00 +0000</pubDate>
      <guid>https://ustc-ac.github.io/publication/dblp-conficassp-chen-gw-16/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Employing subjects&#39; information as privileged information for emotion recognition from EEG signals</title>
      <link>https://ustc-ac.github.io/publication/dblp-conficpr-wu-wzgyj-16/</link>
      <pubDate>Fri, 01 Jan 2016 00:00:00 +0000</pubDate>
      <guid>https://ustc-ac.github.io/publication/dblp-conficpr-wu-wzgyj-16/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Facial Expression Intensity Estimation Using Ordinal Information</title>
      <link>https://ustc-ac.github.io/publication/dblp-confcvpr-zhao-gwj-16/</link>
      <pubDate>Fri, 01 Jan 2016 00:00:00 +0000</pubDate>
      <guid>https://ustc-ac.github.io/publication/dblp-confcvpr-zhao-gwj-16/</guid>
      <description>&lt;p&gt;Previous studies on facial expression analysis have been focused on recognizing basic expression categories.  There is limited  amount  of  work  on  the  continuous  expression intensity estimation,  which is important for detecting and tracking emotion change.  Part of the reason is the lack of labeled data with annotated expression intensity since ex-pression intensity annotation requires expertise and is time consuming.  In this work, we treat the expression intensity estimation as a regression problem. By taking advantage of the natural onset-apex-offset evolution pattern of facial ex-pression, the proposed method can handle different amounts of annotations to perform frame-level expression intensity estimation.    In  fully  supervised  case,  all  the  frames  are provided with intensity annotations.  In weakly supervised case, only the annotations of selected key frames are used.While in unsupervised case, expression intensity can be es-timated without any annotations.  An efficient optimization algorithm based on Alternating Direction Method of Mul-tipliers (ADMM) is developed for solving the optimization problem associated with parameter learning.   We demon-strate the effectiveness of proposed method by comparing it against both fully supervised and unsupervised approaches on benchmark facial expression datasets.&lt;/p&gt;














&lt;figure  id=&#34;figure-a-diagram-showing-the-experiment-process-depending-on-the-experiment-setting-different-amounts-of-intensity-annotation-information-are-fed-into-model-learning-process-resulting-different-models-training-is-performed-using-complete-expression-sequences-while-testing-is-performed-on-each-frame-of-a-sequence&#34;&gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img alt=&#34;A diagram showing the experiment process. Depending on the experiment setting, different amounts of intensity annotation information are fed into model learning process, resulting different models. Training is performed using complete expression sequences while testing is performed on each frame of a sequence.&#34; srcset=&#34;
               /publication/dblp-confcvpr-zhao-gwj-16/featured_hu82eae57b0a637c9c9c6659cadeeccee4_129242_5b734835e2304d704d3efb8a8632198b.jpg 400w,
               /publication/dblp-confcvpr-zhao-gwj-16/featured_hu82eae57b0a637c9c9c6659cadeeccee4_129242_0b80fe4ea1c3b64497154329948583d3.jpg 760w,
               /publication/dblp-confcvpr-zhao-gwj-16/featured_hu82eae57b0a637c9c9c6659cadeeccee4_129242_1200x1200_fit_q75_lanczos.jpg 1200w&#34;
               src=&#34;https://ustc-ac.github.io/publication/dblp-confcvpr-zhao-gwj-16/featured_hu82eae57b0a637c9c9c6659cadeeccee4_129242_5b734835e2304d704d3efb8a8632198b.jpg&#34;
               width=&#34;760&#34;
               height=&#34;153&#34;
               loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;figcaption&gt;
      A diagram showing the experiment process. Depending on the experiment setting, different amounts of intensity annotation information are fed into model learning process, resulting different models. Training is performed using complete expression sequences while testing is performed on each frame of a sequence.
    &lt;/figcaption&gt;&lt;/figure&gt;
</description>
    </item>
    
    <item>
      <title>Facial expression recognition through modeling age-related spatial patterns</title>
      <link>https://ustc-ac.github.io/publication/dblp-journalsmta-wang-wgj-16/</link>
      <pubDate>Fri, 01 Jan 2016 00:00:00 +0000</pubDate>
      <guid>https://ustc-ac.github.io/publication/dblp-journalsmta-wang-wgj-16/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Facial Expression Recognition with Deep two-view Support Vector Machine</title>
      <link>https://ustc-ac.github.io/publication/dblp-confmm-wu-wpc-16/</link>
      <pubDate>Fri, 01 Jan 2016 00:00:00 +0000</pubDate>
      <guid>https://ustc-ac.github.io/publication/dblp-confmm-wu-wpc-16/</guid>
      <description>&lt;p&gt;This paper proposes a novel deep two-view approach to learn features from both visible and thermal images and leverage the commonality among visible and thermal images for facial expression recognition from visible images. The thermal images are used as privileged information, which is required only during training to help visible images learn better features and classifier. Specifically, we first learn a deep model for visible images and thermal images respectively, and use the learned feature representations to train SVM classifiers for expression classification. We then jointly refine the deep models as well as the SVM classifiers for both thermal images and visible images by imposing the constraint that the outputs of the SVM classifiers from two views are similar. Therefore, the resulting representations and classifiers capture the inherent connections among visible facial image, infrared facial image and target expression labels, and hence improve the recognition performance for facial expression recognition from visible images during testing. Experimental results on the benchmark expression database demonstrate the effectiveness of our proposed method.&lt;/p&gt;














&lt;figure  id=&#34;figure-fig-the-scheme-of-deep-network-for-visible-image-data-thermal-image-data-and-the-scheme-of-the-proposed-model&#34;&gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img alt=&#34;Fig. The scheme of deep network for visible image data, thermal image data and the scheme of the proposed model.&#34; srcset=&#34;
               /publication/dblp-confmm-wu-wpc-16/featured_hu4a007d55c83268e1c22352006fcba45d_180260_0ecaa368acedc411a8d4fff2952e216b.JPG 400w,
               /publication/dblp-confmm-wu-wpc-16/featured_hu4a007d55c83268e1c22352006fcba45d_180260_d620d56d8d33e7b0a84c6ddf224ae0a5.JPG 760w,
               /publication/dblp-confmm-wu-wpc-16/featured_hu4a007d55c83268e1c22352006fcba45d_180260_1200x1200_fit_q75_lanczos.JPG 1200w&#34;
               src=&#34;https://ustc-ac.github.io/publication/dblp-confmm-wu-wpc-16/featured_hu4a007d55c83268e1c22352006fcba45d_180260_0ecaa368acedc411a8d4fff2952e216b.JPG&#34;
               width=&#34;760&#34;
               height=&#34;328&#34;
               loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;figcaption&gt;
      Fig. The scheme of deep network for visible image data, thermal image data and the scheme of the proposed model.
    &lt;/figcaption&gt;&lt;/figure&gt;
</description>
    </item>
    
    <item>
      <title>Gender recognition from visible and thermal infrared facial images</title>
      <link>https://ustc-ac.github.io/publication/dblp-journalsmta-wang-ghhj-16/</link>
      <pubDate>Fri, 01 Jan 2016 00:00:00 +0000</pubDate>
      <guid>https://ustc-ac.github.io/publication/dblp-journalsmta-wang-ghhj-16/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Implicit hybrid video emotion tagging by integrating video content and users&#39; multiple physiological responses</title>
      <link>https://ustc-ac.github.io/publication/dblp-conficpr-chen-wwgsj-16/</link>
      <pubDate>Fri, 01 Jan 2016 00:00:00 +0000</pubDate>
      <guid>https://ustc-ac.github.io/publication/dblp-conficpr-chen-wwgsj-16/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Multiple Facial Action Unit recognition by learning joint features and label relations</title>
      <link>https://ustc-ac.github.io/publication/dblp-conficpr-wu-wj-16/</link>
      <pubDate>Fri, 01 Jan 2016 00:00:00 +0000</pubDate>
      <guid>https://ustc-ac.github.io/publication/dblp-conficpr-wu-wj-16/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Multiple facial action unit recognition enhanced by facial expressions</title>
      <link>https://ustc-ac.github.io/publication/dblp-conficpr-yang-wwj-16/</link>
      <pubDate>Fri, 01 Jan 2016 00:00:00 +0000</pubDate>
      <guid>https://ustc-ac.github.io/publication/dblp-conficpr-yang-wwj-16/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Posed and Spontaneous Expression Recognition Through Restricted Boltzmann Machine</title>
      <link>https://ustc-ac.github.io/publication/dblp-confmmm-wu-w-16/</link>
      <pubDate>Fri, 01 Jan 2016 00:00:00 +0000</pubDate>
      <guid>https://ustc-ac.github.io/publication/dblp-confmmm-wu-w-16/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Emotion Recognition from EEG Signals by Leveraging Stimulus Videos</title>
      <link>https://ustc-ac.github.io/publication/dblp-confpcm-gao-w-15/</link>
      <pubDate>Thu, 01 Jan 2015 00:00:00 +0000</pubDate>
      <guid>https://ustc-ac.github.io/publication/dblp-confpcm-gao-w-15/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Emotion Recognition from EEG Signals using Hierarchical Bayesian Network with Privileged Information</title>
      <link>https://ustc-ac.github.io/publication/dblp-confmir-gao-w-15/</link>
      <pubDate>Thu, 01 Jan 2015 00:00:00 +0000</pubDate>
      <guid>https://ustc-ac.github.io/publication/dblp-confmir-gao-w-15/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Emotion Recognition with the Help of Privileged Information</title>
      <link>https://ustc-ac.github.io/publication/dblp-journalstamd-wang-zyj-15/</link>
      <pubDate>Thu, 01 Jan 2015 00:00:00 +0000</pubDate>
      <guid>https://ustc-ac.github.io/publication/dblp-journalstamd-wang-zyj-15/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Enhanced facial expression recognition by age</title>
      <link>https://ustc-ac.github.io/publication/dblp-conffgr-wu-ww-15/</link>
      <pubDate>Thu, 01 Jan 2015 00:00:00 +0000</pubDate>
      <guid>https://ustc-ac.github.io/publication/dblp-conffgr-wu-ww-15/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Expression Recognition from Visible Images with the Help of Thermal Images</title>
      <link>https://ustc-ac.github.io/publication/dblp-confmir-shi-wz-15/</link>
      <pubDate>Thu, 01 Jan 2015 00:00:00 +0000</pubDate>
      <guid>https://ustc-ac.github.io/publication/dblp-confmir-shi-wz-15/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Facial Action Unit Classification with Hidden Knowledge under Incomplete Annotation</title>
      <link>https://ustc-ac.github.io/publication/dblp-confmir-wang-wj-15/</link>
      <pubDate>Thu, 01 Jan 2015 00:00:00 +0000</pubDate>
      <guid>https://ustc-ac.github.io/publication/dblp-confmir-wang-wj-15/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Implicit video emotion tagging from audiences&#39; facial expression</title>
      <link>https://ustc-ac.github.io/publication/dblp-journalsmta-wang-lzhcj-15/</link>
      <pubDate>Thu, 01 Jan 2015 00:00:00 +0000</pubDate>
      <guid>https://ustc-ac.github.io/publication/dblp-journalsmta-wang-lzhcj-15/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Learning with privileged information using Bayesian networks</title>
      <link>https://ustc-ac.github.io/publication/dblp-journalsfcsc-wang-hzhlj-15/</link>
      <pubDate>Thu, 01 Jan 2015 00:00:00 +0000</pubDate>
      <guid>https://ustc-ac.github.io/publication/dblp-journalsfcsc-wang-hzhlj-15/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Multi-instance Hidden Markov Model for facial expression recognition</title>
      <link>https://ustc-ac.github.io/publication/dblp-conffgr-wu-wj-15/</link>
      <pubDate>Thu, 01 Jan 2015 00:00:00 +0000</pubDate>
      <guid>https://ustc-ac.github.io/publication/dblp-conffgr-wu-wj-15/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Multiple Aesthetic Attribute Assessment by Exploiting Relations Among Aesthetic Attributes</title>
      <link>https://ustc-ac.github.io/publication/dblp-confmir-gao-wj-15/</link>
      <pubDate>Thu, 01 Jan 2015 00:00:00 +0000</pubDate>
      <guid>https://ustc-ac.github.io/publication/dblp-confmir-gao-wj-15/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Multiple Emotion Tagging for Multimedia Data by Exploiting High-Order Dependencies Among Emotions</title>
      <link>https://ustc-ac.github.io/publication/dblp-journalstmm-wang-wwj-15/</link>
      <pubDate>Thu, 01 Jan 2015 00:00:00 +0000</pubDate>
      <guid>https://ustc-ac.github.io/publication/dblp-journalstmm-wang-wwj-15/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Multiple emotional tagging of multimedia data by exploiting dependencies among emotions</title>
      <link>https://ustc-ac.github.io/publication/dblp-journalsmta-wang-wj-15/</link>
      <pubDate>Thu, 01 Jan 2015 00:00:00 +0000</pubDate>
      <guid>https://ustc-ac.github.io/publication/dblp-journalsmta-wang-wj-15/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Posed and spontaneous expression recognition through modeling their spatial patterns</title>
      <link>https://ustc-ac.github.io/publication/dblp-journalsmva-wang-whwj-15/</link>
      <pubDate>Thu, 01 Jan 2015 00:00:00 +0000</pubDate>
      <guid>https://ustc-ac.github.io/publication/dblp-journalsmva-wang-whwj-15/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Posed and spontaneous facial expression differentiation using deep Boltzmann machines</title>
      <link>https://ustc-ac.github.io/publication/dblp-confacii-gan-wwj-15/</link>
      <pubDate>Thu, 01 Jan 2015 00:00:00 +0000</pubDate>
      <guid>https://ustc-ac.github.io/publication/dblp-confacii-gan-wwj-15/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Video Affective Content Analysis: A Survey of State-of-the-Art Methods</title>
      <link>https://ustc-ac.github.io/publication/dblp-journalstaffco-wang-j-15/</link>
      <pubDate>Thu, 01 Jan 2015 00:00:00 +0000</pubDate>
      <guid>https://ustc-ac.github.io/publication/dblp-journalstaffco-wang-j-15/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Capture expression-dependent AU relations for expression recognition</title>
      <link>https://ustc-ac.github.io/publication/dblp-conficmcs-wang-whwglj-14/</link>
      <pubDate>Wed, 01 Jan 2014 00:00:00 +0000</pubDate>
      <guid>https://ustc-ac.github.io/publication/dblp-conficmcs-wang-whwglj-14/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Early Facial Expression Recognition Using Hidden Markov Models</title>
      <link>https://ustc-ac.github.io/publication/dblp-conficpr-wang-wj-14-a/</link>
      <pubDate>Wed, 01 Jan 2014 00:00:00 +0000</pubDate>
      <guid>https://ustc-ac.github.io/publication/dblp-conficpr-wang-wj-14-a/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Emotion recognition from thermal infrared images using deep Boltzmann machine</title>
      <link>https://ustc-ac.github.io/publication/dblp-journalsfcsc-wang-hghj-14/</link>
      <pubDate>Wed, 01 Jan 2014 00:00:00 +0000</pubDate>
      <guid>https://ustc-ac.github.io/publication/dblp-journalsfcsc-wang-hghj-14/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Emotion recognition from users&#39; EEG signals with the help of stimulus VIDEOS</title>
      <link>https://ustc-ac.github.io/publication/dblp-conficmcs-zhu-wj-14/</link>
      <pubDate>Wed, 01 Jan 2014 00:00:00 +0000</pubDate>
      <guid>https://ustc-ac.github.io/publication/dblp-conficmcs-zhu-wj-14/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Enhancing multi-label classification by modeling dependencies among labels</title>
      <link>https://ustc-ac.github.io/publication/dblp-journalspr-wang-wwj-14/</link>
      <pubDate>Wed, 01 Jan 2014 00:00:00 +0000</pubDate>
      <guid>https://ustc-ac.github.io/publication/dblp-journalspr-wang-wwj-14/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Exploiting multi-expression dependences for implicit multi-emotion video tagging</title>
      <link>https://ustc-ac.github.io/publication/dblp-journalsivc-wang-lwwlcj-14/</link>
      <pubDate>Wed, 01 Jan 2014 00:00:00 +0000</pubDate>
      <guid>https://ustc-ac.github.io/publication/dblp-journalsivc-wang-lwwlcj-14/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Facial Action Unit recognition by relation modeling from both qualitative knowledge and quantitative data</title>
      <link>https://ustc-ac.github.io/publication/dblp-conficmcs-gao-wwwj-14/</link>
      <pubDate>Wed, 01 Jan 2014 00:00:00 +0000</pubDate>
      <guid>https://ustc-ac.github.io/publication/dblp-conficmcs-gao-wwwj-14/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Fusion of visible and thermal images for facial expression recognition</title>
      <link>https://ustc-ac.github.io/publication/dblp-journalsfcsc-wang-hwhj-14/</link>
      <pubDate>Wed, 01 Jan 2014 00:00:00 +0000</pubDate>
      <guid>https://ustc-ac.github.io/publication/dblp-journalsfcsc-wang-hwhj-14/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Hybrid video emotional tagging using users&#39; EEG and video content</title>
      <link>https://ustc-ac.github.io/publication/dblp-journalsmta-wang-zwj-14/</link>
      <pubDate>Wed, 01 Jan 2014 00:00:00 +0000</pubDate>
      <guid>https://ustc-ac.github.io/publication/dblp-journalsmta-wang-zwj-14/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Multi-label Learning with Missing Labels</title>
      <link>https://ustc-ac.github.io/publication/dblp-conficpr-wu-lwhj-14/</link>
      <pubDate>Wed, 01 Jan 2014 00:00:00 +0000</pubDate>
      <guid>https://ustc-ac.github.io/publication/dblp-conficpr-wu-lwhj-14/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Multiple-Facial Action Unit Recognition by Shared Feature Learning and Semantic Relation Modeling</title>
      <link>https://ustc-ac.github.io/publication/dblp-conficpr-zhu-wyj-14/</link>
      <pubDate>Wed, 01 Jan 2014 00:00:00 +0000</pubDate>
      <guid>https://ustc-ac.github.io/publication/dblp-conficpr-zhu-wyj-14/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Sequence-based bias analysis of spontaneous facial expression databases</title>
      <link>https://ustc-ac.github.io/publication/dblp-conficmcs-wang-wwj-14/</link>
      <pubDate>Wed, 01 Jan 2014 00:00:00 +0000</pubDate>
      <guid>https://ustc-ac.github.io/publication/dblp-conficmcs-wang-wwj-14/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Active Labeling of Facial Feature Points</title>
      <link>https://ustc-ac.github.io/publication/dblp-confacii-he-wj-13/</link>
      <pubDate>Tue, 01 Jan 2013 00:00:00 +0000</pubDate>
      <guid>https://ustc-ac.github.io/publication/dblp-confacii-he-wj-13/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Analyses of a Multimodal Spontaneous Facial Expression Database</title>
      <link>https://ustc-ac.github.io/publication/dblp-journalstaffco-wang-lwwshw-13/</link>
      <pubDate>Tue, 01 Jan 2013 00:00:00 +0000</pubDate>
      <guid>https://ustc-ac.github.io/publication/dblp-journalstaffco-wang-lwwshw-13/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Analyses of the Differences between Posed and Spontaneous Facial Expressions</title>
      <link>https://ustc-ac.github.io/publication/dblp-confacii-he-wlc-13/</link>
      <pubDate>Tue, 01 Jan 2013 00:00:00 +0000</pubDate>
      <guid>https://ustc-ac.github.io/publication/dblp-confacii-he-wlc-13/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Capturing Complex Spatio-temporal Relations among Facial Muscles for Facial Expression Recognition</title>
      <link>https://ustc-ac.github.io/publication/dblp-confcvpr-wang-wj-13/</link>
      <pubDate>Tue, 01 Jan 2013 00:00:00 +0000</pubDate>
      <guid>https://ustc-ac.github.io/publication/dblp-confcvpr-wang-wj-13/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Capturing Global Semantic Relationships for Facial Action Unit Recognition</title>
      <link>https://ustc-ac.github.io/publication/dblp-conficcv-wang-lwj-13/</link>
      <pubDate>Tue, 01 Jan 2013 00:00:00 +0000</pubDate>
      <guid>https://ustc-ac.github.io/publication/dblp-conficcv-wang-lwj-13/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Emotional Influence on SSVEP Based BCI</title>
      <link>https://ustc-ac.github.io/publication/dblp-confacii-zhu-twgwc-13/</link>
      <pubDate>Tue, 01 Jan 2013 00:00:00 +0000</pubDate>
      <guid>https://ustc-ac.github.io/publication/dblp-confacii-zhu-twgwc-13/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Emotional tagging of videos by exploring multiple emotions&#39; coexistence</title>
      <link>https://ustc-ac.github.io/publication/dblp-conffgr-wang-whlj-13/</link>
      <pubDate>Tue, 01 Jan 2013 00:00:00 +0000</pubDate>
      <guid>https://ustc-ac.github.io/publication/dblp-conffgr-wang-whlj-13/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Eye localization from thermal infrared images</title>
      <link>https://ustc-ac.github.io/publication/dblp-journalspr-wang-lsj-13/</link>
      <pubDate>Tue, 01 Jan 2013 00:00:00 +0000</pubDate>
      <guid>https://ustc-ac.github.io/publication/dblp-journalspr-wang-lsj-13/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Facial Expression Recognition Using Deep Boltzmann Machine from Thermal Infrared Images</title>
      <link>https://ustc-ac.github.io/publication/dblp-confacii-he-wlfj-13/</link>
      <pubDate>Tue, 01 Jan 2013 00:00:00 +0000</pubDate>
      <guid>https://ustc-ac.github.io/publication/dblp-confacii-he-wlfj-13/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Implicit video multi-emotion tagging by exploiting multi-expression relations</title>
      <link>https://ustc-ac.github.io/publication/dblp-conffgr-liu-wwj-13/</link>
      <pubDate>Tue, 01 Jan 2013 00:00:00 +0000</pubDate>
      <guid>https://ustc-ac.github.io/publication/dblp-conffgr-liu-wwj-13/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Simultaneous Facial Feature Tracking and Facial Expression Recognition</title>
      <link>https://ustc-ac.github.io/publication/dblp-journalstip-li-wzj-13/</link>
      <pubDate>Tue, 01 Jan 2013 00:00:00 +0000</pubDate>
      <guid>https://ustc-ac.github.io/publication/dblp-journalstip-li-wzj-13/</guid>
      <description></description>
    </item>
    
    <item>
      <title>A qualitative and quantitative study of color emotion using valence-arousal</title>
      <link>https://ustc-ac.github.io/publication/dblp-journalsfcsc-wang-d-12/</link>
      <pubDate>Sun, 01 Jan 2012 00:00:00 +0000</pubDate>
      <guid>https://ustc-ac.github.io/publication/dblp-journalsfcsc-wang-d-12/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Analysis of Affective Effects on Steady-State Visual Evoked Potential Responses</title>
      <link>https://ustc-ac.github.io/publication/dblp-confias-wang-wz-12/</link>
      <pubDate>Sun, 01 Jan 2012 00:00:00 +0000</pubDate>
      <guid>https://ustc-ac.github.io/publication/dblp-confias-wang-wz-12/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Bias analyses of spontaneous facial expression database</title>
      <link>https://ustc-ac.github.io/publication/dblp-conficpr-wang-wzj-12/</link>
      <pubDate>Sun, 01 Jan 2012 00:00:00 +0000</pubDate>
      <guid>https://ustc-ac.github.io/publication/dblp-conficpr-wang-wzj-12/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Eye Localization from Infrared Thermal Images</title>
      <link>https://ustc-ac.github.io/publication/dblp-confmprss-wang-sl-12/</link>
      <pubDate>Sun, 01 Jan 2012 00:00:00 +0000</pubDate>
      <guid>https://ustc-ac.github.io/publication/dblp-confmprss-wang-sl-12/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Facial expression recognition from infrared thermal images using temperature difference by voting</title>
      <link>https://ustc-ac.github.io/publication/dblp-confccis-wang-sl-12/</link>
      <pubDate>Sun, 01 Jan 2012 00:00:00 +0000</pubDate>
      <guid>https://ustc-ac.github.io/publication/dblp-confccis-wang-sl-12/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Facial Expression Recognition from Infrared Thermal Videos</title>
      <link>https://ustc-ac.github.io/publication/dblp-confias-shen-wl-12/</link>
      <pubDate>Sun, 01 Jan 2012 00:00:00 +0000</pubDate>
      <guid>https://ustc-ac.github.io/publication/dblp-confias-shen-wl-12/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Posed and spontaneous expression distinguishment from infrared thermal images</title>
      <link>https://ustc-ac.github.io/publication/dblp-conficpr-liu-w-12/</link>
      <pubDate>Sun, 01 Jan 2012 00:00:00 +0000</pubDate>
      <guid>https://ustc-ac.github.io/publication/dblp-conficpr-liu-w-12/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Similarity Measurement and Feature Selection Using Genetic Algorithm</title>
      <link>https://ustc-ac.github.io/publication/dblp-confisnn-wang-hz-12/</link>
      <pubDate>Sun, 01 Jan 2012 00:00:00 +0000</pubDate>
      <guid>https://ustc-ac.github.io/publication/dblp-confisnn-wang-hz-12/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Spontaneous Facial Expression Recognition by Fusing Thermal Infrared and Visible Images</title>
      <link>https://ustc-ac.github.io/publication/dblp-confias-wang-h-12/</link>
      <pubDate>Sun, 01 Jan 2012 00:00:00 +0000</pubDate>
      <guid>https://ustc-ac.github.io/publication/dblp-confias-wang-h-12/</guid>
      <description></description>
    </item>
    
    <item>
      <title>A real-time attitude recognition by eye-tracking</title>
      <link>https://ustc-ac.github.io/publication/dblp-conficimcs-lv-ws-11/</link>
      <pubDate>Sat, 01 Jan 2011 00:00:00 +0000</pubDate>
      <guid>https://ustc-ac.github.io/publication/dblp-conficimcs-lv-ws-11/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Affective Classification in Video Based on Semi-supervised Learning</title>
      <link>https://ustc-ac.github.io/publication/dblp-confisnn-wang-lh-11/</link>
      <pubDate>Sat, 01 Jan 2011 00:00:00 +0000</pubDate>
      <guid>https://ustc-ac.github.io/publication/dblp-confisnn-wang-lh-11/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Emotion Recognition Using Hidden Markov Models from Facial Temperature Sequence</title>
      <link>https://ustc-ac.github.io/publication/dblp-confacii-liu-w-11/</link>
      <pubDate>Sat, 01 Jan 2011 00:00:00 +0000</pubDate>
      <guid>https://ustc-ac.github.io/publication/dblp-confacii-liu-w-11/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Spontaneous Facial Expression Recognition Based on Feature Point Tracking</title>
      <link>https://ustc-ac.github.io/publication/dblp-conficig-he-wl-11/</link>
      <pubDate>Sat, 01 Jan 2011 00:00:00 +0000</pubDate>
      <guid>https://ustc-ac.github.io/publication/dblp-conficig-he-wl-11/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Spontaneous facial expression recognition by using feature-level fusion of visible and thermal infrared images</title>
      <link>https://ustc-ac.github.io/publication/dblp-confmlsp-wang-w-11/</link>
      <pubDate>Sat, 01 Jan 2011 00:00:00 +0000</pubDate>
      <guid>https://ustc-ac.github.io/publication/dblp-confmlsp-wang-w-11/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Towards robot incremental learning constraints from comparative demonstration</title>
      <link>https://ustc-ac.github.io/publication/dblp-confatal-zhang-wcyccljws-11/</link>
      <pubDate>Sat, 01 Jan 2011 00:00:00 +0000</pubDate>
      <guid>https://ustc-ac.github.io/publication/dblp-confatal-zhang-wcyccljws-11/</guid>
      <description></description>
    </item>
    
    <item>
      <title>A Natural Visible and Infrared Facial Expression Database for Expression Recognition and Emotion Inference</title>
      <link>https://ustc-ac.github.io/publication/dblp-journalstmm-wang-lllwpcw-10/</link>
      <pubDate>Fri, 01 Jan 2010 00:00:00 +0000</pubDate>
      <guid>https://ustc-ac.github.io/publication/dblp-journalstmm-wang-lllwpcw-10/</guid>
      <description></description>
    </item>
    
    <item>
      <title>A spontaneous facial expression recognition method using head motion and AAM features</title>
      <link>https://ustc-ac.github.io/publication/dblp-confnabic-lv-w-10/</link>
      <pubDate>Fri, 01 Jan 2010 00:00:00 +0000</pubDate>
      <guid>https://ustc-ac.github.io/publication/dblp-confnabic-lv-w-10/</guid>
      <description></description>
    </item>
    
    <item>
      <title>An emotional harmony generation system</title>
      <link>https://ustc-ac.github.io/publication/dblp-confcec-xu-wl-10/</link>
      <pubDate>Fri, 01 Jan 2010 00:00:00 +0000</pubDate>
      <guid>https://ustc-ac.github.io/publication/dblp-confcec-xu-wl-10/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Infrared Face Recognition Based on Histogram and K-Nearest Neighbor Classification</title>
      <link>https://ustc-ac.github.io/publication/dblp-confisnn-wang-l-10/</link>
      <pubDate>Fri, 01 Jan 2010 00:00:00 +0000</pubDate>
      <guid>https://ustc-ac.github.io/publication/dblp-confisnn-wang-l-10/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Musical perceptual similarity estimation using interactive genetic algorithm</title>
      <link>https://ustc-ac.github.io/publication/dblp-confcec-wang-z-10/</link>
      <pubDate>Fri, 01 Jan 2010 00:00:00 +0000</pubDate>
      <guid>https://ustc-ac.github.io/publication/dblp-confcec-wang-z-10/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Emotional speech synthesis by XML file using interactive genetic algorithms</title>
      <link>https://ustc-ac.github.io/publication/dblp-confgecco-lv-ww-09/</link>
      <pubDate>Thu, 01 Jan 2009 00:00:00 +0000</pubDate>
      <guid>https://ustc-ac.github.io/publication/dblp-confgecco-lv-ww-09/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Analysis of Relationships between Color and Emotion by Classification Based on Associations</title>
      <link>https://ustc-ac.github.io/publication/dblp-confcsse-ding-whw-08/</link>
      <pubDate>Tue, 01 Jan 2008 00:00:00 +0000</pubDate>
      <guid>https://ustc-ac.github.io/publication/dblp-confcsse-ding-whw-08/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Emotional Music Generation Using Interactive Genetic Algorithm</title>
      <link>https://ustc-ac.github.io/publication/dblp-confcsse-zhu-ww-08/</link>
      <pubDate>Tue, 01 Jan 2008 00:00:00 +0000</pubDate>
      <guid>https://ustc-ac.github.io/publication/dblp-confcsse-zhu-ww-08/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Infrared Facial Expression Recognition Using Wavelet Transform</title>
      <link>https://ustc-ac.github.io/publication/dblp-confiscsct-wang-lw-08/</link>
      <pubDate>Tue, 01 Jan 2008 00:00:00 +0000</pubDate>
      <guid>https://ustc-ac.github.io/publication/dblp-confiscsct-wang-lw-08/</guid>
      <description></description>
    </item>
    
    <item>
      <title>User Fatigue Reduction by an Absolute Rating Data-trained Predictor in IEC</title>
      <link>https://ustc-ac.github.io/publication/dblp-confcec-wang-wt-06/</link>
      <pubDate>Sun, 01 Jan 2006 00:00:00 +0000</pubDate>
      <guid>https://ustc-ac.github.io/publication/dblp-confcec-wang-wt-06/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Case-Based Facial Action Units Recognition Using Interactive Genetic Algorithm</title>
      <link>https://ustc-ac.github.io/publication/dblp-confacii-wang-x-05/</link>
      <pubDate>Sat, 01 Jan 2005 00:00:00 +0000</pubDate>
      <guid>https://ustc-ac.github.io/publication/dblp-confacii-wang-x-05/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Emotion Semantics Image Retrieval: An Brief Overview</title>
      <link>https://ustc-ac.github.io/publication/dblp-confacii-wang-w-05/</link>
      <pubDate>Sat, 01 Jan 2005 00:00:00 +0000</pubDate>
      <guid>https://ustc-ac.github.io/publication/dblp-confacii-wang-w-05/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Evaluation of User Fatigue Reduction Through IEC Rating-Scale Mapping</title>
      <link>https://ustc-ac.github.io/publication/dblp-confwstst-wang-t-05/</link>
      <pubDate>Sat, 01 Jan 2005 00:00:00 +0000</pubDate>
      <guid>https://ustc-ac.github.io/publication/dblp-confwstst-wang-t-05/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Kansei-Oriented Image Retrieval</title>
      <link>https://ustc-ac.github.io/publication/dblp-confamt-wang-chw-01/</link>
      <pubDate>Mon, 01 Jan 2001 00:00:00 +0000</pubDate>
      <guid>https://ustc-ac.github.io/publication/dblp-confamt-wang-chw-01/</guid>
      <description></description>
    </item>
    
    <item>
      <title></title>
      <link>https://ustc-ac.github.io/admin/config.yml</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>https://ustc-ac.github.io/admin/config.yml</guid>
      <description></description>
    </item>
    
    <item>
      <title>Contact</title>
      <link>https://ustc-ac.github.io/contact/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>https://ustc-ac.github.io/contact/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Graduated</title>
      <link>https://ustc-ac.github.io/graduated/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>https://ustc-ac.github.io/graduated/</guid>
      <description></description>
    </item>
    
    <item>
      <title>NVIE</title>
      <link>https://ustc-ac.github.io/medic/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>https://ustc-ac.github.io/medic/</guid>
      <description></description>
    </item>
    
    <item>
      <title>NVIE</title>
      <link>https://ustc-ac.github.io/nvie/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>https://ustc-ac.github.io/nvie/</guid>
      <description></description>
    </item>
    
    <item>
      <title>People</title>
      <link>https://ustc-ac.github.io/people/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>https://ustc-ac.github.io/people/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Research</title>
      <link>https://ustc-ac.github.io/research/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>https://ustc-ac.github.io/research/</guid>
      <description></description>
    </item>
    
  </channel>
</rss>
