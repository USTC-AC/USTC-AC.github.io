[{"authors":["Shi Yin"],"categories":null,"content":"","date":1690712765,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":1690712765,"objectID":"8fc2450bc90592d1f27b659c82b8444d","permalink":"https://ustc-ac.github.io/author/shi-yin/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/author/shi-yin/","section":"authors","summary":"","tags":null,"title":"Shi Yin","type":"authors"},{"authors":null,"categories":null,"content":"","date":1689501180,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":1689501180,"objectID":"81c6ddabdf7be2da26d8dc7e70741be1","permalink":"https://ustc-ac.github.io/author/cong-liang/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/author/cong-liang/","section":"authors","summary":"","tags":null,"title":"Cong Liang","type":"authors"},{"authors":null,"categories":null,"content":"Latest：\nHierarchical control of soft manipulators towards unstructured interactions, The International Journal of Robotics Research (IJRR), 2021. A two-level approach for solving the inverse kinematics of an extensible soft arm considering viscoelastic behavior. IEEE International Conference on Robotics and Automation (ICRA), 2017. Design and simulation analysis of a soft manipulator based on honeycomb pneumatic networks, IEEE International Conference on Robotics and Biomimetics (ROBIO), 2016. ","date":-62135596800,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":-62135596800,"objectID":"1b3d500dae0b9bcd0295819108d36f54","permalink":"https://ustc-ac.github.io/author/hao-jiang/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/author/hao-jiang/","section":"authors","summary":"Latest：\nHierarchical control of soft manipulators towards unstructured interactions, The International Journal of Robotics Research (IJRR), 2021. A two-level approach for solving the inverse kinematics of an extensible soft arm considering viscoelastic behavior.","tags":null,"title":"Hao Jiang","type":"authors"},{"authors":null,"categories":null,"content":"","date":1617710091,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":1625057752,"objectID":"8d31189c68a5127a6bdbcf65514c28a5","permalink":"https://ustc-ac.github.io/author/zhiwei-xu/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/author/zhiwei-xu/","section":"authors","summary":"","tags":null,"title":"Zhiwei Xu","type":"authors"},{"authors":null,"categories":null,"content":"","date":1690277152,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":1690277152,"objectID":"836a9d6e864c90c0dbec3173235b0a19","permalink":"https://ustc-ac.github.io/author/jicai-pan/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/author/jicai-pan/","section":"authors","summary":"","tags":null,"title":"Jicai Pan","type":"authors"},{"authors":null,"categories":null,"content":"","date":1690277152,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":1690277152,"objectID":"e356e42143b62ae14eee17f3b74de575","permalink":"https://ustc-ac.github.io/author/yi-wu/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/author/yi-wu/","section":"authors","summary":"","tags":null,"title":"Yi Wu","type":"authors"},{"authors":null,"categories":null,"content":"","date":1690277152,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":1690277152,"objectID":"a28f1d942f0d4cc8c50008c1b6dccf52","permalink":"https://ustc-ac.github.io/author/zhouan-zhu/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/author/zhouan-zhu/","section":"authors","summary":"","tags":null,"title":"Zhouan Zhu","type":"authors"},{"authors":["Shangfei Wang"],"categories":null,"content":"Shangfei Wang is a professor of artificial intelligence at the USTC Robotics Lab. Her research interests include Pattern Recognition, Affective Computing, Probabilistic Graphical Models, Computation Intelligence. She leads the USTC Affective Computing group.\n","date":1689501180,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":1689501180,"objectID":"ba85e4354e6cb29b1b77a61dc5730b4f","permalink":"https://ustc-ac.github.io/author/shangfei-wang/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/author/shangfei-wang/","section":"authors","summary":"Shangfei Wang is a professor of artificial intelligence at the USTC Robotics Lab. Her research interests include Pattern Recognition, Affective Computing, Probabilistic Graphical Models, Computation Intelligence. She leads the USTC Affective Computing group.","tags":null,"title":"Shangfei Wang","type":"authors"},{"authors":null,"categories":null,"content":"","date":1689067552,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":1689067552,"objectID":"cf771825d341f9d3d30ae1b9ca45b078","permalink":"https://ustc-ac.github.io/author/haofan-zhang/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/author/haofan-zhang/","section":"authors","summary":"","tags":null,"title":"Haofan Zhang","type":"authors"},{"authors":null,"categories":null,"content":"","date":1689067552,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":1689067552,"objectID":"b6580abbb078542222da510d781441f7","permalink":"https://ustc-ac.github.io/author/jiahe-wang/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/author/jiahe-wang/","section":"authors","summary":"","tags":null,"title":"Jiahe Wang","type":"authors"},{"authors":null,"categories":null,"content":"","date":1670406780,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":1670406780,"objectID":"fe62980d9b7595dea5f7502152ea9bde","permalink":"https://ustc-ac.github.io/author/haihan-wang/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/author/haihan-wang/","section":"authors","summary":"","tags":null,"title":"Haihan Wang","type":"authors"},{"authors":null,"categories":null,"content":"","date":1670406780,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":1670406780,"objectID":"32b8ab2f5aead77d6201ee0620299c26","permalink":"https://ustc-ac.github.io/author/lin-fang/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/author/lin-fang/","section":"authors","summary":"","tags":null,"title":"Lin Fang","type":"authors"},{"authors":null,"categories":null,"content":"","date":1656409980,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":1656409980,"objectID":"05612a78daac2bab7cd5e856c81e8922","permalink":"https://ustc-ac.github.io/author/xiangyu-miao/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/author/xiangyu-miao/","section":"authors","summary":"","tags":null,"title":"Xiangyu Miao","type":"authors"},{"authors":null,"categories":null,"content":"Yanan Chang received his BS from Chong Qing University in 2016, and he is currently pursuing his Ph.D in Computer Science in the University of Science and Technology of China, Hefei, China. His research interest is affective computing.\n","date":1648806780,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":1648806780,"objectID":"b8a8d609fdfcef6a090ddd1afda459e2","permalink":"https://ustc-ac.github.io/author/yanan-chang/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/author/yanan-chang/","section":"authors","summary":"Yanan Chang received his BS from Chong Qing University in 2016, and he is currently pursuing his Ph.D in Computer Science in the University of Science and Technology of China, Hefei, China.","tags":null,"title":"Ya'nan Chang","type":"authors"},{"authors":null,"categories":null,"content":"Publications in CityU:\nCLIP-NeRF: Text-and-Image Driven Manipulation of Neural Radiance Fields\nComputer Vision and Pattern Recognition (CVPR), 2022. [Project]\nDeep Portrait Lighting Enhancement with 3D Guidance\nEurographics Symposium on Rendering, selected to CGF Track (EGSR \u0026amp; CGF), 2021. [Project]\nCross-Domain and Disentangled Face Manipulation with 3D Guidance\nIEEE Transactions on Visualization and Computer Graphics, 2021. [Project]\n","date":1638697980,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":1638697980,"objectID":"c4086e262049cff114a736bd4d323119","permalink":"https://ustc-ac.github.io/author/can-wang/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/author/can-wang/","section":"authors","summary":"Publications in CityU:\nCLIP-NeRF: Text-and-Image Driven Manipulation of Neural Radiance Fields\nComputer Vision and Pattern Recognition (CVPR), 2022. [Project]\nDeep Portrait Lighting Enhancement with 3D Guidance\nEurographics Symposium on Rendering, selected to CGF Track (EGSR \u0026amp; CGF), 2021.","tags":null,"title":"Can Wang","type":"authors"},{"authors":null,"categories":null,"content":"","date":1619690581,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":1625057755,"objectID":"cb9feb3d943e6a681eefd64fc179cbe8","permalink":"https://ustc-ac.github.io/author/bin-xia/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/author/bin-xia/","section":"authors","summary":"","tags":null,"title":"Bin Xia","type":"authors"},{"authors":null,"categories":null,"content":"","date":1613649480,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":1625057757,"objectID":"2ec53fca9d2f2a444fcdd1fed3ab6885","permalink":"https://ustc-ac.github.io/author/guozhu-peng/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/author/guozhu-peng/","section":"authors","summary":"","tags":null,"title":"Guozhu Peng","type":"authors"},{"authors":null,"categories":null,"content":"","date":1613649480,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":1625057750,"objectID":"bb97f1e5f3c3b3eddfccab195789c30d","permalink":"https://ustc-ac.github.io/author/zhuangqiang-zheng/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/author/zhuangqiang-zheng/","section":"authors","summary":"","tags":null,"title":"Zhuangqiang Zheng","type":"authors"},{"authors":null,"categories":null,"content":"","date":1609459200,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":1625057763,"objectID":"af7482140382ac613bfb7562500ced31","permalink":"https://ustc-ac.github.io/author/bowen-pan/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/author/bowen-pan/","section":"authors","summary":"","tags":null,"title":"Bowen Pan","type":"authors"},{"authors":null,"categories":null,"content":"","date":1609459200,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":1625057759,"objectID":"f254c08ac40d918b1ca21fc52a95d37f","permalink":"https://ustc-ac.github.io/author/longfei-hao/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/author/longfei-hao/","section":"authors","summary":"","tags":null,"title":"Longfei Hao","type":"authors"},{"authors":null,"categories":null,"content":"","date":1594633980,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":1625057755,"objectID":"6f98f7d5aba7183f90520e918ab8b7ec","permalink":"https://ustc-ac.github.io/author/heyan-ding/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/author/heyan-ding/","section":"authors","summary":"","tags":null,"title":"Heyan Ding","type":"authors"},{"authors":null,"categories":null,"content":"","date":1577836800,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":1625057762,"objectID":"e0743867cec0cd5bd6c5ec3876f684e6","permalink":"https://ustc-ac.github.io/author/jiajia-yang/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/author/jiajia-yang/","section":"authors","summary":"","tags":null,"title":"Jiajia Yang","type":"authors"},{"authors":null,"categories":null,"content":"","date":1546300800,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":1625057754,"objectID":"83f93859bdf2ed6dacd38a11698b7331","permalink":"https://ustc-ac.github.io/author/qisheng-jiang/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/author/qisheng-jiang/","section":"authors","summary":"","tags":null,"title":"Qisheng Jiang","type":"authors"},{"authors":null,"categories":null,"content":"","date":1546300800,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":1625057761,"objectID":"4ee46be285437c3a5a85bd2b193de8fe","permalink":"https://ustc-ac.github.io/author/shiyu-chen/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/author/shiyu-chen/","section":"authors","summary":"","tags":null,"title":"Shiyu Chen","type":"authors"},{"authors":null,"categories":null,"content":"","date":1514764800,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":1625057762,"objectID":"bcbfa4556eaffa4bc47c44f9e77306ef","permalink":"https://ustc-ac.github.io/author/tanfang-chen/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/author/tanfang-chen/","section":"authors","summary":"","tags":null,"title":"Tanfang Chen","type":"authors"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":-62135596800,"objectID":"7b1e02e7603b244d6f4f19f5dba5ebca","permalink":"https://ustc-ac.github.io/author/caichao-zhang/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/author/caichao-zhang/","section":"authors","summary":"","tags":null,"title":"Caichao Zhang","type":"authors"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":-62135596800,"objectID":"8b98bf6f7a5a3b23fe27e30b8891d3b8","permalink":"https://ustc-ac.github.io/author/feiyi-zheng/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/author/feiyi-zheng/","section":"authors","summary":"","tags":null,"title":"Feiyi Zheng","type":"authors"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":-62135596800,"objectID":"0c01cd86dacbe42f34bba5210270a408","permalink":"https://ustc-ac.github.io/author/hao-sun/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/author/hao-sun/","section":"authors","summary":"","tags":null,"title":"Hao Sun","type":"authors"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":-62135596800,"objectID":"48413006e06920dfd0861b2834c3bd1a","permalink":"https://ustc-ac.github.io/author/jiabao-mu/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/author/jiabao-mu/","section":"authors","summary":"","tags":null,"title":"Jiabao Mu","type":"authors"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":-62135596800,"objectID":"f6f64f71fc19b4a942505d373b0b9cad","permalink":"https://ustc-ac.github.io/author/jiale-huang/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/author/jiale-huang/","section":"authors","summary":"","tags":null,"title":"Jiale Huang","type":"authors"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":-62135596800,"objectID":"d9a4da54e9e9465f6a2c5641d2b7eacf","permalink":"https://ustc-ac.github.io/author/jiaqiang-wu/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/author/jiaqiang-wu/","section":"authors","summary":"","tags":null,"title":"Jiaqiang Wu","type":"authors"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":-62135596800,"objectID":"27f3773d7c457c983b8da8aec3f8e387","permalink":"https://ustc-ac.github.io/author/jingqiao-lu/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/author/jingqiao-lu/","section":"authors","summary":"","tags":null,"title":"Jingqiao Lu","type":"authors"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":-62135596800,"objectID":"a13733ee513cbbe48eb35b580e1c59c2","permalink":"https://ustc-ac.github.io/author/jingtian-li/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/author/jingtian-li/","section":"authors","summary":"","tags":null,"title":"Jingtian Li","type":"authors"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":-62135596800,"objectID":"01780852c47ec1bdf4fd573307326ec3","permalink":"https://ustc-ac.github.io/author/mintao-zhang/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/author/mintao-zhang/","section":"authors","summary":"","tags":null,"title":"Mintao Zhang","type":"authors"},{"authors":null,"categories":null,"content":"Qiong Li received her B.S. degree in computer science from Shandong University in 2019, and she is currently pursuing her M.S. degree in Computer Science in the University of Science and Technology of China, Hefei, China. Her research interests cover affective computing and machine learning.\n","date":-62135596800,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":-62135596800,"objectID":"428c18211c9831744ad576aeccdada15","permalink":"https://ustc-ac.github.io/author/qiong-li/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/author/qiong-li/","section":"authors","summary":"Qiong Li received her B.S. degree in computer science from Shandong University in 2019, and she is currently pursuing her M.S. degree in Computer Science in the University of Science and Technology of China, Hefei, China.","tags":null,"title":"Qiong Li","type":"authors"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":-62135596800,"objectID":"8cdbaf345bd1c8aa484d49371f68420f","permalink":"https://ustc-ac.github.io/author/xin-li/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/author/xin-li/","section":"authors","summary":"","tags":null,"title":"Xin Li","type":"authors"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":-62135596800,"objectID":"f7fe6c575b1bc14d855c4caddff11763","permalink":"https://ustc-ac.github.io/author/xuandong-huang/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/author/xuandong-huang/","section":"authors","summary":"","tags":null,"title":"Xuandong Huang","type":"authors"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":-62135596800,"objectID":"1a40b97f38d0af2aab10d64cc6a28a85","permalink":"https://ustc-ac.github.io/author/xuewei-li/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/author/xuewei-li/","section":"authors","summary":"","tags":null,"title":"Xuewei Li","type":"authors"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":-62135596800,"objectID":"de4abcd54cc3428919e52807d6218e16","permalink":"https://ustc-ac.github.io/author/yadong-liu/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/author/yadong-liu/","section":"authors","summary":"","tags":null,"title":"Yadong Liu","type":"authors"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":-62135596800,"objectID":"676a3f7cb3c8b84f343de7b395a90efd","permalink":"https://ustc-ac.github.io/author/yufei-xiao/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/author/yufei-xiao/","section":"authors","summary":"","tags":null,"title":"Yufei Xiao","type":"authors"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":-62135596800,"objectID":"33ae7b0270f5b789193ee618beac759b","permalink":"https://ustc-ac.github.io/author/yunes-al-dhabi/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/author/yunes-al-dhabi/","section":"authors","summary":"","tags":null,"title":"Yunes Al-Dhabi","type":"authors"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":-62135596800,"objectID":"ed63a07276c03bc0cd13151464d3822f","permalink":"https://ustc-ac.github.io/author/zhenjie-liu/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/author/zhenjie-liu/","section":"authors","summary":"","tags":null,"title":"Zhenjie Liu","type":"authors"},{"authors":["Shi Yin","Cong Liang","Heyan Ding","Shangfei Wang"],"categories":[],"content":"Depression has a severe effect on people’s life. The artificial therapy of depression is facing a shortage of expert therapists. The automatic detection of depression can be an auxiliary means of artificial therapy. As a delicate mental symptom, depression cannot be accurately distinguished via single modal observation. To address this, our work utilizes vision, audio and text features. For vision features, gaze direction, 3D position, the orientation of the head and 17 facial action units are considered. For audio, the hidden layers of pre-trained deep models are used. For text, we build features from two aspects. The first one is the semantic embedding of the whole sentence. The second one is the emotional distribution of several words with obvious emotional tendencies. A subject engaging in a multi-turns conversation may produce several video clips sharing a similar theme. Facing the hierarchical characteristic of such data, we design a framework consisting of two hierarchies of bidirectional long short term memories (LSTM) for the depression detection task. The first hierarchy of bidirectional LSTM extracts vision and audio features for every video clip. The second hierarchy of bidirectional LSTM fuses the visual, audio and textual features and regresses the degree of depression. The indicator in concern in the DDS challenge is the PHQ-8 Score, while the proposed method jointly learns the PTSD Severity to facilitate the prediction of the PHQ-8 Score under a multi-task learning schema.\nThe framework of the proposed hierarchical recurrent model. The first hierarchy of Bi-LSTM fuses multi-modal features of a single video clip. And the second hierarchy fuses all clips of a conversation to predict the result. We conduct training on the official training set and test it on the official testing set of the challenge. Compared to the optimal results in baseline methods, our method increases Concordance Correlation Coefficients (CCC) by 19.64% and decreases Root Mean Square Error (RMSE) by 1.79% on the development set, and also increases CCC by 268.33% and decreases RMSE by 13.66% on the testing set, which means a significant performance compared to the baseline methods.\n","date":1571616000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1625057755,"objectID":"a6cff2c28802de0133ed65ab4920b2c3","permalink":"https://ustc-ac.github.io/publication/dblp-confmm-yin-ldw-19/","publishdate":"2021-06-30T12:55:55.008148Z","relpermalink":"/publication/dblp-confmm-yin-ldw-19/","section":"publication","summary":"To utilize all textual, audio, and visual information to predict the severity of depression., we proposed a hierarchical recurrent model. Our method contains two hierarchies of bidirectional long short term memories (LSTM) to fuse multi-modal features and predict the PHQ-8 score. We are the runner-up in the AVEC2019 challenge of depression detection.","tags":["Depression Detection"],"title":"A Multi-Modal Hierarchical Recurrent Neural Network for Depression Detection","type":"publication"},{"authors":null,"categories":null,"content":"","date":1691053077,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1691053077,"objectID":"d13e06c518fd96df20bcc337acac04fe","permalink":"https://ustc-ac.github.io/datasets/medic/","publishdate":"2023-08-03T08:57:57.443Z","relpermalink":"/datasets/medic/","section":"datasets","summary":"","tags":null,"title":"MEDIC","type":"widget_page"},{"authors":["Shi Yin"],"categories":null,"content":"Shi Yin was awarded the 2022 ACM CHINA COUNCIL HEFEI CHAPTER DOCTORAL DISSERTATION AWARD.\n","date":1690712765,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1690712765,"objectID":"e142aea10f9e0765fddfba67e96ab84b","permalink":"https://ustc-ac.github.io/post/award-2023-yinshi/","publishdate":"2023-07-30T10:26:05.898Z","relpermalink":"/post/award-2023-yinshi/","section":"post","summary":"Shi Yin was awarded the 2022 ACM CHINA COUNCIL HEFEI CHAPTER DOCTORAL DISSERTATION AWARD.\n","tags":null,"title":"One student from our group was awarded Doctoral Dissertation Award","type":"post"},{"authors":["Yi Wu","Zhouan Zhu","Jicai Pan"],"categories":null,"content":"Three papers by Yi Wu, Zhouan Zhu, and Jicai Pan are accepted by ACMMM2023\n","date":1690277152,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1690277152,"objectID":"10eeec2f1f8724248634ea2963696297","permalink":"https://ustc-ac.github.io/post/2023-acmmm-threepapers/","publishdate":"2023-07-25T09:25:52.898Z","relpermalink":"/post/2023-acmmm-threepapers/","section":"post","summary":"Three papers by Yi Wu, Zhouan Zhu, and Jicai Pan are accepted by ACMMM2023\n","tags":null,"title":"Three papers are accepted by ACMMM2023","type":"post"},{"authors":["Yi Wu","Shangfei Wang"],"categories":null,"content":"","date":1689501180,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1689501180,"objectID":"9eb88fae5c36cb9e0fae0b8f398d43be","permalink":"https://ustc-ac.github.io/publication/manual-tac23-wu/","publishdate":"2023-07-16T09:53:00.805Z","relpermalink":"/publication/manual-tac23-wu/","section":"publication","summary":"Although expression descriptions provide additional information about facial behaviors despite of different poses, and pose features are beneficial to adapt to pose variety, neither has been fully leveraged in facial expression recognition. This paper proposes a pose-aware text-assisted facial expression recognition method using cross-modality attention. Specifically, the method contains three components. The pose feature extractor extracts pose-related features from facial images, and then cooperates with a fully-connected layer for pose classification. When poses can be clearly discriminated and classified, features obtained from the extractor can represent the corresponding poses. To eliminate bias due to appearance and illumination, cluster centers are taken as the final pose features. The text feature extractor obtains embeddings from expression descriptions. These descriptions are first passed through Intra-Exp attention to obtain preliminary embeddings. To leverage the correlations among expressions, all expression embeddings are then concatenated and passed through Inter-Exp attention. The cross-modality module attempts to learn attention maps that distinguish the importance of facial regions by using prior knowledge about poses and expression descriptions. The image features weighted by the attention maps are utilized to recognize pose and expression jointly. Experiments on three benchmark datasets demonstrate the superiority of the proposed method.","tags":["Expression Recognition","expression descriptions"],"title":"Pose-Aware Facial Expression Recognition Assisted by Expression Descriptions","type":"publication"},{"authors":["Cong Liang","Shangfei Wang","Xiaoping Chen"],"categories":null,"content":"","date":1689501180,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1689501180,"objectID":"e1ac3e878b93ad9858e86317ac775cb9","permalink":"https://ustc-ac.github.io/publication/manual-icme23-liang/","publishdate":"2023-07-16T09:53:00.805Z","relpermalink":"/publication/manual-icme23-liang/","section":"publication","summary":"Cloud-based expression recognition from high-resolution facial images may put the subjects' privacy at risk. We identify two kinds of privacy leakage, the appearance leakage in which the visual appearances of subjects are disclosed and the identity-pattern leakage in which the identity information of subjects is dug out. To address both leakages, we propose privacy-protected facial expression recognition from low-resolution facial images with the help of high-resolution facial images. Specifically, to prevent appearance leakage, we propose to extract identity-invariant representations from downsampled images, from which the visually distinguishable appearances cannot be recovered. To prevent identity-pattern leakage, we propose to eliminate the identity information from the extracted representations by leveraging the disentangled representations of high-resolution images as privileged information. After training, our method can fully capture identity-invariant representations from downsampled images for expression recognition without the requirement of high-resolution samples. These privacy-protected representations can be safely transmitted through the Internet. Experimental results in different scenarios demonstrate that the proposed method protects privacy without significantly inhibiting facial expression recognition.","tags":["Facial Expression Recognition","Privacy-Protection"],"title":"Privacy-Protected Facial Expression Recognition Augmented by High-Resolution Facial Images","type":"publication"},{"authors":["Yi Wu"],"categories":null,"content":"“Pose-Aware Facial Expression Recognition Assisted by Expression Descriptions” by Yi Wu, has been accepted for publication in a future issue of IEEE Transactions on Affective Computing journal.\n","date":1689499552,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1689499552,"objectID":"ea56a86079bec51698bd7b19d1c38f4a","permalink":"https://ustc-ac.github.io/post/2023-tac-wu/","publishdate":"2023-07-16T09:25:52.898Z","relpermalink":"/post/2023-tac-wu/","section":"post","summary":"“Pose-Aware Facial Expression Recognition Assisted by Expression Descriptions” by Yi Wu, has been accepted for publication in a future issue of IEEE Transactions on Affective Computing journal.\n","tags":null,"title":"One paper is accepted by IEEE Transactions on Affective Computing","type":"post"},{"authors":["Cong Liang"],"categories":null,"content":"One paper by Cong Liang is accepted by ICME23\n","date":1689413152,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1689413152,"objectID":"70d0527e0f3889d2d93fca884ceb5460","permalink":"https://ustc-ac.github.io/post/2023-icme-liang/","publishdate":"2023-07-15T09:25:52.898Z","relpermalink":"/post/2023-icme-liang/","section":"post","summary":"One paper by Cong Liang is accepted by ICME23\n","tags":null,"title":"One paper is accepted by ICME2023","type":"post"},{"authors":["Cong Liang","Jiahe Wang","Haofan Zhang"],"categories":null,"content":"Liang Cong, Wang Jiahe and Zhang Haofan participated in the REACT2023 competition (Challenge@ACM-MM23) and won the first place.\n","date":1689067552,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1689067552,"objectID":"39fc26c5daea6eaee3b96f2ac0498825","permalink":"https://ustc-ac.github.io/post/2023-acmmm-react/","publishdate":"2023-07-11T09:25:52.898Z","relpermalink":"/post/2023-acmmm-react/","section":"post","summary":"Liang Cong, Wang Jiahe and Zhang Haofan participated in the REACT2023 competition (Challenge@ACM-MM23) and won the first place.\n","tags":null,"title":"We have gotten the first price of REACT2023","type":"post"},{"authors":null,"categories":null,"content":"Our work Low-Resolution Face Recognition Enhanced by High-Resolution Facial Images received the FG2023 Best Paper\n","date":1674201600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1674201600,"objectID":"86dfe0e144e0a5ba6ec73e19aabc6f9c","permalink":"https://ustc-ac.github.io/post/2023-fg-best/","publishdate":"2023-01-20T08:00:00Z","relpermalink":"/post/2023-fg-best/","section":"post","summary":"Our work Low-Resolution Face Recognition Enhanced by High-Resolution Facial Images received the FG2023 Best Paper\n","tags":null,"title":"One paper is awarded the FG2023 Best Paper","type":"post"},{"authors":["Jiahe Wang","Shangfei Wang"],"categories":null,"content":"","date":1670493180,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1670493180,"objectID":"3dec940a2df2b4d09eca8afe86dd6a51","permalink":"https://ustc-ac.github.io/publication/manual-accv22-wang/","publishdate":"2022-12-08T09:53:00.805Z","relpermalink":"/publication/manual-accv22-wang/","section":"publication","summary":"Recent studies on occluded facial expression recognition typically required fully expression-annotated facial images for training. However, it is time consuming and expensive to collect a large number of facial images with various occlusions and expression annotations. To address this problem, we propose an occluded facial expression recognition method through self-supervised learning, which leverages the profusion of available unlabeled facial images to explore robust facial representations. Specifically, we generate a variety of occluded facial images by randomly adding occlusions to unlabeled facial images. Then we define occlusion prediction as the pretext task for representation learning. We also adopt contrastive learning to make facial representation of a facial image and those of its variations with synthesized occlusions close. Finally, we train an expression classifier as the downstream task. The experimental results on several databases containing both synthesized and realistic occluded facial images demonstrate the superiority of the proposed method over state-of-the-art methods.","tags":["Expression Recognition"],"title":"Occluded Facial Expression Recognition using Self-supervised Learning","type":"publication"},{"authors":["Lin Fang","Shangfei Wang"],"categories":null,"content":"","date":1670406780,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1670406780,"objectID":"f21e2bcdb7c593f49abae8b08e15e8e3","permalink":"https://ustc-ac.github.io/publication/manual-fg23-fang/","publishdate":"2022-12-07T09:53:00.805Z","relpermalink":"/publication/manual-fg23-fang/","section":"publication","summary":"Although the mean square error (mse) of heatmap is an intuitive loss for heatmap-based human pose estimation, the joints localization accuracy may not be improved when heatmap mse reduces. In this paper, we show that a great cause for such misalignment is the unnecessary requirement from heatmap mse on the irrelevant Gaussian parameter, i.e. maximum. The coordinate prediction is precise as long as the probability distribution held by the predicted heatmap is a well-shaped Gaussian distribution and has the same center as the ground truth. However, heatmap mse unnecessarily requires the Gaussian distribution to hold the same maximum as the ground truth. Correspondingly, we introduce mse on the image gradients of the target and predicted heatmap (referred to as gradmap mse) to focus on the shape of the heatmap. Combining heatmap and gradmap mse, we propose a simple yet effective Shape Aware Loss (SAL) method. Being model- agnostic, our method can benefit various existing models. We apply SAL to the three latest network architectures and obtain performance improvements for all of them. Comparisons of the visualized predicted heatmaps further prove the effectiveness of the proposed method.","tags":["Human Pose Estimation"],"title":"Human Pose Estimation with Shape Aware Loss","type":"publication"},{"authors":["Haihan Wang","Shangfei Wang"],"categories":null,"content":"","date":1670406780,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1670406780,"objectID":"ce17573c1df1361b0bcadb4c8f7ecca7","permalink":"https://ustc-ac.github.io/publication/manual-fg23-wang/","publishdate":"2022-12-07T09:53:00.805Z","relpermalink":"/publication/manual-fg23-wang/","section":"publication","summary":"Despite recent advances in high-resolution (HR) face recognition, recognizing identities from low-resolution (LR) facial images remains challenging due to the absence of facial shape and detail. Current research focuses solely on reducing the distribution discrepancy between the HR and LR embeddings from the output layer, rather than thoroughly investigating the superiority of HR facial images for improved performance. In this paper, we propose a novel low-resolution face recognition method enhanced by the guidance of high-resolution facial images in both feature map space and embedding space. Specifically, in feature map space, the similarity constraint across the multi-layer feature maps is adopted to align the intermediate features of facial images. Then we introduce multiple generators to recover HR images from extracted feature maps and utilize the reconstructed loss to supplement the missing facial details in LR images. In embedding space, we propose a supervised auxiliary contrastive loss to encourage the paired HR and LR embedding from the same class to be pulled together, whereas those from different classes are pushed apart. The one-to-many matching strategy and the adaptive weight adjustment strategy are applied to make the network adapt to the inputs of different resolutions. Experiments on four benchmark datasets with both synthesized and realistic LR facial images demonstrate the superiority of the proposed method to state-of-the-art.","tags":["Face Recognition"],"title":"Low-Resolution Face Recognition Enhanced by High-Resolution Facial Images","type":"publication"},{"authors":null,"categories":null,"content":"One paper by Jiahe Wang and Heyan Ding is accepted by ACCV22\n","date":1664697600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1664697600,"objectID":"b02228cec5c4974fa5059e7b832b8ece","permalink":"https://ustc-ac.github.io/post/2022-accv-wang/","publishdate":"2022-10-02T08:00:00Z","relpermalink":"/post/2022-accv-wang/","section":"post","summary":"One paper by Jiahe Wang and Heyan Ding is accepted by ACCV22\n","tags":null,"title":"One paper is accepted by ACCV2022","type":"post"},{"authors":null,"categories":null,"content":"Two papers by Lin Fang and Haihan Wang are accepted FG 2023\n","date":1662888352,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1662888352,"objectID":"fc0cac5cbab27cd5aaaf99b635b5eacb","permalink":"https://ustc-ac.github.io/post/2023-fg-two/","publishdate":"2022-09-11T09:25:52.898Z","relpermalink":"/post/2023-fg-two/","section":"post","summary":"Two papers by Lin Fang and Haihan Wang are accepted FG 2023\n","tags":null,"title":"Two papers are accepted by FG2023","type":"post"},{"authors":null,"categories":null,"content":"Our work Adversarial Stacking Ensemble for Facial Landmark Tracking received the ICPR2022 Track 4 Best Scientific Paper\n","date":1661673600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1661673600,"objectID":"8cb7f1fd403bf0947aab1d63296e34cf","permalink":"https://ustc-ac.github.io/post/2022-icpr-best/","publishdate":"2022-08-28T08:00:00Z","relpermalink":"/post/2022-icpr-best/","section":"post","summary":"Our work Adversarial Stacking Ensemble for Facial Landmark Tracking received the ICPR2022 Track 4 Best Scientific Paper\n","tags":null,"title":"One paper is awarded the ICPR2022 Track 4 Best Scientific Paper","type":"post"},{"authors":null,"categories":null,"content":"Pro. Shangfei Wang will deliver a talk name Video emotional content analysis in VALSE2022\n","date":1660987552,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1660987552,"objectID":"984524f7824b4910c52e4abf6d31c699","permalink":"https://ustc-ac.github.io/post/talk-2022-valse/","publishdate":"2022-08-20T09:25:52.898Z","relpermalink":"/post/talk-2022-valse/","section":"post","summary":"Pro. Shangfei Wang will deliver a talk name Video emotional content analysis in VALSE2022\n","tags":null,"title":"Incoming Talk in VALSE2022","type":"post"},{"authors":null,"categories":null,"content":"Three papers by Wang, Miao, and Pan are accepted by ACMMM2022\n","date":1657440000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1657440000,"objectID":"0bfd03cf2afa1afd66e0a023e3a98e9a","permalink":"https://ustc-ac.github.io/post/2022-mm-three/","publishdate":"2022-07-10T08:00:00Z","relpermalink":"/post/2022-mm-three/","section":"post","summary":"Three papers by Wang, Miao, and Pan are accepted by ACMMM2022\n","tags":null,"title":"Three papers are accepted by ACMMM2022","type":"post"},{"authors":["Xiangyu Miao","Shangfei Wang"],"categories":null,"content":"","date":1656409980,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1656409980,"objectID":"957b14793067d6471406356fab5e8231","permalink":"https://ustc-ac.github.io/publication/manual-mm22-miao/","publishdate":"2022-06-28T09:53:00.805Z","relpermalink":"/publication/manual-mm22-miao/","section":"publication","summary":"","tags":["Face Recognition"],"title":"Knowledge Guided Representation Disentanglement for Face Recognition from Low Illumination Images","type":"publication"},{"authors":["Jicai Pan","Shangfei Wang","Lin Fang"],"categories":null,"content":"","date":1656409980,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1656409980,"objectID":"5a7565bb08f188c61625fb1eef206fa0","permalink":"https://ustc-ac.github.io/publication/manual-mm22-pan/","publishdate":"2022-06-28T09:53:00.805Z","relpermalink":"/publication/manual-mm22-pan/","section":"publication","summary":"","tags":["Multimedia"],"title":"Representation Learning through Multimodal Attention and Time-sync Comments for Video Affective Content Analysis","type":"publication"},{"authors":["Haihan Wang","Shangfei Wang","Lin Fang"],"categories":null,"content":"","date":1656409980,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1656409980,"objectID":"f68ad2c3e5fe77ba7aa5fcd8cab3ac3b","permalink":"https://ustc-ac.github.io/publication/manual-mm22-wang/","publishdate":"2022-06-28T09:53:00.805Z","relpermalink":"/publication/manual-mm22-wang/","section":"publication","summary":"","tags":["Face Recognition"],"title":"Two-Stage Multi-Scale Resolution-Adaptive Network for Low-Resolution Face Recognition","type":"publication"},{"authors":null,"categories":null,"content":"Our work Adversarial Stacking Ensemble for Facial Landmark Tracking (ID 1281) by Yin and Fang is accepted by ICPR22\n","date":1648886400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1648886400,"objectID":"383f93723606b7235a2b0da884c91314","permalink":"https://ustc-ac.github.io/post/2022-icpr-fang/","publishdate":"2022-04-02T08:00:00Z","relpermalink":"/post/2022-icpr-fang/","section":"post","summary":"Our work Adversarial Stacking Ensemble for Facial Landmark Tracking (ID 1281) by Yin and Fang is accepted by ICPR22\n","tags":null,"title":"One paper is accepted by ICPR2022","type":"post"},{"authors":["Lin Fang","Shi Yin","Shangfei Wang","Ya'nan Chang"],"categories":null,"content":"","date":1648806780,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1648806780,"objectID":"dbbcea181f9f70d1e41d6bb59afb09fd","permalink":"https://ustc-ac.github.io/publication/manual-icpr22-fang/","publishdate":"2022-04-01T09:53:00.805Z","relpermalink":"/publication/manual-icpr22-fang/","section":"publication","summary":"Current approaches for facial landmark tracking predict facial landmarks through either a single tracker or an ensemble of trackers. However, the conventional ensemble is not designed for facial landmark tracking and can not capture spatial and temporal patterns of facial landmarks efficiently. In this paper, we propose to extend the conventional stacking with an adversarial training strategy to better suit the facial landmark tracking task. Specifically, the meta learner attempts to distinguish the predictions from the base learners with the ground truths, while the base learners attempt to confuse the meta learner by predicting landmarks close to the ground truths. The adversary between the two levels of learners forces them to fully capture the inherent spatial and temporal patterns of facial landmarks. Moreover, to promote the diversity of different base trackers, we design two classification tasks at both the feature level and prediction level. Experimental results on the 300VW dataset and the TF dataset demonstrate the effectiveness of our method, and we achieve state-of-the-art performances on both datasets.","tags":["Landmark Detection","Landmark Tracking"],"title":"Adversarial Stacking Ensemble for Facial Landmark Tracking","type":"publication"},{"authors":null,"categories":null,"content":"Our work Knowledge-Driven Self-Supervised Representation Learning for Facial Action Unit Recognition (ID 6728) by Chang is accepted by CVPR2022!\n","date":1646294400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1646294400,"objectID":"db98b457dfabe8cf5c3e24198fa2cc50","permalink":"https://ustc-ac.github.io/post/2022-cvpr-chang/","publishdate":"2022-03-03T08:00:00Z","relpermalink":"/post/2022-cvpr-chang/","section":"post","summary":"Our work Knowledge-Driven Self-Supervised Representation Learning for Facial Action Unit Recognition (ID 6728) by Chang is accepted by CVPR2022!\n","tags":null,"title":"One of our work is accepted by CVPR2022 !","type":"post"},{"authors":["Ya'nan Chang","Shangfei Wang"],"categories":null,"content":"","date":1646128380,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1646128380,"objectID":"370db7b78f3732273316e2ee6fa23b09","permalink":"https://ustc-ac.github.io/publication/manual-cvpr22-chang/","publishdate":"2022-03-01T09:53:00.805Z","relpermalink":"/publication/manual-cvpr22-chang/","section":"publication","summary":"Facial action unit (AU) recognition is formulated as a supervised learning problem by recent works. However, the complex labeling process makes it challenging to provide AU annotations for large amounts of facial images. To remedy this, we utilize AU labeling rules defined by the Facial Action Coding System (FACS) to design a novel knowledge-driven self-supervised representation learning framework for AU recognition. The representation encoder is trained using large amounts of facial images without AU annotations. AU labeling rules are summarized from FACS to design facial partition manners and determine correlations between facial regions. The method utilizes a backbone network to extract local facial area representations and a project head to map the representations into a low-dimensional latent space. In the latent space, a contrastive learning component leverages the inter-area difference to learn AU-related local representations while maintaining intra-area instance discrimination. Correlations between facial regions summarized from AU labeling rules are also explored to further learn representations using a predicting learning component. Evaluation on two benchmark databases demonstrates that the learned representation is powerful and data-efficient for AU recognition.","tags":["AU Recognition"],"title":"Knowledge-Driven Self-Supervised Representation Learning for Facial Action Unit Recognition","type":"publication"},{"authors":null,"categories":null,"content":"Pro. Shangfei Wang will deliver a talk name Knowing Faces and Hearts-Emotional Human-Computer Interaction Research in Harbin Institute of Technology Wuhu Research Institute Conference on Artificial Intelligence and Machine Vision (Online)\n","date":1641029152,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1641029152,"objectID":"076c0d18e3ae418f947ca212e618682a","permalink":"https://ustc-ac.github.io/post/talk-2022-ha/","publishdate":"2022-01-01T09:25:52.898Z","relpermalink":"/post/talk-2022-ha/","section":"post","summary":"Pro. Shangfei Wang will deliver a talk name Knowing Faces and Hearts-Emotional Human-Computer Interaction Research in Harbin Institute of Technology Wuhu Research Institute Conference on Artificial Intelligence and Machine Vision (Online)\n","tags":null,"title":"Incoming Talk","type":"post"},{"authors":["Guang Liang","Shangfei Wang","Can Wang"],"categories":null,"content":"","date":1638697980,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1638697980,"objectID":"84608a8cdab9db526636afce27ecdf51","permalink":"https://ustc-ac.github.io/publication/manual-fg21-liang/","publishdate":"2021-12-05T09:53:00.805Z","relpermalink":"/publication/manual-fg21-liang/","section":"publication","summary":"Pose-invariant facial expression recognition is quite challenging due to variations in facial appearance and self-occlusion caused by head rotations. In this paper, we propose an adversarial multi-view subspace learning method for pose-robust facial expression recognition. Specifically, a deep neural network is trained from face images of a certain pose to learn facial representations. Then, an adversarial strategy is adopted to force statistical similarity among the learned representations from facial images with different poses. Simultaneously, an expression classifier is trained on the learned pose-robust facial representations. Through adversarial learning, the proposed method leverages inherent dependencies among multiple pose facial images to construct pose-robust image representations and a classifier during training. The ensemble method is adopted to combine the predictions of multiple deep neural networks and the common expression classifier, so pose estimation is not required. Experimental results on four benchmark databases demonstrate the superiority of the proposed method to state-of-the-art works.","tags":["Expression Recognition"],"title":"Pose-Invariant Facial Expression Recognition","type":"publication"},{"authors":null,"categories":null,"content":"\u0026ldquo;Pose-lnvariant Facial Expression Recognition\u0026rdquo; by Guang Liang has been accepted FG 2021\n","date":1638696352,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1638696352,"objectID":"e4d1d4ec49b48f7a5bb59763869e8232","permalink":"https://ustc-ac.github.io/post/2021-fg-liang/","publishdate":"2021-12-05T09:25:52.898Z","relpermalink":"/post/2021-fg-liang/","section":"post","summary":"\u0026ldquo;Pose-lnvariant Facial Expression Recognition\u0026rdquo; by Guang Liang has been accepted FG 2021\n","tags":null,"title":"One paper is accepted by FG2021","type":"post"},{"authors":null,"categories":null,"content":"\u0026ldquo;Dual Learning for Joint Facial Landmark Detection and Action Unit Recognition\u0026rdquo; by Ya\u0026rsquo;nan Chang, has been accepted for publication in a future issue of IEEE Transactions on Affective Computing journal. You can find it in Publications column and Link\n","date":1632470400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1632470400,"objectID":"56c074833dd5e3537e16fb7af2330520","permalink":"https://ustc-ac.github.io/post/2021-journal-chang/","publishdate":"2021-09-24T08:00:00Z","relpermalink":"/post/2021-journal-chang/","section":"post","summary":"\u0026ldquo;Dual Learning for Joint Facial Landmark Detection and Action Unit Recognition\u0026rdquo; by Ya\u0026rsquo;nan Chang, has been accepted for publication in a future issue of IEEE Transactions on Affective Computing journal. You can find it in Publications column and Link\n","tags":null,"title":"One paper is accepted by IEEE Transactions on Affective Computing","type":"post"},{"authors":null,"categories":null,"content":"Pro. Shangfei Wang will deliver a talk name Facial Action Unit Recognition under Non-Full Annotation in MIPR2021\n","date":1630315552,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1630315552,"objectID":"f31c9a2ac06ec2e86ba7888cd3e4cb5b","permalink":"https://ustc-ac.github.io/post/talk-2022-mipr/","publishdate":"2021-08-30T09:25:52.898Z","relpermalink":"/post/talk-2022-mipr/","section":"post","summary":"Pro. Shangfei Wang will deliver a talk name Facial Action Unit Recognition under Non-Full Annotation in MIPR2021\n","tags":null,"title":"Incoming Talk in MIPR2021","type":"post"},{"authors":null,"categories":null,"content":"Bin Xia\u0026rsquo;s paper “Micro-Expression Recognition Enhanced by Macro-Expression from Spatial-Temporal Domain” is accepted by the 2021 Conference on IJCAI.\nLink\n","date":1624957077,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1624957077,"objectID":"35e6483d4333fe4bd7ade03fece1a595","permalink":"https://ustc-ac.github.io/post/2021-ijcai-bin-xia/","publishdate":"2021-06-29T08:57:57.443Z","relpermalink":"/post/2021-ijcai-bin-xia/","section":"post","summary":"Bin Xia\u0026rsquo;s paper “Micro-Expression Recognition Enhanced by Macro-Expression from Spatial-Temporal Domain” is accepted by the 2021 Conference on IJCAI.\nLink\n","tags":null,"title":"One paper is accepted by IJCAI 2021","type":"post"},{"authors":null,"categories":null,"content":"They accepted the award at the graduation ceremony.\nFig1. Shi Yin (first from the left) Fig2. Zhiwei Xu (first from the rightt) Fig3. Bin Xia (second from the left) ","date":1624184765,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1624184765,"objectID":"b0a08bf8e0be6f9a185b8790efb40238","permalink":"https://ustc-ac.github.io/post/outstanding-graduates/","publishdate":"2021-06-20T10:26:05.898Z","relpermalink":"/post/outstanding-graduates/","section":"post","summary":"They accepted the award at the graduation ceremony.\n","tags":null,"title":"Three students from our group were awarded outstanding graduates","type":"post"},{"authors":["Shangfei Wang","Bin Xia"],"categories":null,"content":"","date":1619690581,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1619690581,"objectID":"732e9a2edaf05996c59ef481e174e03d","permalink":"https://ustc-ac.github.io/publication/manual-ijcai21-xia/","publishdate":"2021-04-29T10:03:01.163Z","relpermalink":"/publication/manual-ijcai21-xia/","section":"publication","summary":"","tags":["Expression Recognition"],"title":"Micro-Expression Recognition Enhanced by Macro-Expression from Spatial-Temporal Domain","type":"publication"},{"authors":["Shangfei Wang","Zhiwei Xu"],"categories":null,"content":"","date":1617710091,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1617710091,"objectID":"56794359d1c3f827b13c876834dbed1b","permalink":"https://ustc-ac.github.io/publication/emotional-attention-detection-and-correlation-exploration-for-image-emotion-distribution-learning/","publishdate":"2021-04-06T11:54:51.193Z","relpermalink":"/publication/emotional-attention-detection-and-correlation-exploration-for-image-emotion-distribution-learning/","section":"publication","summary":"Current works on image emotion distribution learning typically extract visual representations from the holistic image or explore emotion-related regions in the image from a global-wise perspective. However, different regions of an image contribute differently to the arousal of each emotion. Existing works do not deeply explore corresponding emotion-aware regions of each emotion in the image, nor do they fully capture the relationship between each emotion-aware region and the emotion labels. In this paper, we propose a novel attention based emotion distribution learning method, which can explore the emotion-related regions of images from the perspective of each emotion category, and can conduct region relationship learning. Specifically, we introduce a semantic guided attention detection network to generate class-wise attention maps for each emotion and a global-wise attention map for the holistic image. Meanwhile, an emotional graph-based network is adopted to capture the correlation between each region and the emotion distribution. Experiments on several benchmark datasets demonstrate the superiority of the proposed method compared to related works.","tags":null,"title":"Emotional Attention Detection and Correlation Exploration for Image Emotion Distribution Learning","type":"publication"},{"authors":["Shangfei Wang","Guozhu Peng","Zhuangqiang Zheng","Zhiwei Xu"],"categories":null,"content":"Multimedia collections usually induce multiple emotions in audiences. The data distribution of multiple emotions can be leveraged to facilitate the learning process of emotion tagging, yet has not been thoroughly explored. To address this, we propose adversarial learning to fully capture emotion distributions for emotion tagging of multimedia data. The proposed multimedia emotion tagging approach includes an emotion classifier and a discriminator. The emotion classifier predicts emotion labels of multimedia data from their content. The discriminator distinguishes the predicted emotion labels from the ground truth labels. The emotion classifier and the discriminator are trained simultaneously in competition with each other. By jointly minimizing the traditional supervised loss and maximizing the distribution similarity between the predicted emotion labels and the ground truth emotion labels, the proposed multimedia emotion tagging approach successfully captures both the mapping function between multimedia content and emotion labels as well as prior distribution in emotion labels, and thus achieves state-of-the-art performance for multiple emotion tagging, as demonstrated by the experimental results on four benchmark databases.\nFig. The framework of the proposed multi-emotion tagging model. C is the multiple emotion tag classifier, and D is the multiple emotion tag discriminator. C(x) is the multiple emotion prediction for feature vector x. y is the ground truth multiple emotion label of x. y 0 is the real multiple emotion label from ground truth label set. The dotted line indicates that there is an one-to-one correspondence between C(x) and y. See text for details. ","date":1613649480,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1613649480,"objectID":"b3c821b94df60d466ed64fb3f1321031","permalink":"https://ustc-ac.github.io/publication/capturing-emotion-distribution-for-multimedia-emotion-tagging/","publishdate":"2021-02-18T11:58:00.964Z","relpermalink":"/publication/capturing-emotion-distribution-for-multimedia-emotion-tagging/","section":"publication","summary":"To address the statistical similarity between the predicted emotion labels and ground-truth emotion labels without any assumptions, we propose a novel emotion tagging approach through adversarial learning. Specifically, the proposed emotion tagging approach consists of an emotional tag classifier C and a discriminator D.","tags":["Emotion Tagging"],"title":"Capturing Emotion Distribution for Multimedia Emotion Tagging","type":"publication"},{"authors":["Shangfei Wang","Ya'nan Chang","Guozhu Peng","Bowen Pan"],"categories":[],"content":"","date":1609459200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1625057750,"objectID":"44ae44e32b957ac82674b527a3065228","permalink":"https://ustc-ac.github.io/publication/dblp-journalscorrabs-2106-02258/","publishdate":"2021-06-30T12:55:50.00914Z","relpermalink":"/publication/dblp-journalscorrabs-2106-02258/","section":"publication","summary":"","tags":[],"title":"Exploring Adversarial Learning for Deep Semi-Supervised Facial Action Unit Recognition","type":"publication"},{"authors":["Shangfei Wang","Shi Yin","Longfei Hao","Guang Liang"],"categories":[],"content":"","date":1609459200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1625057749,"objectID":"b755f879993bd403d8594db7a8184fda","permalink":"https://ustc-ac.github.io/publication/dblp-journalspr-wang-yhl-21/","publishdate":"2021-06-30T12:55:49.770822Z","relpermalink":"/publication/dblp-journalspr-wang-yhl-21/","section":"publication","summary":"","tags":[],"title":"Multi-task face analyses through adversarial learning","type":"publication"},{"authors":["Shi Yin","Shangfei Wang","Xiaoping Chen","Enhong Chen","Cong Liang"],"categories":null,"content":"Although heatmap regression is considered a state-of-the-art method to locate facial landmarks, it suffers from huge spatial complexity and is prone to quantization error. To address this, we propose a novel attentive one-dimensional heatmap regression method for facial landmark localization. First, we predict two groups of 1D heatmaps to represent the marginal distributions of the 𝑥 and 𝑦 coordinates. These 1D heatmaps reduce spatial complexity significantly compared to current heatmap regression methods, which use 2D heatmaps to represent the joint distributions of 𝑥 and 𝑦 coordinates. With much lower spatial complexity, the proposed method can output high-resolution 1D heatmaps despite limited GPU memory, significantly alleviating the quantization error. Second, a co-attention mechanism is adopted to model the inherent spatial patterns existing in 𝑥 and 𝑦 coordinates, and therefore the joint distributions on the 𝑥 and 𝑦 axes are also captured. Third, based on the 1D heatmap structures, we propose a facial landmark detector capturing spatial patterns for landmark detection on an image; and a tracker further capturing temporal patterns with a temporal refinement mechanism for landmark tracking. Experimental results on four benchmark databases demonstrate the superiority of our method.\nFig. The proposed detector (a) and tracker (b). ","date":1602720000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1602720000,"objectID":"8b80cecd6da22117b3f800f9eb2a9aac","permalink":"https://ustc-ac.github.io/publication/dblp-confmm-yin-wccl-20/","publishdate":"2020-10-15T00:00:00Z","relpermalink":"/publication/dblp-confmm-yin-wccl-20/","section":"publication","summary":"The main contributions of the method are three folds. First, we are the first that propose to predict 1D heatmaps on the 𝑥 and 𝑦 axes instead of using 2D heatmaps to locate landmarks and successfully alleviate the quantization error with a fully boosted output resolution. Second, we propose a co-attention module to capture the joint coordinate distribution on the two axes. Third, based on the proposed heatmap regression method, we design a facial landmark detector and tracker which achieve state-of-the-art performance.","tags":["Landmark Detection","Landmark Tracking"],"title":"Attentive One-Dimensional Heatmap Regression for Facial Landmark Detection and Tracking","type":"publication"},{"authors":["Shangfei Wang","Guozhu Peng","Heyan Ding"],"categories":null,"content":"","date":1594633980,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1594633980,"objectID":"023ece114b7fe3e09d29f486d32befa9","permalink":"https://ustc-ac.github.io/publication/dual-learning-for-facial-action-unit-detection-under-nonfull-annotation/","publishdate":"2020-07-13T09:53:00.805Z","relpermalink":"/publication/dual-learning-for-facial-action-unit-detection-under-nonfull-annotation/","section":"publication","summary":"Most methods for facial action unit (AU) recognition typically\n  require training images that are fully AU labeled. Manual AU annotation is\n  time intensive. To alleviate this, we propose a novel dual learning framework\n  and apply it to AU detection under two scenarios, that is, semisupervised AU\n  detection with partially AU-labeled and fully expression-labeled samples, and\n  weakly supervised AU detection with fully expression-labeled samples alone. We\n  leverage two forms of auxiliary information. The first is the probabilistic\n  duality between the AU detection task and its dual task, in this case, the\n  face synthesis task given AU labels. We also take advantage of the\n  dependencies among multiple AUs, the dependencies between expression and AUs,\n  and the dependencies between facial features and AUs. Specifically, the\n  proposed method consists of a classifier, an image generator, and a\n  discriminator. The classifier and generator yield face-AU-expression tuples,\n  which are forced to coverage of the ground-truth distribution. This joint\n  distribution also includes three kinds of inherent dependencies: 1) the\n  dependencies among multiple AUs; 2) the dependencies between expression and\n  AUs; and 3) the dependencies between facial features and AUs. We reconstruct\n  the inputted face and AU labels and introduce two reconstruction losses. In a\n  semisupervised scenario, the supervised loss is also incorporated into the\n  full objective for AU-labeled samples. In a weakly supervised scenario, we\n  generate pseudo paired data according to the domain knowledge about expression\n  and AUs. Semisupervised and weakly supervised experiments on three widely used\n  datasets demonstrate the superiority of the proposed method for AU detection\n  and facial synthesis tasks over current works.","tags":null,"title":"Dual Learning for Facial Action Unit Detection Under Nonfull Annotation","type":"publication"},{"authors":null,"categories":null,"content":"","date":1593421077,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1593421077,"objectID":"a89bd18f029a2d97989b02cdffd16b02","permalink":"https://ustc-ac.github.io/datasets/nvie/","publishdate":"2020-06-29T08:57:57.443Z","relpermalink":"/datasets/nvie/","section":"datasets","summary":"","tags":null,"title":"NVIE","type":"widget_page"},{"authors":["Shangfei Wang","Zhuangqiang Zheng","Shi Yin","Jiajia Yang","Qiang Ji"],"categories":[],"content":"","date":1577836800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1625057750,"objectID":"366fdf85623aa6fc522d4751ae4b081c","permalink":"https://ustc-ac.github.io/publication/dblp-journalspami-wang-zyyj-20/","publishdate":"2021-06-30T12:55:50.228375Z","relpermalink":"/publication/dblp-journalspami-wang-zyyj-20/","section":"publication","summary":"","tags":[],"title":"A Novel Dynamic Model Capturing Spatial and Temporal Patterns for Facial Expression Analysis","type":"publication"},{"authors":["Shangfei Wang","Guozhu Peng","Zhuangqiang Zheng"],"categories":[],"content":"","date":1577836800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1625057750,"objectID":"8186ee8261e9a4985254a0d22e323775","permalink":"https://ustc-ac.github.io/publication/dblp-journalstkde-wang-pz-20/","publishdate":"2021-06-30T12:55:50.65192Z","relpermalink":"/publication/dblp-journalstkde-wang-pz-20/","section":"publication","summary":"","tags":[],"title":"Capturing Joint Label Distribution for Multi-Label Classification Through Adversarial Learning","type":"publication"},{"authors":["Zhiwei Xu","Shangfei Wang","Can Wang"],"categories":[],"content":"The dependence among emotions is crucial to boost emotion tagging. In this paper, we propose a novel emotion tagging method, that thoroughly explores emotion relations from both the feature and label levels. Specifically, a graph convolutional network is introduced to inject local dependence among emotions into the model at the feature level, while an adversarial learning strategy is applied to constrain the joint distribution of multiple emotions at the label level. In addition, a new balanced loss function that mitigates the adverse effects of intra-class and inter-class imbalance is introduced to deal with the imbalance of emotion labels. Experimental results on several benchmark databases demonstrate the superiority of the proposed method compared to state-of-the-art works.\nFig. The architecture of the proposed emotion tagging method. It consists of an emotional GCN Φg, an encoder Φe , a classifier Φc , and a discriminator D. dgcn denotes the product of the output of emotional GCN and the encoder feature, and d denotes the concatenation of dgcn and the output of the encoder. y and ŷ represent the real label and the predicted label, respectively. ","date":1577836800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1625057752,"objectID":"baf398b76aa91e39125876158f13473c","permalink":"https://ustc-ac.github.io/publication/dblp-confmm-xu-ww-20/","publishdate":"2021-06-30T12:55:52.166103Z","relpermalink":"/publication/dblp-confmm-xu-ww-20/","section":"publication","summary":"To address the shortcomings in emotion tagging, we consider applying emotion relationship patterns at both feature and label levels. We propose a novel emotion tagging framework, that makes full use of the emotion relationship patterns in local and global distribution.","tags":["Emotion Tagging"],"title":"Exploiting Multi-Emotion Relations at Feature and Label Levels for Emotion Tagging","type":"publication"},{"authors":["Shi Yin","Shangfei Wang","Xiaoping Chen","Enhong Chen"],"categories":[],"content":"Current work of facial landmark tracking usually requires large amounts of fully annotated facial videos to train a landmark tracker. To relieve the burden of manual annotations, we propose a novel facial landmark tracking method that makes full use of unlabeled facial videos by exploiting both self-supervised and semi-supervised learning mechanisms. First, self-supervised learning is adopted for representation learning from unlabeled facial videos. Specifically, a facial video and its shuffled version are fed into a feature encoder and a classifier. The feature encoder is used to learn visual representations, and the classifier distinguishes the input videos as the original or the shuffled ones. The feature encoder and the classifier are trained jointly. Through self-supervised learning, the spatial and temporal patterns of a facial video are captured at representation level. After that, the facial landmark tracker, consisting of the pre-trained feature encoder and a regressor, is trained semi-supervisedly. The consistencies among the tracking results of the original, the inverse and the disturbed facial sequences are exploited as the constraints on the unlabeled facial videos, and the supervised loss is adopted for the labeled videos. Through semi-supervised end to-end training, the tracker captures sequential patterns inherent in facial videos despite small amount of manual annotations. Experiments on two benchmark datasets show that the proposed framework outperforms state-of-the art semi-supervised facial landmark tracking methods, and also achieves advanced performance compared to fully supervised facial landmark tracking methods.\nFig. The proposed two-stage learning framework. The first stage is self-supervised learning stage, as shown in the upper part of the Fig. The second stage is semi-supervised learning stage, as shown in the lower part of the figure. ","date":1577836800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1625057752,"objectID":"0a81f3fb14fd2ace0f805b5b3bdcd629","permalink":"https://ustc-ac.github.io/publication/dblp-confmm-yin-wcc-20/","publishdate":"2021-06-30T12:55:52.380753Z","relpermalink":"/publication/dblp-confmm-yin-wcc-20/","section":"publication","summary":"We propose a new semi-supervised learning strategy which trains the tracker by regression tasks from the consistency constraints on the long facial sequence instead of two adjacent frames, such that the long-term dependencies existed in a facial sequence are captured. The proposed semisupervised learning strategy does not require any extra labels. Thus, large scale unlabeled data can be exploited for training.","tags":["Landmark Tracking"],"title":"Exploiting Self-Supervised and Semi-Supervised Learning for Facial Landmark Tracking with Unlabeled Data","type":"publication"},{"authors":["Shangfei Wang","Guozhu Peng","Qiang Ji"],"categories":[],"content":"","date":1577836800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1625057750,"objectID":"c51787370b309abeba14a57323b3f9c2","permalink":"https://ustc-ac.github.io/publication/dblp-journalstaffco-wang-pj-20/","publishdate":"2021-06-30T12:55:50.437458Z","relpermalink":"/publication/dblp-journalstaffco-wang-pj-20/","section":"publication","summary":"","tags":[],"title":"Exploring Domain Knowledge for Facial Expression-Assisted Action Unit Activation Recognition","type":"publication"},{"authors":["Shangfei Wang","Longfei Hao","Qiang Ji"],"categories":[],"content":"","date":1577836800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1625057751,"objectID":"3e120444b3aadbcf7928385172ccbef6","permalink":"https://ustc-ac.github.io/publication/dblp-journalstmm-wang-hj-20/","publishdate":"2021-06-30T12:55:50.865189Z","relpermalink":"/publication/dblp-journalstmm-wang-hj-20/","section":"publication","summary":"","tags":[],"title":"Knowledge-Augmented Multimodal Deep Regression Bayesian Networks for Emotion Video Tagging","type":"publication"},{"authors":["Bin Xia","Weikang Wang","Shangfei Wang","Enhong Chen"],"categories":[],"content":"As one of the most important forms of psychological behaviors, micro-expression can reveal the real emotion. However, the existing labeled micro-expression samples are limited to train a high performance micro-expression classifier. Since micro-expression and macro-expression share some similarities in facial muscle movements and texture changes, in this paper we propose a micro-expression recognition framework that leverages macro-expression samples as guidance. Specifically, we first introduce two Expression Identity Disentangle Network, named MicroNet and MacroNet, as the feature extractor to disentangle expression-related features for micro and macro expression samples. Then MacroNet is fixed and used to guide the fine-tuning of MicroNet from both label and feature space. Adversarial learning strategy and triplet loss are added upon feature level between the MicroNet and MacroNet, so the MicroNet can efficiently capture the shared features of micro-expression and macro-expression samples. Loss inequality regularization is imposed to the label space to make the output of MicroNet converge to that of MicroNet. Comprehensive experiments on three public spontaneous micro-expression databases, i.e., SMIC, CASME2 and SAMM demonstrate the superiority of the proposed method.\nFig. The framework of our micro-expression recognition model. First we pretrain two EIDNets with micro-expression and macro-expression databases separately, named MicroNet and MacroNet. Secondly, MacroNet is fixed and used to guide the fine-tuning of MicroNet from both label and feature space, named MTMNet. ","date":1577836800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1625057752,"objectID":"d5d39ab32a76af00929ccb08776f4180","permalink":"https://ustc-ac.github.io/publication/dblp-confmm-0012-wwc-20/","publishdate":"2021-06-30T12:55:51.942426Z","relpermalink":"/publication/dblp-confmm-0012-wwc-20/","section":"publication","summary":"In order to address problems in micro-expression recognition, we propose a micro-expression recognition framework that leverages macroexpression as guidance. Since subjects in macro-expression and micro-expression databases are different, Expression-Identity Disentangle Network (EIDNet) is introduced as feature extractor to disentangle expression-related features for expression samples.","tags":["Expression Recognition"],"title":"Learning from Macro-expression: a Micro-expression Recognition Framework","type":"publication"},{"authors":["Bin Xia","Shangfei Wang"],"categories":[],"content":"Although facial expression recognition has improved in recent years, it is still very challenging to recognize expressions from occluded facial images in the wild. Due to the lack of large-scale facial expression datasets with diversity of the type and position of occlusions, it is very difficult to learn robust occluded expression classifier directly from limited occluded images. Considering facial images without occlusions usually provide more information for facial expression recognition compared to occluded facial images, we propose a step-wise learning strategy for occluded facial expression recognition that utilizes unpaired non-occluded images as guidance in the feature and label space. Specifically, we first measure the complexity of non-occluded data using distribution density in a feature space and split data into three subsets. In this way, the occluded expression classifier can be guided by basic samples first, and subsequently leverage more meaningful and discriminative samples. Complementary adversarial learning techniques are applied in the global-level and local-level feature space throughout, forcing the distribution of the occluded features to be close to the distribution of the non-occluded features. We also take the variability of the different images\u0026rsquo; transferability into account via adaptive classification loss. Loss inequality regularization is imposed in the label space to calibrate the output values of the occluded network. Experimental results show that our method improves performance on both synthesized occluded databases and realistic occluded databases.\nFig. The framework of the proposed approach consists of an occluded network fo, a non-occluded network ​fc, K​ local-level feature discriminator ​Dlk (k = 1, 2, \u0026hellip;, K)​, and a global feature discriminator ​Dg ","date":1577836800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1625057751,"objectID":"1400a25f3ad3aefaa1fb51d1661942b1","permalink":"https://ustc-ac.github.io/publication/dblp-confmm-0012-w-20/","publishdate":"2021-06-30T12:55:51.733955Z","relpermalink":"/publication/dblp-confmm-0012-w-20/","section":"publication","summary":"To tackle the challenges in occluded facial expression recognition, we propose a step-wise learning strategy including two types of complementary adversarial learning. In this way, the occluded classifier can learn effective information from large-scale unpaired non-occluded facial images.","tags":["Expression Recognition"],"title":"Occluded Facial Expression Recognition with Step-Wise Assistance from Unpaired Non-Occluded Images","type":"publication"},{"authors":["Guang Liang","Shangfei Wang","Can Wang"],"categories":[],"content":"","date":1577836800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1625057753,"objectID":"861f607b064253c0fc0c3d0f85f82be8","permalink":"https://ustc-ac.github.io/publication/dblp-journalscorrabs-2007-05932/","publishdate":"2021-06-30T12:55:52.812534Z","relpermalink":"/publication/dblp-journalscorrabs-2007-05932/","section":"publication","summary":"","tags":[],"title":"Pose-aware Adversarial Domain Adaptation for Personalized Facial Expression Recognition","type":"publication"},{"authors":["Shangfei Wang","Longfei Hao","Qiang Ji"],"categories":[],"content":"","date":1577836800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1625057751,"objectID":"ce3c9cf7a99c39efa06ceedd8f526009","permalink":"https://ustc-ac.github.io/publication/dblp-journalstomccap-wang-hj-20/","publishdate":"2021-06-30T12:55:51.087546Z","relpermalink":"/publication/dblp-journalstomccap-wang-hj-20/","section":"publication","summary":"","tags":[],"title":"Posed and Spontaneous Expression Distinction Using Latent Regression Bayesian Networks","type":"publication"},{"authors":["Bin Xia","Shangfei Wang"],"categories":[],"content":"Current works on multimodal facial expression recognition typically require paired visible and thermal facial images. Although visible cameras are readily available in our daily life, thermal cameras are expensive and less prevalent. It is costly to collect a large quantity of synchronous visible and thermal facial images. To tackle this paired training data bottleneck, we propose an unpaired multimodal facial expression recognition method, which makes full use of the massive number of unpaired visible and thermal images by utilizing thermal images to construct better image representations and classifiers for visible images during training. Specifically, two deep neural networks are trained from visible and thermal images to learn image representations and expression classifiers for two modalities. Then, an adversarial strategy is adopted to force statistical similarity between the learned visible and thermal representations, and to minimize the distribution mismatch between the predictions of the visible and thermal images. Through adversarial learning, the proposed method leverages thermal images to construct better image representations and classifiers for visible images during training, without the requirement of paired data. A decoder network is built upon the visible hidden features in order to preserve some inherent features of the visible view. We also take the variability of the different images’ transferability into account via adaptive classification loss. During testing, only visible images are required and the visible network is used. Thus, the proposed method is appropriate for real-world scenarios, since thermal imaging is rare in these instances. Experiments on two benchmark multimodal expression databases and three visible facial expression databases demonstrate the superiority of the proposed method compared to state-of-the-art methods.\nFig. The framework of the proposed unpaired facial expression recognition method. ","date":1577836800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1625057751,"objectID":"f4daec32264acab7295b967affff0996","permalink":"https://ustc-ac.github.io/publication/dblp-confaccv-0012-w-20/","publishdate":"2021-06-30T12:55:51.310196Z","relpermalink":"/publication/dblp-confaccv-0012-w-20/","section":"publication","summary":"Since collecting paired visible and thermal facial images is often difficult, requiring paired data during training prevents the usage of the many available unpaired visible and thermal images, and thus may degenerate the learning effect of the visible facial expression classifier.\nTo address this, we propose an unpaired adversarial facial expression recognition method. We tackle the unbalanced quantity of visible and thermal images by utilizing thermal images as privileged information. We introduce adversarial learning on the feature-level and label-level spaces to cope with unpaired training data. Finally, we add a decoder network to preserve the inherent visible features.","tags":["Expression Recognition"],"title":"Unpaired Multimodal Facial Expression Recognition","type":"publication"},{"authors":null,"categories":null,"content":"A CCTV-9 documentary called \u0026ldquo;The Power of Science\u0026rdquo;, which filmed the facial expression recognition function of our robot Jiajia is online now!\nLink (17:44)\nOn the 70th anniversary of the founding of New China, in order to showcase the achievements of China\u0026rsquo;s scientific and technological development and tell the story of Chinese science, the Chinese Academy of Sciences and China Central Television have cooperated in-depth to create a series of documentary \u0026ldquo;The Power of Science\u0026rdquo;. In the seventh episode, \u0026ldquo;The Biggest Temptation of the Intelligent Era-Bionic Robots\u0026rdquo;, a story of Jiajia is told, a service robot developed by the University of Science and Technology.\nMore \u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash; Robot KeJia Able to respond to facial expressions with her own micro-level movements during conversations in both Chinese and English, Jiajia is the result of integrating technologies for cognitive modelling, semantic understanding, automated reasoning and planning, knowledge acquisition, kinematics and cloud robotics. These technologies have been studied in the Kejia Robotics project by Chen Xiaoping’s laboratory at USTC. The award-winning development team has received many international accolades, including first places at the popular RoboCup world championship, and the IJCAI Robotics competition.\nReported by Nature(2016) Jiajia was reported by Nature during the 60th anniversary of the University of Science and Technology of China\nPraised by the Minister of Foreign Affairs(2017) On the afternoon of April 11, the Anhui Global Promotion Event of the Ministry of Foreign Affairs with the theme of \u0026ldquo;An Open China: Splendid Anhui Welcomes the World\u0026rdquo; was held in the Blue Hall of the Ministry of Foreign Affairs. At 16:20 in the afternoon, Foreign Minister Yi Wang and the invited envoys to China, accompanied by Secretary of the Provincial Party Committee Jinbin Li and Governor Guoying Li, walked into the comprehensive exhibition area. Jiajia, taking the lead in the exhibition area, was praised by Minister Wang Yi.\nCameron and Jia Jia(2017) At the opening of the 17th United Bank of Switzerland (UBS) Greater China Conference, held in Shanghai, Jiajia had a spectacular conversation with former British Prime Minister Cameron.\n","date":1572600352,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1572600352,"objectID":"2d325d1adabd4d72a011d1b14c2aa34e","permalink":"https://ustc-ac.github.io/post/our-facial-expression-recognition-job-is-filmed-in-a-cctv-9-documentary/","publishdate":"2019-11-01T09:25:52.898Z","relpermalink":"/post/our-facial-expression-recognition-job-is-filmed-in-a-cctv-9-documentary/","section":"post","summary":"A CCTV-9 documentary called \u0026ldquo;The Power of Science\u0026rdquo;, which filmed the facial expression recognition function of our robot Jiajia is online now!\nLink (17:44)\n","tags":null,"title":"Our facial expression recognition job is filmed in a CCTV-9 documentary","type":"post"},{"authors":["Sicheng Zhao","Shangfei Wang","Mohammad Soleymani","Dhiraj Joshi","Qiang Ji"],"categories":[],"content":"","date":1546300800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1625057755,"objectID":"4940b178cf594e84b0d9623327f0e89f","permalink":"https://ustc-ac.github.io/publication/dblp-journalscorrabs-1911-05609/","publishdate":"2021-06-30T12:55:55.671924Z","relpermalink":"/publication/dblp-journalscorrabs-1911-05609/","section":"publication","summary":"","tags":[],"title":"Affective Computing for Large-Scale Heterogeneous Multimedia Data: A Survey","type":"publication"},{"authors":["Shangfei Wang","Shan Wu","Guozhu Peng","Qiang Ji"],"categories":[],"content":"","date":1546300800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1625057753,"objectID":"9961190b5ba68b7662d10a8987fb32ab","permalink":"https://ustc-ac.github.io/publication/dblp-journalstaffco-wang-wpj-19/","publishdate":"2021-06-30T12:55:53.260075Z","relpermalink":"/publication/dblp-journalstaffco-wang-wpj-19/","section":"publication","summary":"","tags":[],"title":"Capturing Feature and Label Relations Simultaneously for Multiple Facial Action Unit Recognition","type":"publication"},{"authors":["Shi Yin","Shangfei Wang","Guozhu Peng","Xiaoping Chen","Bowen Pan"],"categories":[],"content":"The spatial and temporal patterns inherent in facial feature points are crucial for facial landmark tracking, but have not been thoroughly explored yet. In this paper, we propose a novel deep adversarial framework to explore the shape and temporal dependencies from both appearance level and target label level. The proposed deep adversarial framework consists of a deep landmark tracker and a discriminator. The deep landmark tracker is composed of a stacked Hourglass network as well as a convolutional neural network and a long short-term memory network, and thus implicitly capture spatial and temporal patterns from facial appearance for facial landmark tracking. The discriminator is adopted to distinguish the tracked facial landmarks from ground truth ones. It explicitly models shape and temporal dependencies existing in ground truth facial landmarks through another convolutional neural network and another long short-term memory network. The deep landmark tracker and the discriminator compete with each other. Through adversarial learning, the proposed deep adversarial landmark tracking approach leverages inherent spatial and temporal patterns to facilitate facial landmark tracking from both appearance level and target label level. Experimental results on two benchmark databases demonstrate the superiority of the proposed approach to state-of-the-art work.\nFig. The framework of the proposed approach consists of two deep neural networks, i.e., a tracker and a discriminator. The tracker is used to track landmarks from a facial video. The discriminator is introduced to distinguish the predicted landmark positions from the ground truth ones. The tracker tries to confuse the discriminator by predicting landmark positions with joint distributions that are close to the ground truth ones. Through adversarial learning, the inherent spatial and temporal dependencies of a facial sequence are captured from both appearance level and target level for landmark tracking. See text for details. ","date":1546300800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1625057754,"objectID":"ddd11c3184c0af44ebdfc33e42e1076b","permalink":"https://ustc-ac.github.io/publication/dblp-confijcai-yin-wpcp-19/","publishdate":"2021-06-30T12:55:54.573516Z","relpermalink":"/publication/dblp-confijcai-yin-wpcp-19/","section":"publication","summary":"To address the inconsistency between explicit forms of joint label distribution and the ground truth facial landmark distribution, we propose an adversarial learning framework to close the joint distribution inherent in predicted and ground truth facial landmarks.","tags":["Landmark Tracking"],"title":"Capturing Spatial and Temporal Patterns for Facial Landmark Tracking through Adversarial Learning","type":"publication"},{"authors":["Shangfei Wang","Shiyu Chen","Qiang Ji"],"categories":[],"content":"","date":1546300800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1625057753,"objectID":"1615e1e2d6990ee2d98fe794819ce0bf","permalink":"https://ustc-ac.github.io/publication/dblp-journalstaffco-wang-cj-19/","publishdate":"2021-06-30T12:55:53.03301Z","relpermalink":"/publication/dblp-journalstaffco-wang-cj-19/","section":"publication","summary":"","tags":[],"title":"Content-Based Video Emotion Tagging Augmented by Users' Multiple Physiological Responses","type":"publication"},{"authors":["Guozhu Peng","Shangfei Wang"],"categories":[],"content":"Current works on facial action unit (AU) recognition typically require fully AU-labeled training samples. To reduce the reliance on time-consuming manual AU annotations, we propose a novel semi-supervised AU recognition method leveraging two kinds of readily available auxiliary information.The method leverages the dependencies between AUs and expressions as well as the dependencies among AUs, which are caused by facial anatomy and therefore embedded in all facial images, independent on their AU annotation status. The other auxiliary information is facial image synthesis given AUs, the dual task of AU recognition from facial images, and therefore has intrinsic probabilistic connections with AU recognition, regardless of AU annotations. Specifically, we propose a dual semi-supervised generative adversarial network for AU recognition from partially AU-labeled and fully expression-labeled facial images. The proposed network consists of an AU classifier C, an image generator G, and a discriminator D. In addition to minimize the supervised losses of the AU classifier and the face generator for labeled training data,we explore the probabilistic duality between the tasks using adversary learning to force the convergence of the face-AU-expression tuples generated from the AU classifier and the face generator, and the ground-truth distribution in labeled data for all training data. This joint distribution also includes the inherent AU dependencies. Furthermore, we reconstruct the facial image using the output of the AU classifier as the input of the face generator, and create AU labels by feeding the output of the face generator to the AU classifier. We minimize reconstruction losses for all training data, thus exploiting the informative feedback provided by the dual tasks. Within-database and cross-database experiments on three benchmark databases demonstrate the superiority of our method in both AU recognition and face synthesis compared to state-of-the-art works.\nFig. The framework of the proposed dual semi-supervised GAN, consisting of three modules: a discriminator D, a classifier C, and a generator G. ","date":1546300800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1625057754,"objectID":"14acbd2241b8fbc9fcf267a065353773","permalink":"https://ustc-ac.github.io/publication/dblp-confaaai-peng-w-19/","publishdate":"2021-06-30T12:55:54.130866Z","relpermalink":"/publication/dblp-confaaai-peng-w-19/","section":"publication","summary":"Instead of minimizing the distance of two joint distributions directly, which requires the estimation of the marginal distribution of the input, the proposed approach uses an adversarial strategy to exploit the probabilistic duality, thus avoiding the estimation of marginal distribution.","tags":["AU Recognition"],"title":"Dual Semi-Supervised Learning for Facial Action Unit Recognition","type":"publication"},{"authors":["Shangfei Wang","Longfei Hao","Qiang Ji"],"categories":[],"content":"","date":1546300800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1625057753,"objectID":"4ea50dd69212f0ca729762db3c504b16","permalink":"https://ustc-ac.github.io/publication/dblp-journalstip-wang-hj-19/","publishdate":"2021-06-30T12:55:53.470506Z","relpermalink":"/publication/dblp-journalstip-wang-hj-19/","section":"publication","summary":"","tags":[],"title":"Facial Action Unit Recognition and Intensity Estimation Enhanced Through Label Dependencies","type":"publication"},{"authors":["Can Wang","Shangfei Wang","Guang Liang"],"categories":[],"content":"Existing facial expression recognition methods either focus on pose variations or identity bias, but not both simultaneously. This paper proposes an adversarial feature learning method to address both of these issues. Specifically, the proposed method consists of five components: an encoder, an expression classifier, a pose discriminator, a subject discriminator, and a generator. An encoder extracts feature representations, and an expression classifier tries to perform facial expression recognition using the extracted feature representations. The encoder and the expression classifier are trained collaboratively, so that the extracted feature representations are discriminative for expression recognition. A pose discriminator and a subject discriminator classify the pose and the subject from the extracted feature representations respectively. They are trained adversarially with the encoder. Thus, the extracted feature representations are robust to poses and subjects. A generator reconstructs facial images to further favor the feature representations. Experiments on five benchmark databases demonstrate the superiority of the proposed method to state-of-the-art work.\nFig. The structure of the proposed method. It consists of an encoder E , an expression classifier Dc , a pose discriminator Dp , a subject discriminator Ds ,and a generator G . ","date":1546300800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1625057755,"objectID":"1eb448254ec73310599e26bd67ee74db","permalink":"https://ustc-ac.github.io/publication/dblp-confmm-wang-wl-19/","publishdate":"2021-06-30T12:55:55.220612Z","relpermalink":"/publication/dblp-confmm-wang-wl-19/","section":"publication","summary":"Previous facial expression recognition methods either focus on pose variations or identity bias; there is no work that considers both at the same time. To this end, we propose a novel feature representation method that uses adversarial learning to overcome the challenges of both pose variations and identity bias.","tags":["Expression Recognition"],"title":"Identity- and Pose-Robust Facial Expression Recognition through Adversarial Feature Learning","type":"publication"},{"authors":["Bowen Pan","Shangfei Wang","Qisheng Jiang"],"categories":[],"content":"The inherent connections among aesthetic attributes and aesthetics are crucial for image aesthetic assessment, but have not been thoroughly explored yet. In this paper, we propose a novel image aesthetic assessment assisted by attributes through both representation-level and label-level. The attributes are used as privileged information, which is only required during training. Specifically, we first propose a multitask deep convolutional rating network to learn the aesthetic score and attributes simultaneously. The attributes are explored to construct better feature representations for aesthetic assessment through multi-task learning. After that, we introduce a discriminator to distinguish the predicted attributes and aesthetics of the multi-task deep network from the ground truth label distribution embedded in the training data. The multi-task deep network wants to output aesthetic score and attributes as close to the ground truth labels as possible. Thus the deep network and the discriminator compete with each other. Through adversarial learning, the attributes are explored to enforce the distribution of the predicted attributes and aesthetics to converge to the ground truth label distribution. Experimental results on two benchmark databases demonstrate the superiority of the proposed method to state of the art work.\nFig. The framework of the proposed method ","date":1546300800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1625057754,"objectID":"160f004b107f994ebbabd53321ef9b7c","permalink":"https://ustc-ac.github.io/publication/dblp-confaaai-pan-wj-19/","publishdate":"2021-06-30T12:55:53.911515Z","relpermalink":"/publication/dblp-confaaai-pan-wj-19/","section":"publication","summary":"In this paper, we propose a novel attributes enhanced image aesthetic assessment, where the attributes are used as privileged information.","tags":["Aesthetic Assessment"],"title":"Image Aesthetic Assessment Assisted by Attributes through Adversarial Learning","type":"publication"},{"authors":["Shi Yin","Yonggan Fu","Can Wang","Runlong Wu","Heyan Ding","Shangfei Wang"],"categories":[],"content":"We propose a multi-modal method for the OneMinute Empathy Prediction competition. First, we use bottleneck residual and fully-connected network to encode facial images and speeches of the listener. Second, we propose to use the current time stage as a temporal feature and encoded it into the proposed multi-modal network. Third, we select a subset training data based on its performance of empathy prediction on the validation data. Experimental results on the testing set show that the proposed method outperforms the baseline methods significantly according to the CCC metric (0.14 vs 0.06). Fig. Model architecture.The inputs of the proposed multi-modal deep network are facial images, audio signals and time stamps. Specifically, we extract facial images of the listener in each frame through opencv, and then reshape the size of facial images to (120, 120). The preprocessed facial images are fed into a network with one convolution layer and six sequential bottleneck residual modules.See text for details. ","date":1546300800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1625057754,"objectID":"3d0569653113c8bf43ae4e1273ba4d58","permalink":"https://ustc-ac.github.io/publication/dblp-conffgr-yin-fwwdw-19/","publishdate":"2021-06-30T12:55:54.344542Z","relpermalink":"/publication/dblp-conffgr-yin-fwwdw-19/","section":"publication","summary":"In this paper, we propose a multi-modal deep network to predict the empathy of the listener during the conversation between two people. First, we use a bottleneck residual network proposed by to learn visual representation from facial images, and adopt fully connected network to extract audio features from the listener’s speech. Second, we propose to use the current time stage as a temporal feature, and fuse it with the learned visual and audio representations. Neural network regression is used to predict the empathy level. We further select the representative subset training data to train the proposed multi-modal deep network. Experimental results on the One-Minute Empathy Prediction dataset demonstrate the effectiveness of the proposed method.","tags":["Multimedia"],"title":"Integrating Facial Images, Speeches and Time for Empathy Prediction","type":"publication"},{"authors":["Shi Yin","Yi Zhou","Chenguang Li","Shangfei Wang","Jianmin Ji","Xiaoping Chen","Ruili Wang"],"categories":[],"content":"","date":1546300800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1625057754,"objectID":"0ce95e99b3661fa2ee9ec39a8322deee","permalink":"https://ustc-ac.github.io/publication/dblp-confijcnn-yin-zlwjcw-19/","publishdate":"2021-06-30T12:55:54.791225Z","relpermalink":"/publication/dblp-confijcnn-yin-zlwjcw-19/","section":"publication","summary":"","tags":[],"title":"KDSL: a Knowledge-Driven Supervised Learning Framework for Word Sense Disambiguation","type":"publication"},{"authors":["Shangfei Wang","Shi Yin","Longfei Hao","Guang Liang"],"categories":[],"content":"","date":1546300800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1625057756,"objectID":"ec8edb4171378483e6c9ab6405b7006f","permalink":"https://ustc-ac.github.io/publication/dblp-journalscorrabs-1911-07846/","publishdate":"2021-06-30T12:55:55.885319Z","relpermalink":"/publication/dblp-journalscorrabs-1911-07846/","section":"publication","summary":"","tags":[],"title":"Multiple Face Analyses through Adversarial Learning","type":"publication"},{"authors":["Bowen Pan","Shangfei Wang","Bin Xia"],"categories":[],"content":"In this paper, we propose a novel approach of occluded facial expression recognition under the help of non-occluded facial images. The non-occluded facial images are used as privileged information, which is only required during training, but not required during testing. Specifically, two deep neural networks are first trained from occluded and non-occluded facial images respectively. Then the non-occluded network is fixed and is used to guide the fine-tuning of the occluded network from both label space and feature space. Similarity constraint and loss inequality regularization are imposed to the label space to make the output of occluded network converge to that of the non-occluded network. Adversarial leaning is adopted to force the distribution of the learned features from occluded facial images to be close to that from non-occluded facial images. Furthermore, a decoder network is employed to reconstruct the non-occluded facial images from occluded features. Under the guidance of non-occluded facial images, the occluded network is expected to learn better features and classifier during training. Experiments on the benchmark databases with both synthesized and realistic occluded facial images demonstrate the superiority of the proposed method to state-of-the-art.\nFig. The framework of proposed facial expression recognition with occlusions ","date":1546300800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1625057755,"objectID":"48eadf5e94a87c9765eed19edad20a59","permalink":"https://ustc-ac.github.io/publication/dblp-confmm-pan-wx-19/","publishdate":"2021-06-30T12:55:55.440339Z","relpermalink":"/publication/dblp-confmm-pan-wx-19/","section":"publication","summary":"we propose using non-occluded facial images as privileged information to assist the learning process of the occluded view. Specifically, two deep neural networks are first trained from occluded and non-occluded images respectively.","tags":["Expression Recognition"],"title":"Occluded Facial Expression Recognition Enhanced through Privileged Information","type":"publication"},{"authors":["Shangfei Wang","Guozhu Peng"],"categories":[],"content":"","date":1546300800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1625057753,"objectID":"d6f9e0d68c66166369b7814c8a34f7f0","permalink":"https://ustc-ac.github.io/publication/dblp-journalstmm-wang-p-19/","publishdate":"2021-06-30T12:55:53.701773Z","relpermalink":"/publication/dblp-journalstmm-wang-p-19/","section":"publication","summary":"","tags":[],"title":"Weakly Supervised Dual Learning for Facial Action Unit Recognition","type":"publication"},{"authors":["Longfei Hao","Shangfei Wang","Guozhu Peng","Qiang Ji"],"categories":[],"content":"Due to the underlying anatomic mechanism that govern facial muscular interactions, there exist inherent de-pendencies between facial action units (AU). Such dependen-cies carry crucial information for AU recognition, yet have not been thoroughly exploited. Therefore, in this paper, we propose a novel AU recognition method with a three-layer hybrid Bayesian network, whose top two layers consist of a latent regression Bayesian network (LRBN), and the bottom two layers are Bayesian networks. The LRBN is a directed graphical model consisting of one latent layer and one visible layer. Specifically, the visible nodes of LRBN represent the ground-truth AU labels. Due to the “explaining away” effect in Bayesian networks, LRBN is able to capture both the depen-dencies among the latent variables given the observation and the dependencies among visible variables. Such dependencie ssuccessfully and faithfully represent relations among multiple AUs. The bottom two layers are two node Bayesian networks, connecting the ground truth AU labels and their measurements.Efficient learning and inference algorithms are also proposed.Furthermore, we extend the proposed hybrid Bayesian network model for facial expression-assisted AU recognition, since AUrelations are influenced by expressions. By introducing facial expression nodes in the middle visible layer, facial expressions,which are only required during training, facilitate the estima-tion of label dependencies among AUs. Experimental results on three benchmark databases, i.e. the CK+ database, the SEMAINE database, and the BP4D database, demonstrate that the proposed approaches can successfully capture complex AU relationships, and the expression labels available only during training are benefit for AU recognition during testing.\nFig. The proposed AU recognition through AU-relation modeling ","date":1514764800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1625057757,"objectID":"a0baa5e3f65391f871cc2dc1bfb428d5","permalink":"https://ustc-ac.github.io/publication/dblp-conffgr-hao-wpj-18/","publishdate":"2021-06-30T12:55:56.973306Z","relpermalink":"/publication/dblp-conffgr-hao-wpj-18/","section":"publication","summary":"We propose employing the latent regression Bayesian network to effectively capture the high-order and global dependencies among AUs.","tags":["AU Recognition"],"title":"Facial Action Unit Recognition Augmented by Their Dependencies","type":"publication"},{"authors":["Bowen Pan","Shangfei Wang"],"categories":[],"content":"Currently, fusing visible and thermal images for facial expression recognition requires two modalities during both training and testing. Visible cameras are commonly used in real-life applications, and thermal cameras are typically only available in lab situations due to their high price. Thermal imaging for facial expression recognition is not frequently used in real-world situations. To address this, we propose a novel thermally enhanced facial expression recognition method which uses thermal images as privileged information to construct better visible feature representation and improved classifiers by incorporating adversarial learning and similarity constraints during training. Specifically, we train two deep neural networks from visible images and thermal images. We impose adversarial loss to enforce statistical similarity between the learned representations of two modalities, and a similarity constraint to regulate the mapping functions from visible and thermal representation to expressions. Thus, thermal images are leveraged to simultaneously improve visible feature representation and classification during training. To mimic real-world scenarios, only visible images are available during testing. We further extend the proposed expression recognition method for partially unpaired data to explore thermal images\u0026rsquo; supplementary role in visible facial expression recognition when visible images and thermal images are not synchronously recorded. Experimental results on the MAHNOB Laughter database demonstrate that our proposed method can effectively regularize visible representation and expression classifiers with the help of thermal images, achieving state-of-the-art recognition performance.\nFig. The framework of our proposed method for facial expression recognition. ","date":1514764800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1625057757,"objectID":"ff81808044c53741c4277a74fdda9af7","permalink":"https://ustc-ac.github.io/publication/dblp-confmm-pan-w-18/","publishdate":"2021-06-30T12:55:57.432564Z","relpermalink":"/publication/dblp-confmm-pan-w-18/","section":"publication","summary":"In this paper, we propose a novel facial expression recognition method enhanced by thermal images. Our method leverages thermal images to construct better visible feature representation and classifiers during training through adversarial learning and similarity constraints. Specifically, we learn two deep neural networks for expression classification from visible and thermal images.","tags":["Expression Recognition"],"title":"Facial Expression Recognition Enhanced by Thermal Images through Adversarial Learning","type":"publication"},{"authors":["Shangfei Wang","Shiyu Chen","Tanfang Chen","Xiaoxiao Shi"],"categories":[],"content":"","date":1514764800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1625057756,"objectID":"e1e4506385aee6547cd7a277504d9ef2","permalink":"https://ustc-ac.github.io/publication/dblp-journalspr-wang-ccs-18/","publishdate":"2021-06-30T12:55:56.092607Z","relpermalink":"/publication/dblp-journalspr-wang-ccs-18/","section":"publication","summary":"","tags":[],"title":"Learning with privileged information for multi-Label classification","type":"publication"},{"authors":["Can Wang","Shangfei Wang"],"categories":[],"content":"Personalized facial action unit (AU) recognition is challeng\u0002ing due to subject-dependent facial behavior. This paper proposes a method to recognize personalized multiple facial AUs through a novel generative adversarial network, which adapts the distribution of source domain facial images to that of target domain facial images and detects multiple AUs by leveraging AU dependencies. Specifically, we use a generative adversarial network to generate synthetic images from source domain; the synthetic images have a similar appearance to the target subject and retain the AU patterns of the source images. We simultaneously leverage AU dependencies to train a multiple AU classifier. Experimental results on three bench\u0002mark databases demonstrate that the proposed method can successfully realize unsupervised domain adaptation for indi\u0002vidual AU detection, and thus outperforms state-of-the-art AU detection methods.\nFig. Our proposed architecture includes a generator, a discriminator and a classifier. The generator G generates an image conditioned on a source image. The discriminator D discriminates between generated and target images. The classifier R assigns AU labels to an image. ","date":1514764800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1625057757,"objectID":"00b31e56fed7834a9cefa812c02de700","permalink":"https://ustc-ac.github.io/publication/dblp-confmm-wang-w-18/","publishdate":"2021-06-30T12:55:57.205801Z","relpermalink":"/publication/dblp-confmm-wang-w-18/","section":"publication","summary":"we propose a novel generative adversarial recognition network (GARN) for personalized AU recognition without any assumptions. Specifically, the proposed GARN consists of a generator, a discriminator, and a classifier.","tags":["AU Recognition"],"title":"Personalized Multiple Facial Action Unit Recognition through Generative Adversarial Recognition Network","type":"publication"},{"authors":["Shangfei Wang","Bowen Pan","Huaping Chen","Qiang Ji"],"categories":[],"content":"","date":1514764800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1625057756,"objectID":"382219ff47f47601603ef857cc0fcb1b","permalink":"https://ustc-ac.github.io/publication/dblp-journalstcyb-wang-pcj-18/","publishdate":"2021-06-30T12:55:56.312572Z","relpermalink":"/publication/dblp-journalstcyb-wang-pcj-18/","section":"publication","summary":"","tags":[],"title":"Thermal Augmented Expression Recognition","type":"publication"},{"authors":["Guozhu Peng","Shangfei Wang"],"categories":[],"content":"Current works on facial action unit (AU) recognition typically require fully AU-annotated facial images for supervised AU classifier training. AU annotation is a timeconsuming, expensive, and error-prone process. While AUs are hard to annotate, facial expression is relatively easy to label. Furthermore, there exist strong probabilistic dependencies between expressions and AUs as well as dependencies among AUs. Such dependencies are referred to as domain knowledge. In this paper, we propose a novel AU recognition method that learns AU classifiers from domain knowledge and expression-annotated facial images through adversarial training. Specifically, we first generate pseudo AU labels according to the probabilistic dependencies between expressions and AUs as well as correlations among AUs summarized from domain knowledge. Then we propose a weakly supervised AU recognition method via an adversarial process, in which we simultaneously train two models: a recognition model R, which learns AU classifiers, and a discrimination model D, which estimates the probability that AU labels generated from domain knowledge rather than the recognized AU labels from R. The training procedure for R maximizes the probability of D making a mistake. By leveraging the adversarial mechanism, the distribution of recognized AUs is closed to AU prior distribution from domain knowledge. Furthermore, the proposed weakly supervised AU recognition can be extended to semisupervised learning scenarios with partially AU-annotated images. Experimental results on three benchmark databases demonstrate that the proposed method successfully leverages the summarized domain knowledge to weakly supervised AU classifier learning through an adversarial process, and thus achieves state-of-the-art performance.\nFig. The framework of RAN. In Part 1, the facial feature X is inputted into recognizer R and get the “fake” AU vector, the “real” AU data generated in section 2.2 are in Part 2. In part 3, P discriminators are trained, “real” or ”fake” AU data are inputted to corresponding discriminator with the same expression label. See text for details. ","date":1514764800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1625057756,"objectID":"711b4e359229546b8445b579c01e8e25","permalink":"https://ustc-ac.github.io/publication/dblp-confcvpr-peng-w-18/","publishdate":"2021-06-30T12:55:56.749928Z","relpermalink":"/publication/dblp-confcvpr-peng-w-18/","section":"publication","summary":"We propose a novel weakly supervised AU recognition method to learn AU classifiers with only expression labels. Specifically, we notice that there exist domain knowledge about expressions and AUs that can be represented as prior probabilities. We generate pseudo AU data for each expression; for AU classifiers’ training, we propose an RAN model, which consists of a recognition model and a discrimination mode trained simultaneously by leveraging an adversarial process, to make the distribution of the recognized AU close to the distribution of the pseudo AU data. Furthermore, we extend the proposed method to semi-supervised learning with partially AU-annotated images.","tags":["AU Recognition"],"title":"Weakly Supervised Facial Action Unit Recognition Through Adversarial Training","type":"publication"},{"authors":["Shangfei Wang","Guozhu Peng","Shiyu Chen","Qiang Ji"],"categories":[],"content":"","date":1514764800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1625057756,"objectID":"24050b9e126b03170faaabe0e424b269","permalink":"https://ustc-ac.github.io/publication/dblp-journalstcyb-wang-pcj-18-a/","publishdate":"2021-06-30T12:55:56.522516Z","relpermalink":"/publication/dblp-journalstcyb-wang-pcj-18-a/","section":"publication","summary":"","tags":[],"title":"Weakly Supervised Facial Action Unit Recognition With Domain Knowledge","type":"publication"},{"authors":["Quan Gan","Shangfei Wang","Longfei Hao","Qiang Ji"],"categories":[],"content":"The inherent dependencies between visual elements and aural elements are crucial for affective video content analyses, yet have not been successfully exploited. Therefore, we propose a multimodal deep regression Bayesian network (MMDRBN) to capture the dependencies between visual elements and aural elements for affective video content analyses. The regression Bayesian network (RBN) is a directed graphical model consisting of one latent layer and one visible layer. Due to the explaining away effect in Bayesian networks (BN), RBN is able to capture both the dependencies among the latent variables given the observation and the dependencies among visible variables. We propose a fast learning algorithm to learn the RBN. For the MMDRBN, first, we learn several RBNs layer-wisely from visual modality and audio modality respectively. Then we stack these RBNs and obtain two deep networks. After that, a joint representation is extracted from the top layers of the two deep networks, and thus captures the high order dependencies between visual modality and audio modality. In order to predict the valence or arousal score of video contents, we initialize a feed-forward inference network from the MMDRBN whose inference is intractable by minimizing the KullbackCLeibler (KL)divergence between the two networks. The back propagation algorithm is adopted for finetuning the inference network. Experimental results on the LIRIS-ACCEDE database demonstrate that the proposed MMDRBN successfully captures the dependencies between visual and audio elements, and thus achieves better performance compared with state-of-the-art work. Fig. The framework of our proposed method. First, we train a multimodal generative network. It consists of two stacked RBNs that are created for visual and audio modalities respectively. ","date":1483228800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1625057759,"objectID":"a7cd1d723801e45095d3f90e3a32a345","permalink":"https://ustc-ac.github.io/publication/dblp-conficcv-gan-whj-17/","publishdate":"2021-06-30T12:55:59.394431Z","relpermalink":"/publication/dblp-conficcv-gan-whj-17/","section":"publication","summary":"In this paper, we propose a new multimodal learning method, multimodal deep regression Bayesian network (MMDRBN), to construct the high-level joint representation of visual and audio modalities for emotion tagging. Then the MMDRBN is transformed into an inference network by minimizing the KL-divergence. After that, the inference network is used to predict discrete or continuous affective scores from video content.","tags":["Affective Analyses"],"title":"A Multimodal Deep Regression Bayesian Network for Affective Video Content Analyses","type":"publication"},{"authors":["Shan Wu","Shangfei Wang","Qiang Ji"],"categories":[],"content":"In this paper,we tackle the problem of emotion tagging of multimedia data by modeling the dependencies among multiple emotions in both the feature and label spaces. These dependencies,which carry crucial top-down and bottom-up evidence for improving multimedia affective content analysis,have not been thoroughly exploited yet. To this end, we propose two hierarchical models that independently and dependently learn the shared features and global semantic relationships among emotion labels to jointly tag multiple emotion labels of multimedia data. Efficient learning and inference algorithms of the proposed models are also developed. Experiments on three benchmark emotion databases demonstrate the superior performance of our methods to existing methods.\nFig. Two proposed methods.(a) Combining a multi-task RBM with a three-layer RBM to capture dependencies among features and labels independently.(b) Capturing dependencies among features and labels dependently. ","date":1483228800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1625057758,"objectID":"a65ba76dae431f3c4376c1f51c018cf9","permalink":"https://ustc-ac.github.io/publication/dblp-confaaai-wu-wj-17/","publishdate":"2021-06-30T12:55:58.297651Z","relpermalink":"/publication/dblp-confaaai-wu-wj-17/","section":"publication","summary":"To the best of our knowledge, this paper is the first work to assign multiple emotions to multimedia data by exploring the emotional relationships at both feature and label levels. By learning the shared features with a multi-task RBM classifier and modeling the dependencies among emotion labels with a hierarchy RBM model, the proposed approaches can exploit both top-down and bottom-up relations among emotions independently and dependently to improve multiple emotions tagging for multimedia.","tags":["Emotion Tagging"],"title":"Capturing Dependencies among Labels and Features for Multiple Emotion Tagging of Multimedia Data","type":"publication"},{"authors":["Jiajia Yang","Shangfei Wang"],"categories":[],"content":"Spatial and temporal patterns inherent in facial behavior carry crucial information for posed and spontaneous expressions distinction, but have not been thoroughly exploited yet. To address this issue, we propose a novel dynamic model, termed as interval temporal restricted Boltzmann machine (IT-RBM), to jointly capture global spatial patterns and complex temporal patterns embedded in posed and spontaneous expressions respectively for distinguishing between posed and spontaneous expressions. Specifically, we consider a facial expression as a complex activity that consists of temporally overlapping or sequential primitive facial events, which are defined as the motion of feature points. We propose using the Allen s Interval Algebra to represent the complex temporal patterns existing in facial events through a two-layer Bayesian network. Furthermore, we propose employing multi-value restricted Boltzmann machine to capture intrinsic global spatial patterns among facial events. Experimental results on three benchmark databases, the UvA-NEMO smile database, the DISFA+ database, and theSPOS database, demonstrate the proposed interval temporal restricted Boltzmann machine can successfully capture the intrinsic spatial-temporal patterns in facial behavior, and thus outperform state-of-the art work of posed and spontaneous expressions distinction.\nFig. The outline of recognition system ","date":1483228800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1625057760,"objectID":"027429a5f9b536f623c1f5a9a15da002","permalink":"https://ustc-ac.github.io/publication/dblp-confmm-yang-w-17/","publishdate":"2021-06-30T12:55:59.862501Z","relpermalink":"/publication/dblp-confmm-yang-w-17/","section":"publication","summary":"In this paper, we introduce a novel dynamic model, termed as interval temporal restricted Boltzmann machine(IT-RBM), to jointly capture global spatial patterns and complex temporal patterns embedded in posed expressions and spontaneous expressions respectively for distinguishing posed and spontaneous expressions. The proposed IT-RBM is a three-layer hierarchical probabilistic graphical model.","tags":["Expression Recognition"],"title":"Capturing Spatial and Temporal Patterns for Distinguishing between Posed and Spontaneous Expressions","type":"publication"},{"authors":["Shan Wu","Shangfei Wang","Bowen Pan","Qiang Ji"],"categories":[],"content":"Current work on facial action unit (AU) recognition requires AU-labeled facial images. Although large amounts of facial images are readily available, AU annotation is expensive and time consuming. To address this, we propose a deep facial action unit recognition approach learning from partially AU-labeled data. The proposed approach makes full use of both partly available ground-truth AU labels and the readily available large scale facial images without annotation. Specifically, we propose to learn label distribution from the ground-truth AU labels, and then train the AU classifiers from the large-scale facial images by maximizing the log likelihood of the mapping functions of AUs with regard to the learnt label distribution for all training data and minimizing the error between predicted AUs and ground-truth AUs for labeled data simultaneously. A restricted Boltzmann machine is adopted to model AU label distribution, a deep neural network is used to learn facial representation from facial images, and the support vector machine is employed as the classifier. Experiments on two benchmark databases demonstrate the effectiveness of the proposed approach.\n","date":1483228800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1625057759,"objectID":"1767973acd3d4a1d7204be9386d34270","permalink":"https://ustc-ac.github.io/publication/dblp-conficcv-wu-wpj-17/","publishdate":"2021-06-30T12:55:59.170148Z","relpermalink":"/publication/dblp-conficcv-wu-wpj-17/","section":"publication","summary":"Inspired by the observations that AUs are samples of the underlying AU label distributions, we propose a deep facial action unit recognition approach learning from partially AU-labeled training data through incorporating such spatial regular patterns of AU labels presented in ground-truth AU labels into the learning process of AU classifiers from a large-scale facial images without AU annotations.","tags":["AU Recognition"],"title":"Deep Facial Action Unit Recognition from Partially Labeled Data","type":"publication"},{"authors":["Tanfang Chen","Shangfei Wang","Shiyu Chen"],"categories":[],"content":"","date":1483228800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1625057759,"objectID":"129f9146aeeb87a09ebce1f95c723cad","permalink":"https://ustc-ac.github.io/publication/dblp-conficmcs-chen-wc-17/","publishdate":"2021-06-30T12:55:59.612929Z","relpermalink":"/publication/dblp-conficmcs-chen-wc-17/","section":"publication","summary":"","tags":[],"title":"Deep multimodal network for multi-label classification","type":"publication"},{"authors":["Quan Gan","Siqi Nie","Shangfei Wang","Qiang Ji"],"categories":[],"content":"Spatial patterns embedded in human faces are crucial for differentiating posed expressions from spontaneous ones, yet they have not been thoroughly exploited in the literature. To tackle this problem, we present a generative model, i.e., Latent Regression Bayesian Network (LRBN), to effectively capture the spatial patterns embedded in facial landmark points to differentiate between posed and spontaneous facial expressions. The LRBN is a directed graphical model consisting of one latent layer and one visible layer. Due to the “explaining away” effect in Bayesian networks, LRBN is able to capture both the dependencies among the latent variables given the observation and the dependencies among visible variables. We believe that such dependencies are crucial for faithful data representation. Specifically, during training, we construct two LRBNs to capture spatial patterns inherent in displacements of landmark points from spontaneous facial expressions and posed facial expressions respectively. During testing, the samples are classified into posed or spontaneous expressions according to their likelihoods on two models. Efficient learning and inference algorithms are proposed. Experimental results on two benchmark databases demonstrate the advantages of the proposed approach in modeling spatial patterns as well as its superior performance to the existing methods in differentiating between posed and spontaneous expressions.\nFig. The framework of capturing spatial patterns ","date":1483228800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1625057758,"objectID":"75161346ca7b0f063a04bee6dceaff5a","permalink":"https://ustc-ac.github.io/publication/dblp-confaaai-gan-nwj-17/","publishdate":"2021-06-30T12:55:58.520458Z","relpermalink":"/publication/dblp-confaaai-gan-nwj-17/","section":"publication","summary":"In this paper, we propose employing the LRBN to effectively capture the high-order and global dependencies among facial geometric features","tags":["Expression Recognition"],"title":"Differentiating Between Posed and Spontaneous Expressions with Latent Regression Bayesian Network","type":"publication"},{"authors":["Yangyang Shu","Shangfei Wang"],"categories":[],"content":"","date":1483228800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1625057759,"objectID":"401e3b7d58e2330dd2dabab8115521f5","permalink":"https://ustc-ac.github.io/publication/dblp-conficassp-shu-w-17/","publishdate":"2021-06-30T12:55:58.959245Z","relpermalink":"/publication/dblp-conficassp-shu-w-17/","section":"publication","summary":"","tags":[],"title":"Emotion recognition through integrating EEG and peripheral signals","type":"publication"},{"authors":["Tanfang Chen","Yaxin Wang","Shangfei Wang","Shiyu Chen"],"categories":[],"content":"The well-established film grammar is often used to change visual and audio elements of videos to invoke audiences’ emotional experience. Such film grammar, referred to as domain knowledge, is crucial for affective video content analyses, but has not been thoroughly explored yet. In this paper, we propose a novel method to analyze video affective content through exploring domain knowledge. Specifically, take visual elements as an example, we first infer probabilistic dependencies between visual elements and emotions from the summarized film grammar. Then, we transfer the domain knowledge as constraints, and formulate affective video content analyses as a constrained optimization problem. Experiments on the LIRIS-ACCEDE database and the DEAP database demonstrate that the proposed affective content analyses method can successfully leverage well-established film grammar for better emotion classification from video content.\n","date":1483228800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1625057760,"objectID":"87d223e498d092d136178fe064fe307a","permalink":"https://ustc-ac.github.io/publication/dblp-confmm-chen-wwc-17/","publishdate":"2021-06-30T12:56:00.089006Z","relpermalink":"/publication/dblp-confmm-chen-wwc-17/","section":"publication","summary":"Most current works employ discriminative features and efficient classifiers for affective video content analyses, without explicitly exploring and leveraging domain knowledge for affective video content analyses. Therefore, in this paper, we propose a novel method to analyze affective video content through exploring domain knowledge. Both audio elements and visual elements are used by film makers to communicate emotions to audience. As a primary study to explore film grammar for affective video content analyses, this paper takes visual elements as an example to demonstrate the feasibility of the proposed affective video content analyses method enhanced through exploring domain knowledge.","tags":["Affective Analyses"],"title":"Exploring Domain Knowledge for Affective Video Content Analyses","type":"publication"},{"authors":["Shangfei Wang","Quan Gan","Qiang Ji"],"categories":[],"content":"","date":1483228800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1625057758,"objectID":"e6c236736b74432833511957687b9f6f","permalink":"https://ustc-ac.github.io/publication/dblp-journalspr-wang-gj-17/","publishdate":"2021-06-30T12:55:57.870109Z","relpermalink":"/publication/dblp-journalspr-wang-gj-17/","section":"publication","summary":"","tags":[],"title":"Expression-assisted facial action unit recognition under incomplete AU annotation","type":"publication"},{"authors":["Shangfei Wang","Jiajia Yang","Zhen Gao","Qiang Ji"],"categories":[],"content":"","date":1483228800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1625057758,"objectID":"879238b791736ce0f7f82a25788b8ecc","permalink":"https://ustc-ac.github.io/publication/dblp-journalspr-wang-ygj-17/","publishdate":"2021-06-30T12:55:58.082979Z","relpermalink":"/publication/dblp-journalspr-wang-ygj-17/","section":"publication","summary":"","tags":[],"title":"Feature and label relation modeling for multiple-facial action unit classification and intensity estimation","type":"publication"},{"authors":["Shiyu Chen","Shangfei Wang","Tanfang Chen","Xiaoxiao Shi"],"categories":[],"content":"","date":1483228800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1625057760,"objectID":"c5073aadba2c70291a807eb1199cb253","permalink":"https://ustc-ac.github.io/publication/dblp-journalscorr-chen-wcs-17/","publishdate":"2021-06-30T12:56:00.319614Z","relpermalink":"/publication/dblp-journalscorr-chen-wcs-17/","section":"publication","summary":"","tags":[],"title":"Learning with Privileged Information for Multi-Label Classification","type":"publication"},{"authors":["Shan Wu","Shangfei Wang","Zhen Gao"],"categories":[],"content":"","date":1483228800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1625057758,"objectID":"ba82280e52bdcbf34327a37fa903da6c","permalink":"https://ustc-ac.github.io/publication/dblp-conficassp-wu-wg-17/","publishdate":"2021-06-30T12:55:58.750957Z","relpermalink":"/publication/dblp-conficassp-wu-wg-17/","section":"publication","summary":"","tags":[],"title":"Personalized video emotion tagging through a topic model","type":"publication"},{"authors":["Shangfei Wang","Chongliang Wu","Qiang Ji"],"categories":[],"content":"","date":1451606400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1625057760,"objectID":"fd79c574e56228343f5c9eb39194704d","permalink":"https://ustc-ac.github.io/publication/dblp-journalscviu-wang-wj-16/","publishdate":"2021-06-30T12:56:00.534489Z","relpermalink":"/publication/dblp-journalscviu-wang-wj-16/","section":"publication","summary":"","tags":[],"title":"Capturing global spatial patterns for distinguishing posed and spontaneous expressions","type":"publication"},{"authors":["Tanfang Chen","Shangfei Wang","Zhen Gao","Chongliang Wu"],"categories":[],"content":"","date":1451606400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1625057762,"objectID":"c659708b4c8efbde3a5f22ed938725ef","permalink":"https://ustc-ac.github.io/publication/dblp-confmir-chen-wgw-16/","publishdate":"2021-06-30T12:56:02.582568Z","relpermalink":"/publication/dblp-confmir-chen-wgw-16/","section":"publication","summary":"","tags":[],"title":"Emotion Recognition from EEG Signals Enhanced by User's Profile","type":"publication"},{"authors":["Shiyu Chen","Zhen Gao","Shangfei Wang"],"categories":[],"content":"","date":1451606400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1625057761,"objectID":"67f6f6487b7c3749051afbb7882b1ab2","permalink":"https://ustc-ac.github.io/publication/dblp-conficassp-chen-gw-16/","publishdate":"2021-06-30T12:56:01.462751Z","relpermalink":"/publication/dblp-conficassp-chen-gw-16/","section":"publication","summary":"","tags":[],"title":"Emotion recognition from peripheral physiological signals enhanced by EEG","type":"publication"},{"authors":["Shan Wu","Shangfei Wang","Yachen Zhu","Zhen Gao","Lihua Yue","Qiang Ji"],"categories":[],"content":"","date":1451606400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1625057762,"objectID":"3f93f5021e5079a334ffe99bc1833a88","permalink":"https://ustc-ac.github.io/publication/dblp-conficpr-wu-wzgyj-16/","publishdate":"2021-06-30T12:56:01.926903Z","relpermalink":"/publication/dblp-conficpr-wu-wzgyj-16/","section":"publication","summary":"","tags":[],"title":"Employing subjects' information as privileged information for emotion recognition from EEG signals","type":"publication"},{"authors":["Rui Zhao","Quan Gan","Shangfei Wang","Qiang Ji"],"categories":[],"content":"Previous studies on facial expression analysis have been focused on recognizing basic expression categories. There is limited amount of work on the continuous expression intensity estimation, which is important for detecting and tracking emotion change. Part of the reason is the lack of labeled data with annotated expression intensity since ex-pression intensity annotation requires expertise and is time consuming. In this work, we treat the expression intensity estimation as a regression problem. By taking advantage of the natural onset-apex-offset evolution pattern of facial ex-pression, the proposed method can handle different amounts of annotations to perform frame-level expression intensity estimation. In fully supervised case, all the frames are provided with intensity annotations. In weakly supervised case, only the annotations of selected key frames are used.While in unsupervised case, expression intensity can be es-timated without any annotations. An efficient optimization algorithm based on Alternating Direction Method of Mul-tipliers (ADMM) is developed for solving the optimization problem associated with parameter learning. We demon-strate the effectiveness of proposed method by comparing it against both fully supervised and unsupervised approaches on benchmark facial expression datasets.\nA diagram showing the experiment process. Depending on the experiment setting, different amounts of intensity annotation information are fed into model learning process, resulting different models. Training is performed using complete expression sequences while testing is performed on each frame of a sequence. ","date":1451606400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1625057761,"objectID":"38c1e3e4f7b56a5cc3e8c4b3b7b12fa3","permalink":"https://ustc-ac.github.io/publication/dblp-confcvpr-zhao-gwj-16/","publishdate":"2021-06-30T12:56:01.230382Z","relpermalink":"/publication/dblp-confcvpr-zhao-gwj-16/","section":"publication","summary":"Our contributions include the following aspects. First,we propose a regression approach for expression intensity estimation which exploits both ordinal relationship among different frames within an expression sequence and absolute intensity labels if available. Second, we introduce a unified max-margin learning framework to simultaneously exploit the two sources of information. An efficient algorithm to solve the optimization problem is developed.  Third, our method can generalize to different learning settings depend-ing on the availability of expression intensity annotations.","tags":["Expression Intensity Estimation"],"title":"Facial Expression Intensity Estimation Using Ordinal Information","type":"publication"},{"authors":["Shangfei Wang","Shan Wu","Zhen Gao","Qiang Ji"],"categories":[],"content":"","date":1451606400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1625057760,"objectID":"fd6e0b747e5937c5f955ba56dc50dcdb","permalink":"https://ustc-ac.github.io/publication/dblp-journalsmta-wang-wgj-16/","publishdate":"2021-06-30T12:56:00.754113Z","relpermalink":"/publication/dblp-journalsmta-wang-wgj-16/","section":"publication","summary":"","tags":[],"title":"Facial expression recognition through modeling age-related spatial patterns","type":"publication"},{"authors":["Chongliang Wu","Shangfei Wang","Bowen Pan","Huaping Chen"],"categories":[],"content":"This paper proposes a novel deep two-view approach to learn features from both visible and thermal images and leverage the commonality among visible and thermal images for facial expression recognition from visible images. The thermal images are used as privileged information, which is required only during training to help visible images learn better features and classifier. Specifically, we first learn a deep model for visible images and thermal images respectively, and use the learned feature representations to train SVM classifiers for expression classification. We then jointly refine the deep models as well as the SVM classifiers for both thermal images and visible images by imposing the constraint that the outputs of the SVM classifiers from two views are similar. Therefore, the resulting representations and classifiers capture the inherent connections among visible facial image, infrared facial image and target expression labels, and hence improve the recognition performance for facial expression recognition from visible images during testing. Experimental results on the benchmark expression database demonstrate the effectiveness of our proposed method.\nFig. The scheme of deep network for visible image data, thermal image data and the scheme of the proposed model. ","date":1451606400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1625057763,"objectID":"01fc95bd0a1bb85a26a3fafaccba4c4a","permalink":"https://ustc-ac.github.io/publication/dblp-confmm-wu-wpc-16/","publishdate":"2021-06-30T12:56:02.799651Z","relpermalink":"/publication/dblp-confmm-wu-wpc-16/","section":"publication","summary":"Although their constructed representation reflects thermal infrared images’ supplementary role for visible images, it has no direct relationship to target expression labels. Furthermore, the hand-craft visible and thermal features may not thoroughly reflect the expression patterns embedded in images. Therefore, in this paper, we propose a new deep two-view approach to learn features from both visible and thermal images and leverage the commonality among visible and thermal images for expression recognition.","tags":["Expression Recognition"],"title":"Facial Expression Recognition with Deep two-view Support Vector Machine","type":"publication"},{"authors":["Shangfei Wang","Zhen Gao","Shan He","Menghua He","Qiang Ji"],"categories":[],"content":"","date":1451606400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1625057761,"objectID":"6d39225ee5712ed1c85fc316a0a5ba16","permalink":"https://ustc-ac.github.io/publication/dblp-journalsmta-wang-ghhj-16/","publishdate":"2021-06-30T12:56:01.004419Z","relpermalink":"/publication/dblp-journalsmta-wang-ghhj-16/","section":"publication","summary":"","tags":[],"title":"Gender recognition from visible and thermal infrared facial images","type":"publication"},{"authors":["Shiyu Chen","Shangfei Wang","Chongliang Wu","Zhen Gao","Xiaoxiao Shi","Qiang Ji"],"categories":[],"content":"","date":1451606400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1625057761,"objectID":"7c406b8b049b952a4be4d8de1d740578","permalink":"https://ustc-ac.github.io/publication/dblp-conficpr-chen-wwgsj-16/","publishdate":"2021-06-30T12:56:01.692625Z","relpermalink":"/publication/dblp-conficpr-chen-wwgsj-16/","section":"publication","summary":"","tags":[],"title":"Implicit hybrid video emotion tagging by integrating video content and users' multiple physiological responses","type":"publication"},{"authors":["Shan Wu","Shangfei Wang","Qiang Ji"],"categories":[],"content":"","date":1451606400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1625057762,"objectID":"492044f8639588dbfa3fb305a736103a","permalink":"https://ustc-ac.github.io/publication/dblp-conficpr-wu-wj-16/","publishdate":"2021-06-30T12:56:02.136916Z","relpermalink":"/publication/dblp-conficpr-wu-wj-16/","section":"publication","summary":"","tags":[],"title":"Multiple Facial Action Unit recognition by learning joint features and label relations","type":"publication"},{"authors":["Jiajia Yang","Shan Wu","Shangfei Wang","Qiang Ji"],"categories":[],"content":"","date":1451606400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1625057762,"objectID":"fe8a96eba239ffa6917d6ddc8e67f17f","permalink":"https://ustc-ac.github.io/publication/dblp-conficpr-yang-wwj-16/","publishdate":"2021-06-30T12:56:02.365702Z","relpermalink":"/publication/dblp-conficpr-yang-wwj-16/","section":"publication","summary":"","tags":[],"title":"Multiple facial action unit recognition enhanced by facial expressions","type":"publication"},{"authors":["Chongliang Wu","Shangfei Wang"],"categories":[],"content":"","date":1451606400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1625057763,"objectID":"58986aaf344e6b364e460454bbc2436b","permalink":"https://ustc-ac.github.io/publication/dblp-confmmm-wu-w-16/","publishdate":"2021-06-30T12:56:03.027673Z","relpermalink":"/publication/dblp-confmmm-wu-w-16/","section":"publication","summary":"","tags":[],"title":"Posed and Spontaneous Expression Recognition Through Restricted Boltzmann Machine","type":"publication"},{"authors":["Zhen Gao","Shangfei Wang"],"categories":[],"content":"","date":1420070400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1625057766,"objectID":"854d383636f0ce4ced56772dea6bfecd","permalink":"https://ustc-ac.github.io/publication/dblp-confpcm-gao-w-15/","publishdate":"2021-06-30T12:56:06.540008Z","relpermalink":"/publication/dblp-confpcm-gao-w-15/","section":"publication","summary":"","tags":[],"title":"Emotion Recognition from EEG Signals by Leveraging Stimulus Videos","type":"publication"},{"authors":["Zhen Gao","Shangfei Wang"],"categories":[],"content":"","date":1420070400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1625057766,"objectID":"6e347b7f4afce6e757a67a4ce521f909","permalink":"https://ustc-ac.github.io/publication/dblp-confmir-gao-w-15/","publishdate":"2021-06-30T12:56:06.310459Z","relpermalink":"/publication/dblp-confmir-gao-w-15/","section":"publication","summary":"","tags":[],"title":"Emotion Recognition from EEG Signals using Hierarchical Bayesian Network with Privileged Information","type":"publication"},{"authors":["Shangfei Wang","Yachen Zhu","Lihua Yue","Qiang Ji"],"categories":[],"content":"","date":1420070400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1625057764,"objectID":"6f615577b4ca3c6d5c7823d2c1f54496","permalink":"https://ustc-ac.github.io/publication/dblp-journalstamd-wang-zyj-15/","publishdate":"2021-06-30T12:56:04.398538Z","relpermalink":"/publication/dblp-journalstamd-wang-zyj-15/","section":"publication","summary":"","tags":[],"title":"Emotion Recognition with the Help of Privileged Information","type":"publication"},{"authors":["Shan Wu","Shangfei Wang","Jun Wang"],"categories":[],"content":"","date":1420070400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1625057765,"objectID":"4fd618ca6763f47e05b8aad79850c75b","permalink":"https://ustc-ac.github.io/publication/dblp-conffgr-wu-ww-15/","publishdate":"2021-06-30T12:56:05.35752Z","relpermalink":"/publication/dblp-conffgr-wu-ww-15/","section":"publication","summary":"","tags":[],"title":"Enhanced facial expression recognition by age","type":"publication"},{"authors":["Xiaoxiao Shi","Shangfei Wang","Yachen Zhu"],"categories":[],"content":"","date":1420070400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1625057766,"objectID":"5ffc042eeb54d49d9dec354d56574de6","permalink":"https://ustc-ac.github.io/publication/dblp-confmir-shi-wz-15/","publishdate":"2021-06-30T12:56:05.834417Z","relpermalink":"/publication/dblp-confmir-shi-wz-15/","section":"publication","summary":"","tags":[],"title":"Expression Recognition from Visible Images with the Help of Thermal Images","type":"publication"},{"authors":["Jun Wang","Shangfei Wang","Qiang Ji"],"categories":[],"content":"","date":1420070400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1625057765,"objectID":"1bee77228f1ac9ce707a681a03609b67","permalink":"https://ustc-ac.github.io/publication/dblp-confmir-wang-wj-15/","publishdate":"2021-06-30T12:56:05.589311Z","relpermalink":"/publication/dblp-confmir-wang-wj-15/","section":"publication","summary":"","tags":[],"title":"Facial Action Unit Classification with Hidden Knowledge under Incomplete Annotation","type":"publication"},{"authors":["Shangfei Wang","Zhilei Liu","Yachen Zhu","Menghua He","Xiaoping Chen","Qiang Ji"],"categories":[],"content":"","date":1420070400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1625057763,"objectID":"e0c1a9d4bc46945b5d332e27c800b80f","permalink":"https://ustc-ac.github.io/publication/dblp-journalsmta-wang-lzhcj-15/","publishdate":"2021-06-30T12:56:03.727773Z","relpermalink":"/publication/dblp-journalsmta-wang-lzhcj-15/","section":"publication","summary":"","tags":[],"title":"Implicit video emotion tagging from audiences' facial expression","type":"publication"},{"authors":["Shangfei Wang","Menghua He","Yachen Zhu","Shan He","Yue Liu","Qiang Ji"],"categories":[],"content":"","date":1420070400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1625057763,"objectID":"a637bf1480815c8ec37e9b2dfc374923","permalink":"https://ustc-ac.github.io/publication/dblp-journalsfcsc-wang-hzhlj-15/","publishdate":"2021-06-30T12:56:03.241726Z","relpermalink":"/publication/dblp-journalsfcsc-wang-hzhlj-15/","section":"publication","summary":"","tags":[],"title":"Learning with privileged information using Bayesian networks","type":"publication"},{"authors":["Chongliang Wu","Shangfei Wang","Qiang Ji"],"categories":[],"content":"","date":1420070400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1625057765,"objectID":"b853ae0d3d74f233babc176b2521ed80","permalink":"https://ustc-ac.github.io/publication/dblp-conffgr-wu-wj-15/","publishdate":"2021-06-30T12:56:05.079016Z","relpermalink":"/publication/dblp-conffgr-wu-wj-15/","section":"publication","summary":"","tags":[],"title":"Multi-instance Hidden Markov Model for facial expression recognition","type":"publication"},{"authors":["Zhen Gao","Shangfei Wang","Qiang Ji"],"categories":[],"content":"","date":1420070400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1625057766,"objectID":"e526b994ea2ac771409fe07f58fc09c7","permalink":"https://ustc-ac.github.io/publication/dblp-confmir-gao-wj-15/","publishdate":"2021-06-30T12:56:06.066656Z","relpermalink":"/publication/dblp-confmir-gao-wj-15/","section":"publication","summary":"","tags":[],"title":"Multiple Aesthetic Attribute Assessment by Exploiting Relations Among Aesthetic Attributes","type":"publication"},{"authors":["Shangfei Wang","Jun Wang","Ziheng Wang","Qiang Ji"],"categories":[],"content":"","date":1420070400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1625057764,"objectID":"a1cb3e9aae21dcbb205121731f69b7a8","permalink":"https://ustc-ac.github.io/publication/dblp-journalstmm-wang-wwj-15/","publishdate":"2021-06-30T12:56:04.644211Z","relpermalink":"/publication/dblp-journalstmm-wang-wwj-15/","section":"publication","summary":"","tags":[],"title":"Multiple Emotion Tagging for Multimedia Data by Exploiting High-Order Dependencies Among Emotions","type":"publication"},{"authors":["Shangfei Wang","Zhaoyu Wang","Qiang Ji"],"categories":[],"content":"","date":1420070400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1625057763,"objectID":"328791a34336da96967ce11b1ca48254","permalink":"https://ustc-ac.github.io/publication/dblp-journalsmta-wang-wj-15/","publishdate":"2021-06-30T12:56:03.46365Z","relpermalink":"/publication/dblp-journalsmta-wang-wj-15/","section":"publication","summary":"","tags":[],"title":"Multiple emotional tagging of multimedia data by exploiting dependencies among emotions","type":"publication"},{"authors":["Shangfei Wang","Chongliang Wu","Menghua He","Jun Wang","Qiang Ji"],"categories":[],"content":"","date":1420070400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1625057764,"objectID":"3987e38c9c0de5faaa698fda489e1611","permalink":"https://ustc-ac.github.io/publication/dblp-journalsmva-wang-whwj-15/","publishdate":"2021-06-30T12:56:03.949932Z","relpermalink":"/publication/dblp-journalsmva-wang-whwj-15/","section":"publication","summary":"","tags":[],"title":"Posed and spontaneous expression recognition through modeling their spatial patterns","type":"publication"},{"authors":["Quan Gan","Chongliang Wu","Shangfei Wang","Qiang Ji"],"categories":[],"content":"","date":1420070400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1625057765,"objectID":"1c2f039749af9cd9eee7ad328bad631c","permalink":"https://ustc-ac.github.io/publication/dblp-confacii-gan-wwj-15/","publishdate":"2021-06-30T12:56:04.865666Z","relpermalink":"/publication/dblp-confacii-gan-wwj-15/","section":"publication","summary":"","tags":[],"title":"Posed and spontaneous facial expression differentiation using deep Boltzmann machines","type":"publication"},{"authors":["Shangfei Wang","Qiang Ji"],"categories":[],"content":"","date":1420070400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1625057764,"objectID":"68a6b7413047c4d2ea30fd0c78bcf21b","permalink":"https://ustc-ac.github.io/publication/dblp-journalstaffco-wang-j-15/","publishdate":"2021-06-30T12:56:04.171569Z","relpermalink":"/publication/dblp-journalstaffco-wang-j-15/","section":"publication","summary":"","tags":[],"title":"Video Affective Content Analysis: A Survey of State-of-the-Art Methods","type":"publication"},{"authors":["Jun Wang","Shangfei Wang","Mengdi Huai","Chongliang Wu","Zhen Gao","Yue Liu","Qiang Ji"],"categories":[],"content":"","date":1388534400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1625057768,"objectID":"8e08071009858d881e0d7098e2139c66","permalink":"https://ustc-ac.github.io/publication/dblp-conficmcs-wang-whwglj-14/","publishdate":"2021-06-30T12:56:08.13032Z","relpermalink":"/publication/dblp-conficmcs-wang-whwglj-14/","section":"publication","summary":"","tags":[],"title":"Capture expression-dependent AU relations for expression recognition","type":"publication"},{"authors":["Jun Wang","Shangfei Wang","Qiang Ji"],"categories":[],"content":"","date":1388534400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1625057769,"objectID":"86617babe99a4a299a86112de15773f5","permalink":"https://ustc-ac.github.io/publication/dblp-conficpr-wang-wj-14-a/","publishdate":"2021-06-30T12:56:09.227627Z","relpermalink":"/publication/dblp-conficpr-wang-wj-14-a/","section":"publication","summary":"","tags":[],"title":"Early Facial Expression Recognition Using Hidden Markov Models","type":"publication"},{"authors":["Shangfei Wang","Menghua He","Zhen Gao","Shan He","Qiang Ji"],"categories":[],"content":"","date":1388534400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1625057767,"objectID":"a473521add99178dba65a3a131c2f1d6","permalink":"https://ustc-ac.github.io/publication/dblp-journalsfcsc-wang-hghj-14/","publishdate":"2021-06-30T12:56:06.999821Z","relpermalink":"/publication/dblp-journalsfcsc-wang-hghj-14/","section":"publication","summary":"","tags":[],"title":"Emotion recognition from thermal infrared images using deep Boltzmann machine","type":"publication"},{"authors":["Yachen Zhu","Shangfei Wang","Qiang Ji"],"categories":[],"content":"","date":1388534400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1625057768,"objectID":"ed6e4d7d4b7a54b860545ef8f5524b1d","permalink":"https://ustc-ac.github.io/publication/dblp-conficmcs-zhu-wj-14/","publishdate":"2021-06-30T12:56:08.572832Z","relpermalink":"/publication/dblp-conficmcs-zhu-wj-14/","section":"publication","summary":"","tags":[],"title":"Emotion recognition from users' EEG signals with the help of stimulus VIDEOS","type":"publication"},{"authors":["Shangfei Wang","Jun Wang","Zhaoyu Wang","Qiang Ji"],"categories":[],"content":"","date":1388534400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1625057767,"objectID":"6e73b3a917feb58f3de5444ae0188937","permalink":"https://ustc-ac.github.io/publication/dblp-journalspr-wang-wwj-14/","publishdate":"2021-06-30T12:56:07.702515Z","relpermalink":"/publication/dblp-journalspr-wang-wwj-14/","section":"publication","summary":"","tags":[],"title":"Enhancing multi-label classification by modeling dependencies among labels","type":"publication"},{"authors":["Shangfei Wang","Zhilei Liu","Jun Wang","Zhaoyu Wang","Yongqiang Li","Xiaoping Chen","Qiang Ji"],"categories":[],"content":"","date":1388534400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1625057767,"objectID":"b438e078701b6102d9cea927ad1edfc6","permalink":"https://ustc-ac.github.io/publication/dblp-journalsivc-wang-lwwlcj-14/","publishdate":"2021-06-30T12:56:07.243311Z","relpermalink":"/publication/dblp-journalsivc-wang-lwwlcj-14/","section":"publication","summary":"","tags":[],"title":"Exploiting multi-expression dependences for implicit multi-emotion video tagging","type":"publication"},{"authors":["Zhen Gao","Shangfei Wang","Chongliang Wu","Jun Wang","Qiang Ji"],"categories":[],"content":"","date":1388534400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1625057768,"objectID":"5fd14963704aa05ab9b6ace42ce0d022","permalink":"https://ustc-ac.github.io/publication/dblp-conficmcs-gao-wwwj-14/","publishdate":"2021-06-30T12:56:07.917019Z","relpermalink":"/publication/dblp-conficmcs-gao-wwwj-14/","section":"publication","summary":"","tags":[],"title":"Facial Action Unit recognition by relation modeling from both qualitative knowledge and quantitative data","type":"publication"},{"authors":["Shangfei Wang","Shan He","Yue Wu","Menghua He","Qiang Ji"],"categories":[],"content":"","date":1388534400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1625057766,"objectID":"78317dead9c340d61a9739bca4d68af3","permalink":"https://ustc-ac.github.io/publication/dblp-journalsfcsc-wang-hwhj-14/","publishdate":"2021-06-30T12:56:06.786209Z","relpermalink":"/publication/dblp-journalsfcsc-wang-hwhj-14/","section":"publication","summary":"","tags":[],"title":"Fusion of visible and thermal images for facial expression recognition","type":"publication"},{"authors":["Shangfei Wang","Yachen Zhu","Guobing Wu","Qiang Ji"],"categories":[],"content":"","date":1388534400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1625057767,"objectID":"453b9c1ee19d15e392df62465d3935c0","permalink":"https://ustc-ac.github.io/publication/dblp-journalsmta-wang-zwj-14/","publishdate":"2021-06-30T12:56:07.456858Z","relpermalink":"/publication/dblp-journalsmta-wang-zwj-14/","section":"publication","summary":"","tags":[],"title":"Hybrid video emotional tagging using users' EEG and video content","type":"publication"},{"authors":["Baoyuan Wu","Zhilei Liu","Shangfei Wang","Bao-Gang Hu","Qiang Ji"],"categories":[],"content":"","date":1388534400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1625057769,"objectID":"c9db80d616749d38c6dd6a0bb4cc1657","permalink":"https://ustc-ac.github.io/publication/dblp-conficpr-wu-lwhj-14/","publishdate":"2021-06-30T12:56:09.002549Z","relpermalink":"/publication/dblp-conficpr-wu-lwhj-14/","section":"publication","summary":"","tags":[],"title":"Multi-label Learning with Missing Labels","type":"publication"},{"authors":["Yachen Zhu","Shangfei Wang","Lihua Yue","Qiang Ji"],"categories":[],"content":"","date":1388534400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1625057768,"objectID":"e9cf7fa00ab460e51875d32ac9b9e072","permalink":"https://ustc-ac.github.io/publication/dblp-conficpr-zhu-wyj-14/","publishdate":"2021-06-30T12:56:08.787074Z","relpermalink":"/publication/dblp-conficpr-zhu-wyj-14/","section":"publication","summary":"","tags":[],"title":"Multiple-Facial Action Unit Recognition by Shared Feature Learning and Semantic Relation Modeling","type":"publication"},{"authors":["Zhaoyu Wang","Jun Wang","Shangfei Wang","Qiang Ji"],"categories":[],"content":"","date":1388534400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1625057768,"objectID":"059b0d4c2f995d41a4c8be2dc7b34d4b","permalink":"https://ustc-ac.github.io/publication/dblp-conficmcs-wang-wwj-14/","publishdate":"2021-06-30T12:56:08.350627Z","relpermalink":"/publication/dblp-conficmcs-wang-wwj-14/","section":"publication","summary":"","tags":[],"title":"Sequence-based bias analysis of spontaneous facial expression databases","type":"publication"},{"authors":["Menghua He","Shangfei Wang","Qiang Ji"],"categories":[],"content":"","date":1356998400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1625057770,"objectID":"57f61f409e967c83fa26a2a6067786af","permalink":"https://ustc-ac.github.io/publication/dblp-confacii-he-wj-13/","publishdate":"2021-06-30T12:56:10.093781Z","relpermalink":"/publication/dblp-confacii-he-wj-13/","section":"publication","summary":"","tags":[],"title":"Active Labeling of Facial Feature Points","type":"publication"},{"authors":["Shangfei Wang","Zhilei Liu","Zhaoyu Wang","Guobing Wu","Peijia Shen","Shan He","Xufa Wang"],"categories":[],"content":"","date":1356998400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1625057769,"objectID":"a224c16de92b8be5353318433d05560b","permalink":"https://ustc-ac.github.io/publication/dblp-journalstaffco-wang-lwwshw-13/","publishdate":"2021-06-30T12:56:09.676647Z","relpermalink":"/publication/dblp-journalstaffco-wang-lwwshw-13/","section":"publication","summary":"","tags":[],"title":"Analyses of a Multimodal Spontaneous Facial Expression Database","type":"publication"},{"authors":["Menghua He","Shangfei Wang","Zhilei Liu","Xiaoping Chen"],"categories":[],"content":"","date":1356998400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1625057770,"objectID":"23e8dade0758eb36209535f71ebde73b","permalink":"https://ustc-ac.github.io/publication/dblp-confacii-he-wlc-13/","publishdate":"2021-06-30T12:56:10.325485Z","relpermalink":"/publication/dblp-confacii-he-wlc-13/","section":"publication","summary":"","tags":[],"title":"Analyses of the Differences between Posed and Spontaneous Facial Expressions","type":"publication"},{"authors":["Ziheng Wang","Shangfei Wang","Qiang Ji"],"categories":[],"content":"","date":1356998400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1625057771,"objectID":"a6fb5c0146cc21e2f87e8af13ed9047a","permalink":"https://ustc-ac.github.io/publication/dblp-confcvpr-wang-wj-13/","publishdate":"2021-06-30T12:56:11.002203Z","relpermalink":"/publication/dblp-confcvpr-wang-wj-13/","section":"publication","summary":"","tags":[],"title":"Capturing Complex Spatio-temporal Relations among Facial Muscles for Facial Expression Recognition","type":"publication"},{"authors":["Ziheng Wang","Yongqiang Li","Shangfei Wang","Qiang Ji"],"categories":[],"content":"","date":1356998400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1625057771,"objectID":"23408e69f466d42f94b1b7d083df41b3","permalink":"https://ustc-ac.github.io/publication/dblp-conficcv-wang-lwj-13/","publishdate":"2021-06-30T12:56:11.663861Z","relpermalink":"/publication/dblp-conficcv-wang-lwj-13/","section":"publication","summary":"","tags":[],"title":"Capturing Global Semantic Relationships for Facial Action Unit Recognition","type":"publication"},{"authors":["Yachen Zhu","Xilan Tian","Guobing Wu","Gilles Gasso","Shangfei Wang","Stéphane Canu"],"categories":[],"content":"","date":1356998400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1625057770,"objectID":"8342be2d4f9e866a466bbad77f27cbac","permalink":"https://ustc-ac.github.io/publication/dblp-confacii-zhu-twgwc-13/","publishdate":"2021-06-30T12:56:10.775938Z","relpermalink":"/publication/dblp-confacii-zhu-twgwc-13/","section":"publication","summary":"","tags":[],"title":"Emotional Influence on SSVEP Based BCI","type":"publication"},{"authors":["Zhaoyu Wang","Shangfei Wang","Menghua He","Zhilei Liu","Qiang Ji"],"categories":[],"content":"","date":1356998400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1625057771,"objectID":"8ec4124666326f8f0246ad4f5602d02d","permalink":"https://ustc-ac.github.io/publication/dblp-conffgr-wang-whlj-13/","publishdate":"2021-06-30T12:56:11.451103Z","relpermalink":"/publication/dblp-conffgr-wang-whlj-13/","section":"publication","summary":"","tags":[],"title":"Emotional tagging of videos by exploring multiple emotions' coexistence","type":"publication"},{"authors":["Shangfei Wang","Zhilei Liu","Peijia Shen","Qiang Ji"],"categories":[],"content":"","date":1356998400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1625057769,"objectID":"bd2020f8314cdeb3bcb27a2609e0940e","permalink":"https://ustc-ac.github.io/publication/dblp-journalspr-wang-lsj-13/","publishdate":"2021-06-30T12:56:09.453771Z","relpermalink":"/publication/dblp-journalspr-wang-lsj-13/","section":"publication","summary":"","tags":[],"title":"Eye localization from thermal infrared images","type":"publication"},{"authors":["Shan He","Shangfei Wang","Wuwei Lan","Huan Fu","Qiang Ji"],"categories":[],"content":"","date":1356998400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1625057770,"objectID":"ec92a54cb89f87c8e929f183ff05fcf5","permalink":"https://ustc-ac.github.io/publication/dblp-confacii-he-wlfj-13/","publishdate":"2021-06-30T12:56:10.543775Z","relpermalink":"/publication/dblp-confacii-he-wlfj-13/","section":"publication","summary":"","tags":[],"title":"Facial Expression Recognition Using Deep Boltzmann Machine from Thermal Infrared Images","type":"publication"},{"authors":["Zhilei Liu","Shangfei Wang","Zhaoyu Wang","Qiang Ji"],"categories":[],"content":"","date":1356998400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1625057771,"objectID":"ad1115b4f0b7a83ed47250a0f6dcb4a0","permalink":"https://ustc-ac.github.io/publication/dblp-conffgr-liu-wwj-13/","publishdate":"2021-06-30T12:56:11.233495Z","relpermalink":"/publication/dblp-conffgr-liu-wwj-13/","section":"publication","summary":"","tags":[],"title":"Implicit video multi-emotion tagging by exploiting multi-expression relations","type":"publication"},{"authors":["Yongqiang Li","Shangfei Wang","Yongping Zhao","Qiang Ji"],"categories":[],"content":"","date":1356998400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1625057770,"objectID":"e176fc059536c4c2c81a3005f3c97f77","permalink":"https://ustc-ac.github.io/publication/dblp-journalstip-li-wzj-13/","publishdate":"2021-06-30T12:56:09.886709Z","relpermalink":"/publication/dblp-journalstip-li-wzj-13/","section":"publication","summary":"","tags":[],"title":"Simultaneous Facial Feature Tracking and Facial Expression Recognition","type":"publication"},{"authors":["Shangfei Wang","Rui Ding"],"categories":[],"content":"","date":1325376000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1625057772,"objectID":"8fe2669d5404e68e18884eaba4df7bba","permalink":"https://ustc-ac.github.io/publication/dblp-journalsfcsc-wang-d-12/","publishdate":"2021-06-30T12:56:11.89594Z","relpermalink":"/publication/dblp-journalsfcsc-wang-d-12/","section":"publication","summary":"","tags":[],"title":"A qualitative and quantitative study of color emotion using valence-arousal","type":"publication"},{"authors":["Shangfei Wang","Guobing Wu","Yachen Zhu"],"categories":[],"content":"","date":1325376000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1625057772,"objectID":"c9ed95e57b4d8e565de3fdce768e06e4","permalink":"https://ustc-ac.github.io/publication/dblp-confias-wang-wz-12/","publishdate":"2021-06-30T12:56:12.788644Z","relpermalink":"/publication/dblp-confias-wang-wz-12/","section":"publication","summary":"","tags":[],"title":"Analysis of Affective Effects on Steady-State Visual Evoked Potential Responses","type":"publication"},{"authors":["Zhaoyu Wang","Shangfei Wang","Yachen Zhu","Qiang Ji"],"categories":[],"content":"","date":1325376000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1625057773,"objectID":"46d8bb4079a4530f7d67ff27c6457b86","permalink":"https://ustc-ac.github.io/publication/dblp-conficpr-wang-wzj-12/","publishdate":"2021-06-30T12:56:13.22242Z","relpermalink":"/publication/dblp-conficpr-wang-wzj-12/","section":"publication","summary":"","tags":[],"title":"Bias analyses of spontaneous facial expression database","type":"publication"},{"authors":["Shangfei Wang","Peijia Shen","Zhilei Liu"],"categories":[],"content":"","date":1325376000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1625057773,"objectID":"cf4a7b496b929606c29b49170fdf1268","permalink":"https://ustc-ac.github.io/publication/dblp-confmprss-wang-sl-12/","publishdate":"2021-06-30T12:56:13.649022Z","relpermalink":"/publication/dblp-confmprss-wang-sl-12/","section":"publication","summary":"","tags":[],"title":"Eye Localization from Infrared Thermal Images","type":"publication"},{"authors":["Shangfei Wang","Peijia Shen","Zhilei Liu"],"categories":[],"content":"","date":1325376000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1625057772,"objectID":"26b4d5d83db17d5e9b98d9552fadd5f5","permalink":"https://ustc-ac.github.io/publication/dblp-confccis-wang-sl-12/","publishdate":"2021-06-30T12:56:12.116887Z","relpermalink":"/publication/dblp-confccis-wang-sl-12/","section":"publication","summary":"","tags":[],"title":"Facial expression recognition from infrared thermal images using temperature difference by voting","type":"publication"},{"authors":["Peijia Shen","Shangfei Wang","Zhilei Liu"],"categories":[],"content":"","date":1325376000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1625057772,"objectID":"4f405eba24418f20b97869d89bd8209a","permalink":"https://ustc-ac.github.io/publication/dblp-confias-shen-wl-12/","publishdate":"2021-06-30T12:56:12.554304Z","relpermalink":"/publication/dblp-confias-shen-wl-12/","section":"publication","summary":"","tags":[],"title":"Facial Expression Recognition from Infrared Thermal Videos","type":"publication"},{"authors":["Zhilei Liu","Shangfei Wang"],"categories":[],"content":"","date":1325376000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1625057773,"objectID":"8c7901243aed1716a3e73d44221def49","permalink":"https://ustc-ac.github.io/publication/dblp-conficpr-liu-w-12/","publishdate":"2021-06-30T12:56:13.008345Z","relpermalink":"/publication/dblp-conficpr-liu-w-12/","section":"publication","summary":"","tags":[],"title":"Posed and spontaneous expression distinguishment from infrared thermal images","type":"publication"},{"authors":["Shangfei Wang","Shan He","Hua Zhu"],"categories":[],"content":"","date":1325376000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1625057773,"objectID":"7e9df97db9d8a5c1010ea171074cd3a0","permalink":"https://ustc-ac.github.io/publication/dblp-confisnn-wang-hz-12/","publishdate":"2021-06-30T12:56:13.432684Z","relpermalink":"/publication/dblp-confisnn-wang-hz-12/","section":"publication","summary":"","tags":[],"title":"Similarity Measurement and Feature Selection Using Genetic Algorithm","type":"publication"},{"authors":["Shangfei Wang","Shan He"],"categories":[],"content":"","date":1325376000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1625057772,"objectID":"e7cb30fe42723520c876edf1790961e9","permalink":"https://ustc-ac.github.io/publication/dblp-confias-wang-h-12/","publishdate":"2021-06-30T12:56:12.324481Z","relpermalink":"/publication/dblp-confias-wang-h-12/","section":"publication","summary":"","tags":[],"title":"Spontaneous Facial Expression Recognition by Fusing Thermal Infrared and Visible Images","type":"publication"},{"authors":["Yanpeng Lv","Shangfei Wang","Peijia Shen"],"categories":[],"content":"","date":1293840000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1625057774,"objectID":"5ceb61b761be679187db288fbdf72ca1","permalink":"https://ustc-ac.github.io/publication/dblp-conficimcs-lv-ws-11/","publishdate":"2021-06-30T12:56:14.532526Z","relpermalink":"/publication/dblp-conficimcs-lv-ws-11/","section":"publication","summary":"","tags":[],"title":"A real-time attitude recognition by eye-tracking","type":"publication"},{"authors":["Shangfei Wang","Huan Lin","Yongjie Hu"],"categories":[],"content":"","date":1293840000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1625057774,"objectID":"a729283fb9bd3f710d6e5f7e552f6481","permalink":"https://ustc-ac.github.io/publication/dblp-confisnn-wang-lh-11/","publishdate":"2021-06-30T12:56:14.762428Z","relpermalink":"/publication/dblp-confisnn-wang-lh-11/","section":"publication","summary":"","tags":[],"title":"Affective Classification in Video Based on Semi-supervised Learning","type":"publication"},{"authors":["Zhilei Liu","Shangfei Wang"],"categories":[],"content":"","date":1293840000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1625057774,"objectID":"bf004297b53e77687fb6c5921d05b073","permalink":"https://ustc-ac.github.io/publication/dblp-confacii-liu-w-11/","publishdate":"2021-06-30T12:56:13.881623Z","relpermalink":"/publication/dblp-confacii-liu-w-11/","section":"publication","summary":"","tags":[],"title":"Emotion Recognition Using Hidden Markov Models from Facial Temperature Sequence","type":"publication"},{"authors":["Shan He","Shangfei Wang","Yanpeng Lv"],"categories":[],"content":"","date":1293840000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1625057774,"objectID":"9ab2ec7ad6359162244d2186eb081ddd","permalink":"https://ustc-ac.github.io/publication/dblp-conficig-he-wl-11/","publishdate":"2021-06-30T12:56:14.30901Z","relpermalink":"/publication/dblp-conficig-he-wl-11/","section":"publication","summary":"","tags":[],"title":"Spontaneous Facial Expression Recognition Based on Feature Point Tracking","type":"publication"},{"authors":["Zhaoyu Wang","Shangfei Wang"],"categories":[],"content":"","date":1293840000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1625057775,"objectID":"d3d0d6c394347dbb60e79706082b4a7d","permalink":"https://ustc-ac.github.io/publication/dblp-confmlsp-wang-w-11/","publishdate":"2021-06-30T12:56:14.978557Z","relpermalink":"/publication/dblp-confmlsp-wang-w-11/","section":"publication","summary":"","tags":[],"title":"Spontaneous facial expression recognition by using feature-level fusion of visible and thermal infrared images","type":"publication"},{"authors":["Rong Zhang","Shangfei Wang","Xiaoping Chen","Dong Yin","Shijia Chen","Min Cheng","Yanpeng Lv","Jianmin Ji","Dejian Wang","Peijia Shen"],"categories":[],"content":"","date":1293840000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1625057774,"objectID":"2e9e18075c566527a4ef49f3aac8a3e3","permalink":"https://ustc-ac.github.io/publication/dblp-confatal-zhang-wcyccljws-11/","publishdate":"2021-06-30T12:56:14.08928Z","relpermalink":"/publication/dblp-confatal-zhang-wcyccljws-11/","section":"publication","summary":"","tags":[],"title":"Towards robot incremental learning constraints from comparative demonstration","type":"publication"},{"authors":["Shangfei Wang","Zhilei Liu","Siliang Lv","Yanpeng Lv","Guobing Wu","Peng Peng","Fei Chen","Xufa Wang"],"categories":[],"content":"","date":1262304000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1625057775,"objectID":"53bb7f48afa5a9d33f01cfe2ba155920","permalink":"https://ustc-ac.github.io/publication/dblp-journalstmm-wang-lllwpcw-10/","publishdate":"2021-06-30T12:56:15.205635Z","relpermalink":"/publication/dblp-journalstmm-wang-lllwpcw-10/","section":"publication","summary":"","tags":[],"title":"A Natural Visible and Infrared Facial Expression Database for Expression Recognition and Emotion Inference","type":"publication"},{"authors":["Yanpeng Lv","Shangfei Wang"],"categories":[],"content":"","date":1262304000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1625057776,"objectID":"d84e4a8a7a116be3dbb5b8e682b88c1e","permalink":"https://ustc-ac.github.io/publication/dblp-confnabic-lv-w-10/","publishdate":"2021-06-30T12:56:16.071008Z","relpermalink":"/publication/dblp-confnabic-lv-w-10/","section":"publication","summary":"","tags":[],"title":"A spontaneous facial expression recognition method using head motion and AAM features","type":"publication"},{"authors":["Bin Xu","Shangfei Wang","Xian Li"],"categories":[],"content":"","date":1262304000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1625057775,"objectID":"6d349bbe682cbd1a4bd8df704c835293","permalink":"https://ustc-ac.github.io/publication/dblp-confcec-xu-wl-10/","publishdate":"2021-06-30T12:56:15.650637Z","relpermalink":"/publication/dblp-confcec-xu-wl-10/","section":"publication","summary":"","tags":[],"title":"An emotional harmony generation system","type":"publication"},{"authors":["Shangfei Wang","Zhilei Liu"],"categories":[],"content":"","date":1262304000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1625057776,"objectID":"3231fe1c1f053c8d6cd3ffbb0758f032","permalink":"https://ustc-ac.github.io/publication/dblp-confisnn-wang-l-10/","publishdate":"2021-06-30T12:56:15.858117Z","relpermalink":"/publication/dblp-confisnn-wang-l-10/","section":"publication","summary":"","tags":[],"title":"Infrared Face Recognition Based on Histogram and K-Nearest Neighbor Classification","type":"publication"},{"authors":["Shangfei Wang","Hua Zhu"],"categories":[],"content":"","date":1262304000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1625057775,"objectID":"c893ff62d41ffb164b21a736cd09128e","permalink":"https://ustc-ac.github.io/publication/dblp-confcec-wang-z-10/","publishdate":"2021-06-30T12:56:15.431847Z","relpermalink":"/publication/dblp-confcec-wang-z-10/","section":"publication","summary":"","tags":[],"title":"Musical perceptual similarity estimation using interactive genetic algorithm","type":"publication"},{"authors":["Siliang Lv","Shangfei Wang","Xufa Wang"],"categories":[],"content":"","date":1230768000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1625057776,"objectID":"1bec00615c939fb863389e292c22c822","permalink":"https://ustc-ac.github.io/publication/dblp-confgecco-lv-ww-09/","publishdate":"2021-06-30T12:56:16.29235Z","relpermalink":"/publication/dblp-confgecco-lv-ww-09/","section":"publication","summary":"","tags":[],"title":"Emotional speech synthesis by XML file using interactive genetic algorithms","type":"publication"},{"authors":["Rui Ding","Shangfei Wang","Yongjie Hu","Haibao Wang"],"categories":[],"content":"","date":1199145600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1625057776,"objectID":"8601319ba420af2ff0a7068928e89195","permalink":"https://ustc-ac.github.io/publication/dblp-confcsse-ding-whw-08/","publishdate":"2021-06-30T12:56:16.51824Z","relpermalink":"/publication/dblp-confcsse-ding-whw-08/","section":"publication","summary":"","tags":[],"title":"Analysis of Relationships between Color and Emotion by Classification Based on Associations","type":"publication"},{"authors":["Hua Zhu","Shangfei Wang","Zhen Wang"],"categories":[],"content":"","date":1199145600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1625057776,"objectID":"778f0c276d0a8474e3c951221e044797","permalink":"https://ustc-ac.github.io/publication/dblp-confcsse-zhu-ww-08/","publishdate":"2021-06-30T12:56:16.743202Z","relpermalink":"/publication/dblp-confcsse-zhu-ww-08/","section":"publication","summary":"","tags":[],"title":"Emotional Music Generation Using Interactive Genetic Algorithm","type":"publication"},{"authors":["Shangfei Wang","Siliang Lv","Xufa Wang"],"categories":[],"content":"","date":1199145600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1625057777,"objectID":"f970ed1ec30a3f54828a7725e611c6bb","permalink":"https://ustc-ac.github.io/publication/dblp-confiscsct-wang-lw-08/","publishdate":"2021-06-30T12:56:16.959721Z","relpermalink":"/publication/dblp-confiscsct-wang-lw-08/","section":"publication","summary":"","tags":[],"title":"Infrared Facial Expression Recognition Using Wavelet Transform","type":"publication"},{"authors":["Shangfei Wang","Xufa Wang","Hideyuki Takagi"],"categories":[],"content":"","date":1136073600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1625057777,"objectID":"deec054e976ce1e1eb2f18d56eb7e71c","permalink":"https://ustc-ac.github.io/publication/dblp-confcec-wang-wt-06/","publishdate":"2021-06-30T12:56:17.171965Z","relpermalink":"/publication/dblp-confcec-wang-wt-06/","section":"publication","summary":"","tags":[],"title":"User Fatigue Reduction by an Absolute Rating Data-trained Predictor in IEC","type":"publication"},{"authors":["Shangfei Wang","Jia Xue"],"categories":[],"content":"","date":1104537600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1625057777,"objectID":"8f2c4d6f99386ab6d2d8b411ca39cbad","permalink":"https://ustc-ac.github.io/publication/dblp-confacii-wang-x-05/","publishdate":"2021-06-30T12:56:17.39284Z","relpermalink":"/publication/dblp-confacii-wang-x-05/","section":"publication","summary":"","tags":[],"title":"Case-Based Facial Action Units Recognition Using Interactive Genetic Algorithm","type":"publication"},{"authors":["Shangfei Wang","Xufa Wang"],"categories":[],"content":"","date":1104537600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1625057777,"objectID":"053080374b7c9c9c8de9be4832329d99","permalink":"https://ustc-ac.github.io/publication/dblp-confacii-wang-w-05/","publishdate":"2021-06-30T12:56:17.628967Z","relpermalink":"/publication/dblp-confacii-wang-w-05/","section":"publication","summary":"","tags":[],"title":"Emotion Semantics Image Retrieval: An Brief Overview","type":"publication"},{"authors":["Shangfei Wang","Hideyuki Takagi"],"categories":[],"content":"","date":1104537600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1625057778,"objectID":"0f2f6bb23b49c628467115bd36070106","permalink":"https://ustc-ac.github.io/publication/dblp-confwstst-wang-t-05/","publishdate":"2021-06-30T12:56:17.85971Z","relpermalink":"/publication/dblp-confwstst-wang-t-05/","section":"publication","summary":"","tags":[],"title":"Evaluation of User Fatigue Reduction Through IEC Rating-Scale Mapping","type":"publication"},{"authors":["Shangfei Wang","Enhong Chen","Jing Hu","Xufa Wang"],"categories":[],"content":"","date":978307200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1625057778,"objectID":"dede9e8abedc71fe6f22e41bac5a53be","permalink":"https://ustc-ac.github.io/publication/dblp-confamt-wang-chw-01/","publishdate":"2021-06-30T12:56:18.084253Z","relpermalink":"/publication/dblp-confamt-wang-chw-01/","section":"publication","summary":"","tags":[],"title":"Kansei-Oriented Image Retrieval","type":"publication"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":-62135596800,"objectID":"f26b5133c34eec1aa0a09390a36c2ade","permalink":"https://ustc-ac.github.io/admin/config.yml","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/admin/config.yml","section":"","summary":"","tags":null,"title":"","type":"wowchemycms"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":-62135596800,"objectID":"6d99026b9e19e4fa43d5aadf147c7176","permalink":"https://ustc-ac.github.io/contact/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/contact/","section":"","summary":"","tags":null,"title":"Contact","type":"widget_page"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":-62135596800,"objectID":"a38ba68942e5c05bd36906c9f2b71266","permalink":"https://ustc-ac.github.io/graduated/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/graduated/","section":"","summary":"","tags":null,"title":"Graduated","type":"widget_page"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":-62135596800,"objectID":"c1ef5ba2dde508442e9712f00ccfc82a","permalink":"https://ustc-ac.github.io/medic/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/medic/","section":"","summary":"","tags":null,"title":"NVIE","type":"widget_page"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":-62135596800,"objectID":"14d80a559941ebcf20a18bfe29a48aa4","permalink":"https://ustc-ac.github.io/nvie/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/nvie/","section":"","summary":"","tags":null,"title":"NVIE","type":"widget_page"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":-62135596800,"objectID":"c1d17ff2b20dca0ad6653a3161942b64","permalink":"https://ustc-ac.github.io/people/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/people/","section":"","summary":"","tags":null,"title":"People","type":"widget_page"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":-62135596800,"objectID":"f1d044c0738ab9f19347f15c290a71a1","permalink":"https://ustc-ac.github.io/research/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/research/","section":"","summary":"","tags":null,"title":"Research","type":"widget_page"}]