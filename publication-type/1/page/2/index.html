<!DOCTYPE html><html lang="en-us" >

<head><script src="/livereload.js?mindelay=10&amp;v=2&amp;port=1313&amp;path=livereload" data-no-instant defer></script>
  <meta charset="utf-8" />
  <meta name="viewport" content="width=device-width, initial-scale=1" />
  <meta name="author" content="Indigo6" src="https://github.com/Indigo6"  />
  <meta name="copyright" content="Copyright Â© Indigo6. All rights reserved."  src="https://github.com/Indigo6" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge" />
  
  
  
    <meta name="generator" content="Wowchemy 5.3.0 for Hugo" />
  

  <!--busuanzi view count -->
  <script async src="//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script>
  
  

  
  
  
  
  
    
    
    
  
  

  <meta name="author" content="Lin Fang" />

  
  
  
    
  
  <meta name="description" content="My research group description." />

  
  <link rel="alternate" hreflang="en-us" href="http://localhost:1313/publication-type/1/" />

  









  




  
  

  
  
  
    <meta name="theme-color" content="#1565c0" />
  

  
  

  
  
  
  
    
    
      <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/academicons/1.9.0/css/academicons.min.css" integrity="sha512-W4yqoT1+8NLkinBLBZko+dFB2ZbHsYLDdr50VElllRcNt2Q4/GSs6u71UHKxB7S6JEMCp5Ve4xjh3eGQl/HRvg==" crossorigin="anonymous" media="print" onload="this.media='all'">
    
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.14.0/css/all.min.css" integrity="sha256-FMvZuGapsJLjouA6k7Eo2lusoAX9i0ShlWFG6qt7SLc=" crossorigin="anonymous" media="print" onload="this.media='all'">

    
    
    
    
      
      
    
    
    

    
    
    
      
    
    

    
    
    
      <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/leaflet/1.7.1/leaflet.min.css" integrity="sha512-1xoFisiGdy9nvho8EgXuXvnpR5GAMSjFwp40gSRE3NwdUdIMIKuPa7bqoUhLD0O/5tPNhteAsE5XyyMi5reQVA==" crossorigin="anonymous" media="print" onload="this.media='all'">
    

    

    
    
      
      

      
      

      
    
      
      

      
      

      
    
      
      

      
      

      
    
      
      

      
      

      
    
      
      

      
      

      
    
      
      

      
      

      
    
      
      

      
      

      
    
      
      

      
      

      
    
      
      

      
      

      
    
      
      

      
      

      
    
      
      

      
      

      
    
      
      

      
      

      
    
      
      

      
      

      
    
      
      

      
      

      
    
      
      

      
      
        
      

      
    
      
      

      
      

      
    
      
      

      
      

      
    
  

  
  
  

  
  
  
  
  
  <link rel="stylesheet" href="/css/wowchemy.css" />

  



  

  

  




  
  
  

  
    <link rel="alternate" href="/publication-type/1/index.xml" type="application/rss+xml" title="USTC-AC" />
  

  
    <link rel="manifest" href="/manifest.webmanifest" />
  

  <link rel="icon" type="image/png" href="/media/icon_huc3da470f1a231b6fd15916059be12d5d_66382_32x32_fill_lanczos_center_3.png" />
  <link rel="apple-touch-icon" type="image/png" href="/media/icon_huc3da470f1a231b6fd15916059be12d5d_66382_180x180_fill_lanczos_center_3.png" />

  <link rel="canonical" href="http://localhost:1313/publication-type/1/" />

  
  
  
  
  
  
  
  
    
    
  
  
  <meta property="twitter:card" content="summary" />
  
  <meta property="og:site_name" content="USTC-AC" />
  <meta property="og:url" content="http://localhost:1313/publication-type/1/" />
  <meta property="og:title" content="1 | USTC-AC" />
  <meta property="og:description" content="My research group description." /><meta property="og:image" content="http://localhost:1313/media/icon_huc3da470f1a231b6fd15916059be12d5d_66382_512x512_fill_lanczos_center_3.png" />
    <meta property="twitter:image" content="http://localhost:1313/media/icon_huc3da470f1a231b6fd15916059be12d5d_66382_512x512_fill_lanczos_center_3.png" /><meta property="og:locale" content="en-us" />
  
    
      <meta property="og:updated_time" content="2024-07-25T09:53:00&#43;00:00" />
    
  

  



  

  





  <title>1 | USTC-AC</title>
</head>


<body id="top" data-spy="scroll" data-offset="70" data-target='#TableOfContents' class='page-wrapper   '  >

  
  
  
  
  
  
  
  
  
  <script src="/js/wowchemy-init.min.js"></script>

  


<aside class="search-modal" id="search">
  <div class="container">
    <section class="search-header">

      <div class="row no-gutters justify-content-between mb-3">
        <div class="col-6">
          <h1>Search</h1>
        </div>
        <div class="col-6 col-search-close">
          <a class="js-search" href="#" aria-label="Close"><i class="fas fa-times-circle text-muted" aria-hidden="true"></i></a>
        </div>
      </div>

      <div id="search-box">
        
        <input name="q" id="search-query" placeholder="Search..." autocapitalize="off"
        autocomplete="off" autocorrect="off" spellcheck="false" type="search" class="form-control"
        aria-label="Search...">
        
      </div>

      
      

      

    </section>
    <section class="section-search-results">

      <div id="search-hits">
        
      </div>

    </section>
  </div>
</aside>



  <div class="page-header">
    












<nav class="navbar navbar-expand-lg navbar-light compensate-for-scrollbar" id="navbar-main">
  <div class="container-xl">

    
    <div class="d-none d-lg-inline-flex">
      <a class="navbar-brand" href="/">USTC-AC</a>
    </div>
    

    
    <button type="button" class="navbar-toggler" data-toggle="collapse"
            data-target="#navbar-content" aria-controls="navbar-content" aria-expanded="false" aria-label="Toggle navigation">
    <span><i class="fas fa-bars"></i></span>
    </button>
    

    
    <div class="navbar-brand-mobile-wrapper d-inline-flex d-lg-none">
      <a class="navbar-brand" href="/">USTC-AC</a>
    </div>
    

    
    
    <div class="navbar-collapse main-menu-item collapse justify-content-end" id="navbar-content">

      
      <ul class="navbar-nav d-md-inline-flex">
        

        

        
        
        

        

        
        
        
        

        
          
            
            
          
          
        

        <li class="nav-item">
          <a class="nav-link " href="/post"><span>News</span></a>
        </li>

        
        

        

        
        
        
          
        

        

        
        
        
        

        
          
            
            
          
          
        

        <li class="nav-item">
          <a class="nav-link " href="/people"><span>People</span></a>
        </li>

        
        

        

        
        
        
          
        

        

        
        
        
        

        
          
            
            
          
          
        

        <li class="nav-item">
          <a class="nav-link " href="/graduated"><span>Graduated</span></a>
        </li>

        
        

        

        
        
        
          
        

        

        
        
        
        

        
          
            
            
          
          
        

        <li class="nav-item">
          <a class="nav-link " href="/research"><span>Research</span></a>
        </li>

        
        

        

        
        
        
          
        

        

        
        
        
        

        
          
            
            
          
          
        

        <li class="nav-item">
          <a class="nav-link " href="/publication"><span>Publications</span></a>
        </li>

        
        

        

        
        
        
          
        

        

        
        
        
        

        
          
            
            
          
          
        

        <li class="nav-item">
          <a class="nav-link " href="/datasets"><span>Datasets</span></a>
        </li>

        
        

        

        
        
        
          
        

        

        
        
        
        

        
          
            
            
          
          
        

        <li class="nav-item">
          <a class="nav-link " href="/contact"><span>Contact</span></a>
        </li>

        
        

      

        
      </ul>
    </div>

    <ul class="nav-icons navbar-nav flex-row ml-auto d-flex pl-md-2">

      
      

      
      
      <li class="nav-item">
        <a class="nav-link js-search" href="#" aria-label="Search"><i class="fas fa-search" aria-hidden="true"></i></a>
      </li>
      

      
      
      <li class="nav-item dropdown theme-dropdown">
        <a href="#" class="nav-link" data-toggle="dropdown" aria-haspopup="true" aria-label="Display preferences">
          <i class="fas fa-moon" aria-hidden="true"></i>
        </a>
        <div class="dropdown-menu">
          <a href="#" class="dropdown-item js-set-theme-light">
            <span>Light</span>
          </a>
          <a href="#" class="dropdown-item js-set-theme-dark">
            <span>Dark</span>
          </a>
          <a href="#" class="dropdown-item js-set-theme-auto">
            <span>Automatic</span>
          </a>
        </div>
      </li>
      

      
      

    </ul>

  </div>
</nav>


  </div>

  <div class="page-body">
    












  

  
  
  
    
  
<div class="universal-wrapper pt-3">
  <h1>1</h1>

  

  
</div>



<div class="universal-wrapper">
  

  
  
    
      







  







  


<div class="media stream-item">
  <div class="col-12 col-md-9">

    <div class="section-subheading article-title mb-0 mt-0">
      <a href="/publication/manual-fg21-liang/" >Pose-Invariant Facial Expression Recognition</a>
    </div>

    
    <a href="/publication/manual-fg21-liang/"  class="summary-link">
      <div class="article-style">
        Pose-invariant facial expression recognition is quite challenging due to variations in facial appearance and self-occlusion caused by â¦
      </div>
    </a>
    

    <div class="stream-meta article-metadata">

      

      
      <div>
        

  <span >
      <a href="/author/guang-liang/">Guang Liang</a></span>, <span >
      <a href="/author/shangfei-wang/">Shangfei Wang</a></span>, <span >
      <a href="/author/can-wang/">Can Wang</a></span>
      </div>
      
    </div>

    
    <div class="btn-links">
      








  
    
  



<a class="btn btn-outline-primary btn-page-header btn-sm" href="https://ieeexplore.ieee.org/document/9666974" target="_blank" rel="noopener">
  PDF
</a>















<a class="btn btn-outline-primary btn-page-header btn-sm" href="https://doi.org/10.1109/FG52635.2021.9666974" target="_blank" rel="noopener">
  DOI
</a>



    </div>
    

  </div>
  <div class="col-12 col-md-3">
    
    
    
    
    
    
        
        
        

        
          
          
            
          
          
        
          
          
          
        
          
          
          
        
        

        <img src="/publication/manual-fg21-liang/featured_hu5438825b9b6d1014226d20d231e650c2_1500562_9f0e50a7676ae18598d04941db82419b.jpg" srcset="/publication/manual-fg21-liang/featured_hu5438825b9b6d1014226d20d231e650c2_1500562_5dd63fa22d573c6fc213e4090ff51a29.jpg 1200w,/publication/manual-fg21-liang/featured_hu5438825b9b6d1014226d20d231e650c2_1500562_24da995eff36cbc12e9a705d7a0d3fb5.jpg 800w,/publication/manual-fg21-liang/featured_hu5438825b9b6d1014226d20d231e650c2_1500562_9f0e50a7676ae18598d04941db82419b.jpg 400w" width="450" height="253" alt="Pose-Invariant Facial Expression Recognition">

    
    
  </div>
</div>

    
  
    
      







  








<div class="media stream-item">
  <div class="col-12 col-md-9">

    <div class="section-subheading article-title mb-0 mt-0">
      <a href="/publication/manual-ijcai21-xia/" >Micro-Expression Recognition Enhanced by Macro-Expression from Spatial-Temporal Domain</a>
    </div>

    

    <div class="stream-meta article-metadata">

      

      
      <div>
        

  <span >
      <a href="/author/shangfei-wang/">Shangfei Wang</a></span>, <span >
      <a href="/author/bin-xia/">Bin Xia</a></span>
      </div>
      
    </div>

    
    <div class="btn-links">
      








  
    
  



<a class="btn btn-outline-primary btn-page-header btn-sm" href="https://www.ijcai.org/proceedings/2021/164" target="_blank" rel="noopener">
  PDF
</a>



<a href="#" class="btn btn-outline-primary btn-page-header btn-sm js-cite-modal"
        data-filename="/publication/manual-ijcai21-xia/cite.bib">
  Cite
</a>













<a class="btn btn-outline-primary btn-page-header btn-sm" href="https://doi.org/10.24963/ijcai.2021/164" target="_blank" rel="noopener">
  DOI
</a>



    </div>
    

  </div>
  <div class="col-12 col-md-3">
    
    
    
  </div>
</div>

    
  
    
      







  







  


<div class="media stream-item">
  <div class="col-12 col-md-9">

    <div class="section-subheading article-title mb-0 mt-0">
      <a href="/publication/dblp-confmm-yin-wccl-20/" >Attentive One-Dimensional Heatmap Regression for Facial Landmark Detection and Tracking</a>
    </div>

    
    <a href="/publication/dblp-confmm-yin-wccl-20/"  class="summary-link">
      <div class="article-style">
        The main contributions of the method are three folds. First, we are the first that propose to predict 1D heatmaps on the ð¥ and ð¦ axes instead of using 2D heatmaps to locate landmarks and successfully alleviate the quantization error with a fully boosted output resolution. Second, we propose a co-attention module to capture the joint coordinate distribution on the two axes. Third, based on the proposed heatmap regression method, we design a facial landmark detector and tracker which achieve state-of-the-art performance.
      </div>
    </a>
    

    <div class="stream-meta article-metadata">

      

      
      <div>
        

  <span >
      <a href="/author/shi-yin/">Shi Yin</a></span>, <span >
      <a href="/author/shangfei-wang/">Shangfei Wang</a></span>, <span >
      <a href="/author/xiaoping-chen/">Xiaoping Chen</a></span>, <span >
      <a href="/author/enhong-chen/">Enhong Chen</a></span>, <span >
      <a href="/author/cong-liang/">Cong Liang</a></span>
      </div>
      
    </div>

    
    <div class="btn-links">
      








  
    
  



<a class="btn btn-outline-primary btn-page-header btn-sm" href="https://doi.org/10.1145/3394171.3413509" target="_blank" rel="noopener">
  PDF
</a>



<a href="#" class="btn btn-outline-primary btn-page-header btn-sm js-cite-modal"
        data-filename="/publication/dblp-confmm-yin-wccl-20/cite.bib">
  Cite
</a>













<a class="btn btn-outline-primary btn-page-header btn-sm" href="https://doi.org/10.1145/3394171.3413509" target="_blank" rel="noopener">
  DOI
</a>



    </div>
    

  </div>
  <div class="col-12 col-md-3">
    
    
    
    
    
    
        
        
        

        
          
          
            
          
          
        
          
          
          
        
          
          
          
        
        

        <img src="/publication/dblp-confmm-yin-wccl-20/featured_hu22c09b2bbf7a4fd412912d6cba403c45_869902_43481045afbcec0f9844f86e4ec34cfd.png" srcset="/publication/dblp-confmm-yin-wccl-20/featured_hu22c09b2bbf7a4fd412912d6cba403c45_869902_73cb60b5af68f4246e8e8a1465554b5f.png 1200w,/publication/dblp-confmm-yin-wccl-20/featured_hu22c09b2bbf7a4fd412912d6cba403c45_869902_6a28ba0c000be97c1c35542887d58c59.png 800w,/publication/dblp-confmm-yin-wccl-20/featured_hu22c09b2bbf7a4fd412912d6cba403c45_869902_43481045afbcec0f9844f86e4ec34cfd.png 400w" width="450" height="295" alt="Attentive One-Dimensional Heatmap Regression for Facial Landmark Detection and Tracking">

    
    
  </div>
</div>

    
  
    
      







  







  


<div class="media stream-item">
  <div class="col-12 col-md-9">

    <div class="section-subheading article-title mb-0 mt-0">
      <a href="/publication/dblp-confmm-xu-ww-20/" >Exploiting Multi-Emotion Relations at Feature and Label Levels for Emotion Tagging</a>
    </div>

    
    <a href="/publication/dblp-confmm-xu-ww-20/"  class="summary-link">
      <div class="article-style">
        To address the shortcomings in emotion tagging, we consider applying emotion relationship patterns at both feature and label levels. We propose a novel emotion tagging framework, that makes full use of the emotion relationship patterns in local and global distribution.
      </div>
    </a>
    

    <div class="stream-meta article-metadata">

      

      
      <div>
        

  <span >
      <a href="/author/zhiwei-xu/">Zhiwei Xu</a></span>, <span >
      <a href="/author/shangfei-wang/">Shangfei Wang</a></span>, <span >
      <a href="/author/can-wang/">Can Wang</a></span>
      </div>
      
    </div>

    
    <div class="btn-links">
      








  
    
  



<a class="btn btn-outline-primary btn-page-header btn-sm" href="https://doi.org/10.1145/3394171.3413506" target="_blank" rel="noopener">
  PDF
</a>



<a href="#" class="btn btn-outline-primary btn-page-header btn-sm js-cite-modal"
        data-filename="/publication/dblp-confmm-xu-ww-20/cite.bib">
  Cite
</a>













<a class="btn btn-outline-primary btn-page-header btn-sm" href="https://doi.org/10.1145/3394171.3413506" target="_blank" rel="noopener">
  DOI
</a>



    </div>
    

  </div>
  <div class="col-12 col-md-3">
    
    
    
    
    
    
        
        
        

        
          
          
            
          
          
        
          
          
          
        
          
          
          
        
        

        <img src="/publication/dblp-confmm-xu-ww-20/featured_hu3761510e6e147991b8afa8272c353f87_120835_f5a9dd2198c840aadbfaa76945d277a6.jpg" srcset="/publication/dblp-confmm-xu-ww-20/featured_hu3761510e6e147991b8afa8272c353f87_120835_90ca1dac7dcf75569db8b4362e78e627.jpg 1200w,/publication/dblp-confmm-xu-ww-20/featured_hu3761510e6e147991b8afa8272c353f87_120835_4ac60dec3ee94c314dcf4d94f000cf00.jpg 800w,/publication/dblp-confmm-xu-ww-20/featured_hu3761510e6e147991b8afa8272c353f87_120835_f5a9dd2198c840aadbfaa76945d277a6.jpg 400w" width="450" height="180" alt="Exploiting Multi-Emotion Relations at Feature and Label Levels for Emotion Tagging">

    
    
  </div>
</div>

    
  
    
      







  







  


<div class="media stream-item">
  <div class="col-12 col-md-9">

    <div class="section-subheading article-title mb-0 mt-0">
      <a href="/publication/dblp-confmm-yin-wcc-20/" >Exploiting Self-Supervised and Semi-Supervised Learning for Facial Landmark Tracking with Unlabeled Data</a>
    </div>

    
    <a href="/publication/dblp-confmm-yin-wcc-20/"  class="summary-link">
      <div class="article-style">
        We propose a new semi-supervised learning strategy which trains the tracker by regression tasks from the consistency constraints on the long facial sequence instead of two adjacent frames, such that the long-term dependencies existed in a facial sequence are captured. The proposed semisupervised learning strategy does not require any extra labels. Thus, large scale unlabeled data can be exploited for training.
      </div>
    </a>
    

    <div class="stream-meta article-metadata">

      

      
      <div>
        

  <span >
      <a href="/author/shi-yin/">Shi Yin</a></span>, <span >
      <a href="/author/shangfei-wang/">Shangfei Wang</a></span>, <span >
      <a href="/author/xiaoping-chen/">Xiaoping Chen</a></span>, <span >
      <a href="/author/enhong-chen/">Enhong Chen</a></span>
      </div>
      
    </div>

    
    <div class="btn-links">
      








  
    
  



<a class="btn btn-outline-primary btn-page-header btn-sm" href="https://doi.org/10.1145/3394171.3413547" target="_blank" rel="noopener">
  PDF
</a>



<a href="#" class="btn btn-outline-primary btn-page-header btn-sm js-cite-modal"
        data-filename="/publication/dblp-confmm-yin-wcc-20/cite.bib">
  Cite
</a>













<a class="btn btn-outline-primary btn-page-header btn-sm" href="https://doi.org/10.1145/3394171.3413547" target="_blank" rel="noopener">
  DOI
</a>



    </div>
    

  </div>
  <div class="col-12 col-md-3">
    
    
    
    
    
    
        
        
        

        
          
          
            
          
          
        
          
          
          
        
          
          
          
        
        

        <img src="/publication/dblp-confmm-yin-wcc-20/featured_hub08f116c5b40a4b0bfaddc695f5ffa21_256954_5b6335b88b1442f10857a1130426c8d0.jpg" srcset="/publication/dblp-confmm-yin-wcc-20/featured_hub08f116c5b40a4b0bfaddc695f5ffa21_256954_6892f46a3751114af5ddb66c827fea35.jpg 1200w,/publication/dblp-confmm-yin-wcc-20/featured_hub08f116c5b40a4b0bfaddc695f5ffa21_256954_62adde5b198f78c5383ac56dd987565c.jpg 800w,/publication/dblp-confmm-yin-wcc-20/featured_hub08f116c5b40a4b0bfaddc695f5ffa21_256954_5b6335b88b1442f10857a1130426c8d0.jpg 400w" width="450" height="220" alt="Exploiting Self-Supervised and Semi-Supervised Learning for Facial Landmark Tracking with Unlabeled Data">

    
    
  </div>
</div>

    
  
    
      







  







  


<div class="media stream-item">
  <div class="col-12 col-md-9">

    <div class="section-subheading article-title mb-0 mt-0">
      <a href="/publication/dblp-confmm-0012-wwc-20/" >Learning from Macro-expression: a Micro-expression Recognition Framework</a>
    </div>

    
    <a href="/publication/dblp-confmm-0012-wwc-20/"  class="summary-link">
      <div class="article-style">
        In order to address problems in micro-expression recognition, we propose a micro-expression recognition framework that leverages macroexpression as guidance. Since subjects in macro-expression and micro-expression databases are different, Expression-Identity Disentangle Network (EIDNet) is introduced as feature extractor to disentangle expression-related features for expression samples.
      </div>
    </a>
    

    <div class="stream-meta article-metadata">

      

      
      <div>
        

  <span >
      <a href="/author/bin-xia/">Bin Xia</a></span>, <span >
      <a href="/author/weikang-wang/">Weikang Wang</a></span>, <span >
      <a href="/author/shangfei-wang/">Shangfei Wang</a></span>, <span >
      <a href="/author/enhong-chen/">Enhong Chen</a></span>
      </div>
      
    </div>

    
    <div class="btn-links">
      








  
    
  



<a class="btn btn-outline-primary btn-page-header btn-sm" href="https://doi.org/10.1145/3394171.3413774" target="_blank" rel="noopener">
  PDF
</a>



<a href="#" class="btn btn-outline-primary btn-page-header btn-sm js-cite-modal"
        data-filename="/publication/dblp-confmm-0012-wwc-20/cite.bib">
  Cite
</a>













<a class="btn btn-outline-primary btn-page-header btn-sm" href="https://doi.org/10.1145/3394171.3413774" target="_blank" rel="noopener">
  DOI
</a>



    </div>
    

  </div>
  <div class="col-12 col-md-3">
    
    
    
    
    
    
        
        
        

        
          
          
            
          
          
        
          
          
          
        
          
          
          
        
        

        <img src="/publication/dblp-confmm-0012-wwc-20/featured_hucbe6b30a302d6c912ad5aaa64ac11de4_85027_c1bb20ee2b7045a1967d40c977d033dc.jpg" srcset="/publication/dblp-confmm-0012-wwc-20/featured_hucbe6b30a302d6c912ad5aaa64ac11de4_85027_c4972dbde6b6e62010b30c5e61a8443a.jpg 1200w,/publication/dblp-confmm-0012-wwc-20/featured_hucbe6b30a302d6c912ad5aaa64ac11de4_85027_e7a8bd1513b3c421043883a770a0ccc9.jpg 800w,/publication/dblp-confmm-0012-wwc-20/featured_hucbe6b30a302d6c912ad5aaa64ac11de4_85027_c1bb20ee2b7045a1967d40c977d033dc.jpg 400w" width="450" height="240" alt="Learning from Macro-expression: a Micro-expression Recognition Framework">

    
    
  </div>
</div>

    
  
    
      







  







  


<div class="media stream-item">
  <div class="col-12 col-md-9">

    <div class="section-subheading article-title mb-0 mt-0">
      <a href="/publication/dblp-confmm-0012-w-20/" >Occluded Facial Expression Recognition with Step-Wise Assistance from Unpaired Non-Occluded Images</a>
    </div>

    
    <a href="/publication/dblp-confmm-0012-w-20/"  class="summary-link">
      <div class="article-style">
        To tackle the challenges in occluded facial expression recognition, we propose a step-wise learning strategy including two types of complementary adversarial learning. In this way, the occluded classifier can learn effective information from large-scale unpaired non-occluded facial images.
      </div>
    </a>
    

    <div class="stream-meta article-metadata">

      

      
      <div>
        

  <span >
      <a href="/author/bin-xia/">Bin Xia</a></span>, <span >
      <a href="/author/shangfei-wang/">Shangfei Wang</a></span>
      </div>
      
    </div>

    
    <div class="btn-links">
      








  
    
  



<a class="btn btn-outline-primary btn-page-header btn-sm" href="https://doi.org/10.1145/3394171.3413773" target="_blank" rel="noopener">
  PDF
</a>



<a href="#" class="btn btn-outline-primary btn-page-header btn-sm js-cite-modal"
        data-filename="/publication/dblp-confmm-0012-w-20/cite.bib">
  Cite
</a>













<a class="btn btn-outline-primary btn-page-header btn-sm" href="https://doi.org/10.1145/3394171.3413773" target="_blank" rel="noopener">
  DOI
</a>



    </div>
    

  </div>
  <div class="col-12 col-md-3">
    
    
    
    
    
    
        
        
        

        
          
          
            
          
          
        
          
          
          
        
          
          
          
        
        

        <img src="/publication/dblp-confmm-0012-w-20/featured_hu82f0b636e3d7f43c2e4e1586bc541f2e_109475_b8f5b691541651ba9f5a2c07b74496f2.jpg" srcset="/publication/dblp-confmm-0012-w-20/featured_hu82f0b636e3d7f43c2e4e1586bc541f2e_109475_4c4bdfd67c436f77a9d243fb599bb5e6.jpg 1200w,/publication/dblp-confmm-0012-w-20/featured_hu82f0b636e3d7f43c2e4e1586bc541f2e_109475_b6a039d3697cc2df0a5c6af04a6be7ec.jpg 800w,/publication/dblp-confmm-0012-w-20/featured_hu82f0b636e3d7f43c2e4e1586bc541f2e_109475_b8f5b691541651ba9f5a2c07b74496f2.jpg 400w" width="450" height="162" alt="Occluded Facial Expression Recognition with Step-Wise Assistance from Unpaired Non-Occluded Images">

    
    
  </div>
</div>

    
  
    
      







  







  


<div class="media stream-item">
  <div class="col-12 col-md-9">

    <div class="section-subheading article-title mb-0 mt-0">
      <a href="/publication/dblp-confaccv-0012-w-20/" >Unpaired Multimodal Facial Expression Recognition</a>
    </div>

    
    <a href="/publication/dblp-confaccv-0012-w-20/"  class="summary-link">
      <div class="article-style">
        Since collecting paired visible and thermal facial images is often difficult, requiring paired data during training prevents the usage of the many available unpaired visible and thermal images, and thus may degenerate the learning effect of the visible facial expression classifier.
To address this, we propose an unpaired adversarial facial expression recognition method. We tackle the unbalanced quantity of visible and thermal images by utilizing thermal images as privileged information. We introduce adversarial learning on the feature-level and label-level spaces to cope with unpaired training data. Finally, we add a decoder network to preserve the inherent visible features.
      </div>
    </a>
    

    <div class="stream-meta article-metadata">

      

      
      <div>
        

  <span >
      <a href="/author/bin-xia/">Bin Xia</a></span>, <span >
      <a href="/author/shangfei-wang/">Shangfei Wang</a></span>
      </div>
      
    </div>

    
    <div class="btn-links">
      








  
    
  



<a class="btn btn-outline-primary btn-page-header btn-sm" href="https://doi.org/10.1007/978-3-030-69541-5_4" target="_blank" rel="noopener">
  PDF
</a>



<a href="#" class="btn btn-outline-primary btn-page-header btn-sm js-cite-modal"
        data-filename="/publication/dblp-confaccv-0012-w-20/cite.bib">
  Cite
</a>













<a class="btn btn-outline-primary btn-page-header btn-sm" href="https://doi.org/10.1007/978-3-030-69541-5_4" target="_blank" rel="noopener">
  DOI
</a>



    </div>
    

  </div>
  <div class="col-12 col-md-3">
    
    
    
    
    
    
        
        
        

        
          
          
            
          
          
        
          
          
          
        
          
          
          
        
        

        <img src="/publication/dblp-confaccv-0012-w-20/featured_hu3699aaa94be3003441bff909318fbaba_230095_9e551947ef3bcdca637c2bb5c050fcbf.png" srcset="/publication/dblp-confaccv-0012-w-20/featured_hu3699aaa94be3003441bff909318fbaba_230095_862d72f0490051bb43a6663b92f8dd73.png 1200w,/publication/dblp-confaccv-0012-w-20/featured_hu3699aaa94be3003441bff909318fbaba_230095_f86ba04a087b624366df91a22ed0c753.png 800w,/publication/dblp-confaccv-0012-w-20/featured_hu3699aaa94be3003441bff909318fbaba_230095_9e551947ef3bcdca637c2bb5c050fcbf.png 400w" width="450" height="213" alt="Unpaired Multimodal Facial Expression Recognition">

    
    
  </div>
</div>

    
  
    
      







  







  


<div class="media stream-item">
  <div class="col-12 col-md-9">

    <div class="section-subheading article-title mb-0 mt-0">
      <a href="/publication/dblp-confijcai-yin-wpcp-19/" >Capturing Spatial and Temporal Patterns for Facial Landmark Tracking through Adversarial Learning</a>
    </div>

    
    <a href="/publication/dblp-confijcai-yin-wpcp-19/"  class="summary-link">
      <div class="article-style">
        To address the inconsistency between explicit forms of joint label distribution and the ground truth facial landmark distribution, we propose an adversarial learning framework to close the joint distribution inherent in predicted and ground truth facial landmarks.
      </div>
    </a>
    

    <div class="stream-meta article-metadata">

      

      
      <div>
        

  <span >
      <a href="/author/shi-yin/">Shi Yin</a></span>, <span >
      <a href="/author/shangfei-wang/">Shangfei Wang</a></span>, <span >
      <a href="/author/guozhu-peng/">Guozhu Peng</a></span>, <span >
      <a href="/author/xiaoping-chen/">Xiaoping Chen</a></span>, <span >
      <a href="/author/bowen-pan/">Bowen Pan</a></span>
      </div>
      
    </div>

    
    <div class="btn-links">
      








  
    
  



<a class="btn btn-outline-primary btn-page-header btn-sm" href="https://doi.org/10.24963/ijcai.2019/142" target="_blank" rel="noopener">
  PDF
</a>



<a href="#" class="btn btn-outline-primary btn-page-header btn-sm js-cite-modal"
        data-filename="/publication/dblp-confijcai-yin-wpcp-19/cite.bib">
  Cite
</a>













<a class="btn btn-outline-primary btn-page-header btn-sm" href="https://doi.org/10.24963/ijcai.2019/142" target="_blank" rel="noopener">
  DOI
</a>



    </div>
    

  </div>
  <div class="col-12 col-md-3">
    
    
    
    
    
    
        
        
        

        
          
          
            
          
          
        
          
          
          
        
          
          
          
        
        

        <img src="/publication/dblp-confijcai-yin-wpcp-19/featured_hu83191d9c4948d0db84b838d1cd056b88_246371_203eeff9d147b115ffa0366f9d391da8.jpg" srcset="/publication/dblp-confijcai-yin-wpcp-19/featured_hu83191d9c4948d0db84b838d1cd056b88_246371_047819c914bd33c4771fee5c8e595572.jpg 1200w,/publication/dblp-confijcai-yin-wpcp-19/featured_hu83191d9c4948d0db84b838d1cd056b88_246371_b251731992d8b9fa6b85e5380248108a.jpg 800w,/publication/dblp-confijcai-yin-wpcp-19/featured_hu83191d9c4948d0db84b838d1cd056b88_246371_203eeff9d147b115ffa0366f9d391da8.jpg 400w" width="450" height="159" alt="Capturing Spatial and Temporal Patterns for Facial Landmark Tracking through Adversarial Learning">

    
    
  </div>
</div>

    
  
    
      







  







  


<div class="media stream-item">
  <div class="col-12 col-md-9">

    <div class="section-subheading article-title mb-0 mt-0">
      <a href="/publication/dblp-confaaai-peng-w-19/" >Dual Semi-Supervised Learning for Facial Action Unit Recognition</a>
    </div>

    
    <a href="/publication/dblp-confaaai-peng-w-19/"  class="summary-link">
      <div class="article-style">
        Instead of minimizing the distance of two joint distributions directly, which requires the estimation of the marginal distribution of the input, the proposed approach uses an adversarial strategy to exploit the probabilistic duality, thus avoiding the estimation of marginal distribution.
      </div>
    </a>
    

    <div class="stream-meta article-metadata">

      

      
      <div>
        

  <span >
      <a href="/author/guozhu-peng/">Guozhu Peng</a></span>, <span >
      <a href="/author/shangfei-wang/">Shangfei Wang</a></span>
      </div>
      
    </div>

    
    <div class="btn-links">
      








  
    
  



<a class="btn btn-outline-primary btn-page-header btn-sm" href="https://doi.org/10.1609/aaai.v33i01.33018827" target="_blank" rel="noopener">
  PDF
</a>



<a href="#" class="btn btn-outline-primary btn-page-header btn-sm js-cite-modal"
        data-filename="/publication/dblp-confaaai-peng-w-19/cite.bib">
  Cite
</a>













<a class="btn btn-outline-primary btn-page-header btn-sm" href="https://doi.org/10.1609/aaai.v33i01.33018827" target="_blank" rel="noopener">
  DOI
</a>



    </div>
    

  </div>
  <div class="col-12 col-md-3">
    
    
    
    
    
    
        
        
        

        
          
          
            
          
          
        
          
          
          
        
          
          
          
        
        

        <img src="/publication/dblp-confaaai-peng-w-19/featured_hudd7daa1dca088bf83220ca908efacfc0_71773_089b54974db4556ba93b7e166bdddc5c.JPG" srcset="/publication/dblp-confaaai-peng-w-19/featured_hudd7daa1dca088bf83220ca908efacfc0_71773_397388136cfcbe0c20df51a10d7235aa.JPG 1200w,/publication/dblp-confaaai-peng-w-19/featured_hudd7daa1dca088bf83220ca908efacfc0_71773_8b1bb0cc537cef96719bdf0ec4a1393c.JPG 800w,/publication/dblp-confaaai-peng-w-19/featured_hudd7daa1dca088bf83220ca908efacfc0_71773_089b54974db4556ba93b7e166bdddc5c.JPG 400w" width="450" height="186" alt="Dual Semi-Supervised Learning for Facial Action Unit Recognition">

    
    
  </div>
</div>

    
  

  
<nav class="mt-1">
  <ul class="pagination justify-content-center">
    
    <li class="page-item"><a class="page-link" href="/publication-type/1/">&laquo;</a></li>
    
    
    <li class="page-item"><a class="page-link" href="/publication-type/1/page/3/">&raquo;</a></li>
    
  </ul>
</nav>


</div>
  </div>

  <div class="page-footer">
    
    
    <div class="container">
      <footer class="site-footer">

  



  <div class="powered-by d-flex flex-wrap pb-2 justify-content-center">
    <span id="busuanzi_container_site_pv">Page View:<span id="busuanzi_value_site_pv"></span></span>
  </div>
  <div class="powered-by d-flex flex-wrap pb-2 justify-content-center">
    <span id="busuanzi_container_site_uv">Unique Visitor:<span id="busuanzi_value_site_uv"></span></span>
  </div>
  

  

  

  
  






  <p class="powered-by">
    
    
    
      
      
      
      
      
      
      Published with <a href="https://wowchemy.com/?utm_campaign=poweredby" target="_blank" rel="noopener">Wowchemy</a> â the free, <a href="https://github.com/wowchemy/wowchemy-hugo-modules" target="_blank" rel="noopener">open source</a> website builder that empowers creators.
    
  </p>
</footer>

    </div>
    
  </div>

  
<div id="modal" class="modal fade" role="dialog">
  <div class="modal-dialog">
    <div class="modal-content">
      <div class="modal-header">
        <h5 class="modal-title">Cite</h5>
        <button type="button" class="close" data-dismiss="modal" aria-label="Close">
          <span aria-hidden="true">&times;</span>
        </button>
      </div>
      <div class="modal-body">
        <pre><code class="tex hljs"></code></pre>
      </div>
      <div class="modal-footer">
        <a class="btn btn-outline-primary my-1 js-copy-cite" href="#" target="_blank">
          <i class="fas fa-copy"></i> Copy
        </a>
        <a class="btn btn-outline-primary my-1 js-download-cite" href="#" target="_blank">
          <i class="fas fa-download"></i> Download
        </a>
        <div id="modal-error"></div>
      </div>
    </div>
  </div>
</div>

      

    
    
    
      <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.5.1/jquery.min.js" integrity="sha256-9/aliU8dGd2tb6OSsuzixeV4y/faTqgFtohetphbbj0=" crossorigin="anonymous"></script>
      <script src="https://cdnjs.cloudflare.com/ajax/libs/instant.page/5.1.0/instantpage.min.js" integrity="sha512-1+qUtKoh9XZW7j+6LhRMAyOrgSQKenQ4mluTR+cvxXjP1Z54RxZuzstR/H9kgPXQsVB8IW7DMDFUJpzLjvhGSQ==" crossorigin="anonymous"></script>

      
      
        <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery.imagesloaded/4.1.4/imagesloaded.pkgd.min.js" integrity="sha256-lqvxZrPLtfffUl2G/e7szqSvPBILGbwmsGE1MKlOi0Q=" crossorigin="anonymous"></script>
        <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery.isotope/3.0.6/isotope.pkgd.min.js" integrity="sha256-CBrpuqrMhXwcLLUd5tvQ4euBHCdh7wGlDfNz8vbu/iI=" crossorigin="anonymous"></script>
      

      
      

      

      

    

    
    
    
      <script src="https://cdnjs.cloudflare.com/ajax/libs/leaflet/1.7.1/leaflet.min.js" integrity="sha512-SeiQaaDh73yrb56sTW/RgVdi/mMqNeM2oBwubFHagc5BkixSpP1fvqF47mKzPGWYSSy4RwbBunrJBQ4Co8fRWA==" crossorigin="anonymous"></script>
    

    
    

    
    
    
      
      <script id="search-hit-fuse-template" type="text/x-template">
        <div class="search-hit" id="summary-{{key}}">
          <div class="search-hit-content">
            <div class="search-hit-name">
              <a href="{{relpermalink}}">{{title}}</a>
              <div class="article-metadata search-hit-type">{{type}}</div>
              <p class="search-hit-description">{{snippet}}</p>
            </div>
          </div>
        </div>
      </script>
      
        <script src="https://cdnjs.cloudflare.com/ajax/libs/fuse.js/3.2.1/fuse.min.js" integrity="sha256-VzgmKYmhsGNNN4Ph1kMW+BjoYJM2jV5i4IlFoeZA9XI=" crossorigin="anonymous"></script>
        <script src="https://cdnjs.cloudflare.com/ajax/libs/mark.js/8.11.1/jquery.mark.min.js" integrity="sha256-4HLtjeVgH0eIB3aZ9mLYF6E8oU5chNdjU6p6rrXpl9U=" crossorigin="anonymous"></script>
      
    

    
    

    
    
    
    

    
    <script src="/js/bootstrap.bundle.min.js"></script>

    
    
      
      
      
      
      
      
      
    

    
    
    
    
    
    
    
    
      
      
    
    
    <script src="/en/js/wowchemy.min.js"></script>

    






</body>
</html>
