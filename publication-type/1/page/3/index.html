<!DOCTYPE html><html lang="en-us" >

<head><script src="/livereload.js?mindelay=10&amp;v=2&amp;port=1313&amp;path=livereload" data-no-instant defer></script>
  <meta charset="utf-8" />
  <meta name="viewport" content="width=device-width, initial-scale=1" />
  <meta name="author" content="Indigo6" src="https://github.com/Indigo6"  />
  <meta name="copyright" content="Copyright © Indigo6. All rights reserved."  src="https://github.com/Indigo6" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge" />
  
  
  
    <meta name="generator" content="Wowchemy 5.3.0 for Hugo" />
  

  <!--busuanzi view count -->
  <script async src="//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script>
  
  

  
  
  
  
  
    
    
    
  
  

  <meta name="author" content="Lin Fang" />

  
  
  
    
  
  <meta name="description" content="My research group description." />

  
  <link rel="alternate" hreflang="en-us" href="http://localhost:1313/publication-type/1/" />

  









  




  
  

  
  
  
    <meta name="theme-color" content="#1565c0" />
  

  
  

  
  
  
  
    
    
      <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/academicons/1.9.0/css/academicons.min.css" integrity="sha512-W4yqoT1+8NLkinBLBZko+dFB2ZbHsYLDdr50VElllRcNt2Q4/GSs6u71UHKxB7S6JEMCp5Ve4xjh3eGQl/HRvg==" crossorigin="anonymous" media="print" onload="this.media='all'">
    
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.14.0/css/all.min.css" integrity="sha256-FMvZuGapsJLjouA6k7Eo2lusoAX9i0ShlWFG6qt7SLc=" crossorigin="anonymous" media="print" onload="this.media='all'">

    
    
    
    
      
      
    
    
    

    
    
    
      
    
    

    
    
    
      <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/leaflet/1.7.1/leaflet.min.css" integrity="sha512-1xoFisiGdy9nvho8EgXuXvnpR5GAMSjFwp40gSRE3NwdUdIMIKuPa7bqoUhLD0O/5tPNhteAsE5XyyMi5reQVA==" crossorigin="anonymous" media="print" onload="this.media='all'">
    

    

    
    
      
      

      
      

      
    
      
      

      
      

      
    
      
      

      
      

      
    
      
      

      
      

      
    
      
      

      
      

      
    
      
      

      
      

      
    
      
      

      
      

      
    
      
      

      
      

      
    
      
      

      
      

      
    
      
      

      
      

      
    
      
      

      
      

      
    
      
      

      
      

      
    
      
      

      
      

      
    
      
      

      
      

      
    
      
      

      
      
        
      

      
    
      
      

      
      

      
    
      
      

      
      

      
    
  

  
  
  

  
  
  
  
  
  <link rel="stylesheet" href="/css/wowchemy.css" />

  



  

  

  




  
  
  

  
    <link rel="alternate" href="/publication-type/1/index.xml" type="application/rss+xml" title="USTC-AC" />
  

  
    <link rel="manifest" href="/manifest.webmanifest" />
  

  <link rel="icon" type="image/png" href="/media/icon_huc3da470f1a231b6fd15916059be12d5d_66382_32x32_fill_lanczos_center_3.png" />
  <link rel="apple-touch-icon" type="image/png" href="/media/icon_huc3da470f1a231b6fd15916059be12d5d_66382_180x180_fill_lanczos_center_3.png" />

  <link rel="canonical" href="http://localhost:1313/publication-type/1/" />

  
  
  
  
  
  
  
  
    
    
  
  
  <meta property="twitter:card" content="summary" />
  
  <meta property="og:site_name" content="USTC-AC" />
  <meta property="og:url" content="http://localhost:1313/publication-type/1/" />
  <meta property="og:title" content="1 | USTC-AC" />
  <meta property="og:description" content="My research group description." /><meta property="og:image" content="http://localhost:1313/media/icon_huc3da470f1a231b6fd15916059be12d5d_66382_512x512_fill_lanczos_center_3.png" />
    <meta property="twitter:image" content="http://localhost:1313/media/icon_huc3da470f1a231b6fd15916059be12d5d_66382_512x512_fill_lanczos_center_3.png" /><meta property="og:locale" content="en-us" />
  
    
      <meta property="og:updated_time" content="2023-12-04T09:53:00&#43;00:00" />
    
  

  



  

  





  <title>1 | USTC-AC</title>
</head>


<body id="top" data-spy="scroll" data-offset="70" data-target='#TableOfContents' class='page-wrapper   '  >

  
  
  
  
  
  
  
  
  
  <script src="/js/wowchemy-init.min.js"></script>

  


<aside class="search-modal" id="search">
  <div class="container">
    <section class="search-header">

      <div class="row no-gutters justify-content-between mb-3">
        <div class="col-6">
          <h1>Search</h1>
        </div>
        <div class="col-6 col-search-close">
          <a class="js-search" href="#" aria-label="Close"><i class="fas fa-times-circle text-muted" aria-hidden="true"></i></a>
        </div>
      </div>

      <div id="search-box">
        
        <input name="q" id="search-query" placeholder="Search..." autocapitalize="off"
        autocomplete="off" autocorrect="off" spellcheck="false" type="search" class="form-control"
        aria-label="Search...">
        
      </div>

      
      

      

    </section>
    <section class="section-search-results">

      <div id="search-hits">
        
      </div>

    </section>
  </div>
</aside>



  <div class="page-header">
    












<nav class="navbar navbar-expand-lg navbar-light compensate-for-scrollbar" id="navbar-main">
  <div class="container-xl">

    
    <div class="d-none d-lg-inline-flex">
      <a class="navbar-brand" href="/">USTC-AC</a>
    </div>
    

    
    <button type="button" class="navbar-toggler" data-toggle="collapse"
            data-target="#navbar-content" aria-controls="navbar-content" aria-expanded="false" aria-label="Toggle navigation">
    <span><i class="fas fa-bars"></i></span>
    </button>
    

    
    <div class="navbar-brand-mobile-wrapper d-inline-flex d-lg-none">
      <a class="navbar-brand" href="/">USTC-AC</a>
    </div>
    

    
    
    <div class="navbar-collapse main-menu-item collapse justify-content-end" id="navbar-content">

      
      <ul class="navbar-nav d-md-inline-flex">
        

        

        
        
        

        

        
        
        
        

        
          
            
            
          
          
        

        <li class="nav-item">
          <a class="nav-link " href="/post"><span>News</span></a>
        </li>

        
        

        

        
        
        
          
        

        

        
        
        
        

        
          
            
            
          
          
        

        <li class="nav-item">
          <a class="nav-link " href="/people"><span>People</span></a>
        </li>

        
        

        

        
        
        
          
        

        

        
        
        
        

        
          
            
            
          
          
        

        <li class="nav-item">
          <a class="nav-link " href="/graduated"><span>Graduated</span></a>
        </li>

        
        

        

        
        
        
          
        

        

        
        
        
        

        
          
            
            
          
          
        

        <li class="nav-item">
          <a class="nav-link " href="/research"><span>Research</span></a>
        </li>

        
        

        

        
        
        
          
        

        

        
        
        
        

        
          
            
            
          
          
        

        <li class="nav-item">
          <a class="nav-link " href="/publication"><span>Publications</span></a>
        </li>

        
        

        

        
        
        
          
        

        

        
        
        
        

        
          
            
            
          
          
        

        <li class="nav-item">
          <a class="nav-link " href="/datasets"><span>Datasets</span></a>
        </li>

        
        

        

        
        
        
          
        

        

        
        
        
        

        
          
            
            
          
          
        

        <li class="nav-item">
          <a class="nav-link " href="/contact"><span>Contact</span></a>
        </li>

        
        

      

        
      </ul>
    </div>

    <ul class="nav-icons navbar-nav flex-row ml-auto d-flex pl-md-2">

      
      

      
      
      <li class="nav-item">
        <a class="nav-link js-search" href="#" aria-label="Search"><i class="fas fa-search" aria-hidden="true"></i></a>
      </li>
      

      
      
      <li class="nav-item dropdown theme-dropdown">
        <a href="#" class="nav-link" data-toggle="dropdown" aria-haspopup="true" aria-label="Display preferences">
          <i class="fas fa-moon" aria-hidden="true"></i>
        </a>
        <div class="dropdown-menu">
          <a href="#" class="dropdown-item js-set-theme-light">
            <span>Light</span>
          </a>
          <a href="#" class="dropdown-item js-set-theme-dark">
            <span>Dark</span>
          </a>
          <a href="#" class="dropdown-item js-set-theme-auto">
            <span>Automatic</span>
          </a>
        </div>
      </li>
      

      
      

    </ul>

  </div>
</nav>


  </div>

  <div class="page-body">
    












  

  
  
  
    
  
<div class="universal-wrapper pt-3">
  <h1>1</h1>

  

  
</div>



<div class="universal-wrapper">
  

  
  
    
      







  







  


<div class="media stream-item">
  <div class="col-12 col-md-9">

    <div class="section-subheading article-title mb-0 mt-0">
      <a href="/publication/dblp-confaaai-pan-wj-19/" >Image Aesthetic Assessment Assisted by Attributes through Adversarial Learning</a>
    </div>

    
    <a href="/publication/dblp-confaaai-pan-wj-19/"  class="summary-link">
      <div class="article-style">
        In this paper, we propose a novel attributes enhanced image aesthetic assessment, where the attributes are used as privileged information.
      </div>
    </a>
    

    <div class="stream-meta article-metadata">

      

      
      <div>
        

  <span >
      <a href="/author/bowen-pan/">Bowen Pan</a></span>, <span >
      <a href="/author/shangfei-wang/">Shangfei Wang</a></span>, <span >
      <a href="/author/qisheng-jiang/">Qisheng Jiang</a></span>
      </div>
      
    </div>

    
    <div class="btn-links">
      








  
    
  



<a class="btn btn-outline-primary btn-page-header btn-sm" href="https://doi.org/10.1609/aaai.v33i01.3301679" target="_blank" rel="noopener">
  PDF
</a>



<a href="#" class="btn btn-outline-primary btn-page-header btn-sm js-cite-modal"
        data-filename="/publication/dblp-confaaai-pan-wj-19/cite.bib">
  Cite
</a>













<a class="btn btn-outline-primary btn-page-header btn-sm" href="https://doi.org/10.1609/aaai.v33i01.3301679" target="_blank" rel="noopener">
  DOI
</a>



    </div>
    

  </div>
  <div class="col-12 col-md-3">
    
    
    
    
    
    
        
        
        

        
          
          
            
          
          
        
          
          
          
        
          
          
          
        
        

        <img src="/publication/dblp-confaaai-pan-wj-19/featured_hu2a714477b00e9d9445d21b555b800bff_206782_1aa0903c7926b51abb2e357eb4a283a7.PNG" srcset="/publication/dblp-confaaai-pan-wj-19/featured_hu2a714477b00e9d9445d21b555b800bff_206782_169ca3d33492725134ceb088215de3d6.PNG 1200w,/publication/dblp-confaaai-pan-wj-19/featured_hu2a714477b00e9d9445d21b555b800bff_206782_6d7d94f873330971c33927e75c9f8df4.PNG 800w,/publication/dblp-confaaai-pan-wj-19/featured_hu2a714477b00e9d9445d21b555b800bff_206782_1aa0903c7926b51abb2e357eb4a283a7.PNG 400w" width="450" height="281" alt="Image Aesthetic Assessment Assisted by Attributes through Adversarial Learning">

    
    
  </div>
</div>

    
  
    
      







  







  


<div class="media stream-item">
  <div class="col-12 col-md-9">

    <div class="section-subheading article-title mb-0 mt-0">
      <a href="/publication/dblp-conffgr-yin-fwwdw-19/" >Integrating Facial Images, Speeches and Time for Empathy Prediction</a>
    </div>

    
    <a href="/publication/dblp-conffgr-yin-fwwdw-19/"  class="summary-link">
      <div class="article-style">
        In this paper, we propose a multi-modal deep network to predict the empathy of the listener during the conversation between two people. First, we use a bottleneck residual network proposed by to learn visual representation from facial images, and adopt fully connected network to extract audio features from the listener’s speech. Second, we propose to use the current time stage as a temporal feature, and fuse it with the learned visual and audio representations. Neural network regression is used to predict the empathy level. We further select the representative subset training data to train the proposed multi-modal deep network. Experimental results on the One-Minute Empathy Prediction dataset demonstrate the effectiveness of the proposed method.
      </div>
    </a>
    

    <div class="stream-meta article-metadata">

      

      
      <div>
        

  <span >
      <a href="/author/shi-yin/">Shi Yin</a></span>, <span >
      <a href="/author/yonggan-fu/">Yonggan Fu</a></span>, <span >
      <a href="/author/can-wang/">Can Wang</a></span>, <span >
      <a href="/author/runlong-wu/">Runlong Wu</a></span>, <span >
      <a href="/author/heyan-ding/">Heyan Ding</a></span>, <span >
      <a href="/author/shangfei-wang/">Shangfei Wang</a></span>
      </div>
      
    </div>

    
    <div class="btn-links">
      








  
    
  



<a class="btn btn-outline-primary btn-page-header btn-sm" href="https://doi.org/10.1109/FG.2019.8756621" target="_blank" rel="noopener">
  PDF
</a>



<a href="#" class="btn btn-outline-primary btn-page-header btn-sm js-cite-modal"
        data-filename="/publication/dblp-conffgr-yin-fwwdw-19/cite.bib">
  Cite
</a>













<a class="btn btn-outline-primary btn-page-header btn-sm" href="https://doi.org/10.1109/FG.2019.8756621" target="_blank" rel="noopener">
  DOI
</a>



    </div>
    

  </div>
  <div class="col-12 col-md-3">
    
    
    
    
    
    
        
        
        

        
          
          
            
          
          
        
          
          
          
        
          
          
          
        
        

        <img src="/publication/dblp-conffgr-yin-fwwdw-19/featured_hu2c69917c91855ca124047f9f7218696c_145547_73b16d004db21e231b2d143ae054666b.jpg" srcset="/publication/dblp-conffgr-yin-fwwdw-19/featured_hu2c69917c91855ca124047f9f7218696c_145547_8f86281a72809879b36ac3dfd090be20.jpg 1200w,/publication/dblp-conffgr-yin-fwwdw-19/featured_hu2c69917c91855ca124047f9f7218696c_145547_6c81e6c620ad74ab22f9bf8e33a2d5e3.jpg 800w,/publication/dblp-conffgr-yin-fwwdw-19/featured_hu2c69917c91855ca124047f9f7218696c_145547_73b16d004db21e231b2d143ae054666b.jpg 400w" width="450" height="257" alt="Integrating Facial Images, Speeches and Time for Empathy Prediction">

    
    
  </div>
</div>

    
  
    
      







  








<div class="media stream-item">
  <div class="col-12 col-md-9">

    <div class="section-subheading article-title mb-0 mt-0">
      <a href="/publication/dblp-confijcnn-yin-zlwjcw-19/" >KDSL: a Knowledge-Driven Supervised Learning Framework for Word Sense Disambiguation</a>
    </div>

    

    <div class="stream-meta article-metadata">

      

      
      <div>
        

  <span >
      <a href="/author/shi-yin/">Shi Yin</a></span>, <span >
      <a href="/author/yi-zhou/">Yi Zhou</a></span>, <span >
      <a href="/author/chenguang-li/">Chenguang Li</a></span>, <span >
      <a href="/author/shangfei-wang/">Shangfei Wang</a></span>, <span >
      <a href="/author/jianmin-ji/">Jianmin Ji</a></span>, <span >
      <a href="/author/xiaoping-chen/">Xiaoping Chen</a></span>, <span >
      <a href="/author/ruili-wang/">Ruili Wang</a></span>
      </div>
      
    </div>

    
    <div class="btn-links">
      








  
    
  



<a class="btn btn-outline-primary btn-page-header btn-sm" href="https://doi.org/10.1109/IJCNN.2019.8851718" target="_blank" rel="noopener">
  PDF
</a>



<a href="#" class="btn btn-outline-primary btn-page-header btn-sm js-cite-modal"
        data-filename="/publication/dblp-confijcnn-yin-zlwjcw-19/cite.bib">
  Cite
</a>













<a class="btn btn-outline-primary btn-page-header btn-sm" href="https://doi.org/10.1109/IJCNN.2019.8851718" target="_blank" rel="noopener">
  DOI
</a>



    </div>
    

  </div>
  <div class="col-12 col-md-3">
    
    
    
  </div>
</div>

    
  
    
      







  







  


<div class="media stream-item">
  <div class="col-12 col-md-9">

    <div class="section-subheading article-title mb-0 mt-0">
      <a href="/publication/dblp-confmm-pan-wx-19/" >Occluded Facial Expression Recognition Enhanced through Privileged Information</a>
    </div>

    
    <a href="/publication/dblp-confmm-pan-wx-19/"  class="summary-link">
      <div class="article-style">
        we propose using non-occluded facial images as privileged information to assist the learning process of the occluded view. Specifically, two deep neural networks are first trained from occluded and non-occluded images respectively.
      </div>
    </a>
    

    <div class="stream-meta article-metadata">

      

      
      <div>
        

  <span >
      <a href="/author/bowen-pan/">Bowen Pan</a></span>, <span >
      <a href="/author/shangfei-wang/">Shangfei Wang</a></span>, <span >
      <a href="/author/bin-xia/">Bin Xia</a></span>
      </div>
      
    </div>

    
    <div class="btn-links">
      








  
    
  



<a class="btn btn-outline-primary btn-page-header btn-sm" href="https://doi.org/10.1145/3343031.3351049" target="_blank" rel="noopener">
  PDF
</a>



<a href="#" class="btn btn-outline-primary btn-page-header btn-sm js-cite-modal"
        data-filename="/publication/dblp-confmm-pan-wx-19/cite.bib">
  Cite
</a>













<a class="btn btn-outline-primary btn-page-header btn-sm" href="https://doi.org/10.1145/3343031.3351049" target="_blank" rel="noopener">
  DOI
</a>



    </div>
    

  </div>
  <div class="col-12 col-md-3">
    
    
    
    
    
    
        
        
        

        
          
          
            
          
          
        
          
          
          
        
          
          
          
        
        

        <img src="/publication/dblp-confmm-pan-wx-19/featured_hucf65a7887d8ff00a6d7a1a8fcf59d04f_218950_b4a29631e522b4e6470db369efa91f8a.png" srcset="/publication/dblp-confmm-pan-wx-19/featured_hucf65a7887d8ff00a6d7a1a8fcf59d04f_218950_a4606a5b5320fd2d1b6d846834d0b4ed.png 1200w,/publication/dblp-confmm-pan-wx-19/featured_hucf65a7887d8ff00a6d7a1a8fcf59d04f_218950_c93c12d80b71b21a0891b5f7d12e4869.png 800w,/publication/dblp-confmm-pan-wx-19/featured_hucf65a7887d8ff00a6d7a1a8fcf59d04f_218950_b4a29631e522b4e6470db369efa91f8a.png 400w" width="450" height="291" alt="Occluded Facial Expression Recognition Enhanced through Privileged Information">

    
    
  </div>
</div>

    
  
    
      







  







  


<div class="media stream-item">
  <div class="col-12 col-md-9">

    <div class="section-subheading article-title mb-0 mt-0">
      <a href="/publication/dblp-conffgr-hao-wpj-18/" >Facial Action Unit Recognition Augmented by Their Dependencies</a>
    </div>

    
    <a href="/publication/dblp-conffgr-hao-wpj-18/"  class="summary-link">
      <div class="article-style">
        We propose employing the latent regression Bayesian network to effectively capture the high-order and global dependencies among AUs.
      </div>
    </a>
    

    <div class="stream-meta article-metadata">

      

      
      <div>
        

  <span >
      <a href="/author/longfei-hao/">Longfei Hao</a></span>, <span >
      <a href="/author/shangfei-wang/">Shangfei Wang</a></span>, <span >
      <a href="/author/guozhu-peng/">Guozhu Peng</a></span>, <span >
      <a href="/author/qiang-ji/">Qiang Ji</a></span>
      </div>
      
    </div>

    
    <div class="btn-links">
      








  
    
  



<a class="btn btn-outline-primary btn-page-header btn-sm" href="https://doi.org/10.1109/FG.2018.00036" target="_blank" rel="noopener">
  PDF
</a>



<a href="#" class="btn btn-outline-primary btn-page-header btn-sm js-cite-modal"
        data-filename="/publication/dblp-conffgr-hao-wpj-18/cite.bib">
  Cite
</a>













<a class="btn btn-outline-primary btn-page-header btn-sm" href="https://doi.org/10.1109/FG.2018.00036" target="_blank" rel="noopener">
  DOI
</a>



    </div>
    

  </div>
  <div class="col-12 col-md-3">
    
    
    
    
    
    
        
        
        

        
          
          
            
          
          
        
          
          
          
        
          
          
          
        
        

        <img src="/publication/dblp-conffgr-hao-wpj-18/featured_hu186a5b8e8dbef9b8cf16ed6628856e29_401689_4e5aed58f4f12f680ea9f495b9deccb7.jpg" srcset="/publication/dblp-conffgr-hao-wpj-18/featured_hu186a5b8e8dbef9b8cf16ed6628856e29_401689_851a8c69bd5eca99c456bfbc95d65496.jpg 1200w,/publication/dblp-conffgr-hao-wpj-18/featured_hu186a5b8e8dbef9b8cf16ed6628856e29_401689_33a134ae6a03059b8c7d8b37bda51b22.jpg 800w,/publication/dblp-conffgr-hao-wpj-18/featured_hu186a5b8e8dbef9b8cf16ed6628856e29_401689_4e5aed58f4f12f680ea9f495b9deccb7.jpg 400w" width="450" height="225" alt="Facial Action Unit Recognition Augmented by Their Dependencies">

    
    
  </div>
</div>

    
  
    
      







  







  


<div class="media stream-item">
  <div class="col-12 col-md-9">

    <div class="section-subheading article-title mb-0 mt-0">
      <a href="/publication/dblp-confmm-pan-w-18/" >Facial Expression Recognition Enhanced by Thermal Images through Adversarial Learning</a>
    </div>

    
    <a href="/publication/dblp-confmm-pan-w-18/"  class="summary-link">
      <div class="article-style">
        In this paper, we propose a novel facial expression recognition method enhanced by thermal images. Our method leverages thermal images to construct better visible feature representation and classifiers during training through adversarial learning and similarity constraints. Specifically, we learn two deep neural networks for expression classification from visible and thermal images.
      </div>
    </a>
    

    <div class="stream-meta article-metadata">

      

      
      <div>
        

  <span >
      <a href="/author/bowen-pan/">Bowen Pan</a></span>, <span >
      <a href="/author/shangfei-wang/">Shangfei Wang</a></span>
      </div>
      
    </div>

    
    <div class="btn-links">
      








  
    
  



<a class="btn btn-outline-primary btn-page-header btn-sm" href="https://doi.org/10.1145/3240508.3240608" target="_blank" rel="noopener">
  PDF
</a>



<a href="#" class="btn btn-outline-primary btn-page-header btn-sm js-cite-modal"
        data-filename="/publication/dblp-confmm-pan-w-18/cite.bib">
  Cite
</a>













<a class="btn btn-outline-primary btn-page-header btn-sm" href="https://doi.org/10.1145/3240508.3240608" target="_blank" rel="noopener">
  DOI
</a>



    </div>
    

  </div>
  <div class="col-12 col-md-3">
    
    
    
    
    
    
        
        
        

        
          
          
            
          
          
        
          
          
          
        
          
          
          
        
        

        <img src="/publication/dblp-confmm-pan-w-18/featured_hubbc45ea9bf79dd9eb975f82036f90e3d_46195_1f791eea86e50dfa6e30ca0216207fee.jpg" srcset="/publication/dblp-confmm-pan-w-18/featured_hubbc45ea9bf79dd9eb975f82036f90e3d_46195_b0d24c5046306cf24ec3e7b1927afd38.jpg 1200w,/publication/dblp-confmm-pan-w-18/featured_hubbc45ea9bf79dd9eb975f82036f90e3d_46195_0e1a6fe593d50f19eb6dde8bc2cbfc8c.jpg 800w,/publication/dblp-confmm-pan-w-18/featured_hubbc45ea9bf79dd9eb975f82036f90e3d_46195_1f791eea86e50dfa6e30ca0216207fee.jpg 400w" width="450" height="293" alt="Facial Expression Recognition Enhanced by Thermal Images through Adversarial Learning">

    
    
  </div>
</div>

    
  
    
      







  







  


<div class="media stream-item">
  <div class="col-12 col-md-9">

    <div class="section-subheading article-title mb-0 mt-0">
      <a href="/publication/dblp-confmm-wang-w-18/" >Personalized Multiple Facial Action Unit Recognition through Generative Adversarial Recognition Network</a>
    </div>

    
    <a href="/publication/dblp-confmm-wang-w-18/"  class="summary-link">
      <div class="article-style">
        we propose a novel generative adversarial recognition network (GARN) for personalized AU recognition without any assumptions. Specifically, the proposed GARN consists of a generator, a discriminator, and a classifier.
      </div>
    </a>
    

    <div class="stream-meta article-metadata">

      

      
      <div>
        

  <span >
      <a href="/author/can-wang/">Can Wang</a></span>, <span >
      <a href="/author/shangfei-wang/">Shangfei Wang</a></span>
      </div>
      
    </div>

    
    <div class="btn-links">
      








  
    
  



<a class="btn btn-outline-primary btn-page-header btn-sm" href="https://doi.org/10.1145/3240508.3240613" target="_blank" rel="noopener">
  PDF
</a>



<a href="#" class="btn btn-outline-primary btn-page-header btn-sm js-cite-modal"
        data-filename="/publication/dblp-confmm-wang-w-18/cite.bib">
  Cite
</a>













<a class="btn btn-outline-primary btn-page-header btn-sm" href="https://doi.org/10.1145/3240508.3240613" target="_blank" rel="noopener">
  DOI
</a>



    </div>
    

  </div>
  <div class="col-12 col-md-3">
    
    
    
    
    
    
        
        
        

        
          
          
            
          
          
        
          
          
          
        
          
          
          
        
        

        <img src="/publication/dblp-confmm-wang-w-18/featured_hu2b619861a230c980126dcce3eefea7b7_166103_b0154ae2cdcb7222c38c08ab743fd4b8.png" srcset="/publication/dblp-confmm-wang-w-18/featured_hu2b619861a230c980126dcce3eefea7b7_166103_588c971da9bdb565ab7ef81a180df4f4.png 1200w,/publication/dblp-confmm-wang-w-18/featured_hu2b619861a230c980126dcce3eefea7b7_166103_6d0772433fa9c175cfbc853d189ccf16.png 800w,/publication/dblp-confmm-wang-w-18/featured_hu2b619861a230c980126dcce3eefea7b7_166103_b0154ae2cdcb7222c38c08ab743fd4b8.png 400w" width="450" height="196" alt="Personalized Multiple Facial Action Unit Recognition through Generative Adversarial Recognition Network">

    
    
  </div>
</div>

    
  
    
      







  







  


<div class="media stream-item">
  <div class="col-12 col-md-9">

    <div class="section-subheading article-title mb-0 mt-0">
      <a href="/publication/dblp-confcvpr-peng-w-18/" >Weakly Supervised Facial Action Unit Recognition Through Adversarial Training</a>
    </div>

    
    <a href="/publication/dblp-confcvpr-peng-w-18/"  class="summary-link">
      <div class="article-style">
        We propose a novel weakly supervised AU recognition method to learn AU classifiers with only expression labels. Specifically, we notice that there exist domain knowledge about expressions and AUs that can be represented as prior probabilities. We generate pseudo AU data for each expression; for AU classifiers’ training, we propose an RAN model, which consists of a recognition model and a discrimination mode trained simultaneously by leveraging an adversarial process, to make the distribution of the recognized AU close to the distribution of the pseudo AU data. Furthermore, we extend the proposed method to semi-supervised learning with partially AU-annotated images.
      </div>
    </a>
    

    <div class="stream-meta article-metadata">

      

      
      <div>
        

  <span >
      <a href="/author/guozhu-peng/">Guozhu Peng</a></span>, <span >
      <a href="/author/shangfei-wang/">Shangfei Wang</a></span>
      </div>
      
    </div>

    
    <div class="btn-links">
      








  
    
  



<a class="btn btn-outline-primary btn-page-header btn-sm" href="http://openaccess.thecvf.com/content_cvpr_2018/html/Peng_Weakly_Supervised_Facial_CVPR_2018_paper.html" target="_blank" rel="noopener">
  PDF
</a>



<a href="#" class="btn btn-outline-primary btn-page-header btn-sm js-cite-modal"
        data-filename="/publication/dblp-confcvpr-peng-w-18/cite.bib">
  Cite
</a>













<a class="btn btn-outline-primary btn-page-header btn-sm" href="https://doi.org/10.1109/CVPR.2018.00233" target="_blank" rel="noopener">
  DOI
</a>



    </div>
    

  </div>
  <div class="col-12 col-md-3">
    
    
    
    
    
    
        
        
        

        
          
          
            
          
          
        
          
          
          
        
          
          
          
        
        

        <img src="/publication/dblp-confcvpr-peng-w-18/featured_hu1005fd87fac0ca886d0287f946d2eb97_137194_e1237ffd5f8876a8ef2a7432ef0f0626.jpg" srcset="/publication/dblp-confcvpr-peng-w-18/featured_hu1005fd87fac0ca886d0287f946d2eb97_137194_eb845323703deb37263a01cdd2794b56.jpg 1200w,/publication/dblp-confcvpr-peng-w-18/featured_hu1005fd87fac0ca886d0287f946d2eb97_137194_e8a295a412a8af7dea13b83b886c461c.jpg 800w,/publication/dblp-confcvpr-peng-w-18/featured_hu1005fd87fac0ca886d0287f946d2eb97_137194_e1237ffd5f8876a8ef2a7432ef0f0626.jpg 400w" width="450" height="224" alt="Weakly Supervised Facial Action Unit Recognition Through Adversarial Training">

    
    
  </div>
</div>

    
  
    
      







  







  


<div class="media stream-item">
  <div class="col-12 col-md-9">

    <div class="section-subheading article-title mb-0 mt-0">
      <a href="/publication/dblp-conficcv-gan-whj-17/" >A Multimodal Deep Regression Bayesian Network for Affective Video Content Analyses</a>
    </div>

    
    <a href="/publication/dblp-conficcv-gan-whj-17/"  class="summary-link">
      <div class="article-style">
        In this paper, we propose a new multimodal learning method, multimodal deep regression Bayesian network (MMDRBN), to construct the high-level joint representation of visual and audio modalities for emotion tagging. Then the MMDRBN is transformed into an inference network by minimizing the KL-divergence. After that, the inference network is used to predict discrete or continuous affective scores from video content.
      </div>
    </a>
    

    <div class="stream-meta article-metadata">

      

      
      <div>
        

  <span >
      <a href="/author/quan-gan/">Quan Gan</a></span>, <span >
      <a href="/author/shangfei-wang/">Shangfei Wang</a></span>, <span >
      <a href="/author/longfei-hao/">Longfei Hao</a></span>, <span >
      <a href="/author/qiang-ji/">Qiang Ji</a></span>
      </div>
      
    </div>

    
    <div class="btn-links">
      








  
    
  



<a class="btn btn-outline-primary btn-page-header btn-sm" href="https://doi.org/10.1109/ICCV.2017.547" target="_blank" rel="noopener">
  PDF
</a>



<a href="#" class="btn btn-outline-primary btn-page-header btn-sm js-cite-modal"
        data-filename="/publication/dblp-conficcv-gan-whj-17/cite.bib">
  Cite
</a>













<a class="btn btn-outline-primary btn-page-header btn-sm" href="https://doi.org/10.1109/ICCV.2017.547" target="_blank" rel="noopener">
  DOI
</a>



    </div>
    

  </div>
  <div class="col-12 col-md-3">
    
    
    
    
    
    
        
        
        

        
          
          
            
          
          
        
          
          
          
        
          
          
          
        
        

        <img src="/publication/dblp-conficcv-gan-whj-17/featured_hu0ff0903a4051f103a27ffc3b5d24d0b2_144232_73833da6a7494787d2806f1758247750.jpg" srcset="/publication/dblp-conficcv-gan-whj-17/featured_hu0ff0903a4051f103a27ffc3b5d24d0b2_144232_751564fd9d32dff94ee9db54beb8d778.jpg 1200w,/publication/dblp-conficcv-gan-whj-17/featured_hu0ff0903a4051f103a27ffc3b5d24d0b2_144232_8690e66d2d92c552193cfe97d4866ff2.jpg 800w,/publication/dblp-conficcv-gan-whj-17/featured_hu0ff0903a4051f103a27ffc3b5d24d0b2_144232_73833da6a7494787d2806f1758247750.jpg 400w" width="450" height="243" alt="A Multimodal Deep Regression Bayesian Network for Affective Video Content Analyses">

    
    
  </div>
</div>

    
  
    
      







  







  


<div class="media stream-item">
  <div class="col-12 col-md-9">

    <div class="section-subheading article-title mb-0 mt-0">
      <a href="/publication/dblp-confaaai-wu-wj-17/" >Capturing Dependencies among Labels and Features for Multiple Emotion Tagging of Multimedia Data</a>
    </div>

    
    <a href="/publication/dblp-confaaai-wu-wj-17/"  class="summary-link">
      <div class="article-style">
        To the best of our knowledge, this paper is the first work to assign multiple emotions to multimedia data by exploring the emotional relationships at both feature and label levels. By learning the shared features with a multi-task RBM classifier and modeling the dependencies among emotion labels with a hierarchy RBM model, the proposed approaches can exploit both top-down and bottom-up relations among emotions independently and dependently to improve multiple emotions tagging for multimedia.
      </div>
    </a>
    

    <div class="stream-meta article-metadata">

      

      
      <div>
        

  <span >
      <a href="/author/shan-wu/">Shan Wu</a></span>, <span >
      <a href="/author/shangfei-wang/">Shangfei Wang</a></span>, <span >
      <a href="/author/qiang-ji/">Qiang Ji</a></span>
      </div>
      
    </div>

    
    <div class="btn-links">
      








  
    
  



<a class="btn btn-outline-primary btn-page-header btn-sm" href="http://aaai.org/ocs/index.php/AAAI/AAAI17/paper/view/14672" target="_blank" rel="noopener">
  PDF
</a>



<a href="#" class="btn btn-outline-primary btn-page-header btn-sm js-cite-modal"
        data-filename="/publication/dblp-confaaai-wu-wj-17/cite.bib">
  Cite
</a>















    </div>
    

  </div>
  <div class="col-12 col-md-3">
    
    
    
    
    
    
        
        
        

        
          
          
            
          
          
        
          
          
          
        
          
          
          
        
        

        <img src="/publication/dblp-confaaai-wu-wj-17/featured_hu703fb9dff318081c89de1de4167dd036_98867_fc2719919d1bcc59ee45e8bbe3b7b669.JPG" srcset="/publication/dblp-confaaai-wu-wj-17/featured_hu703fb9dff318081c89de1de4167dd036_98867_0710191a3a1426d0908c53e974bcce47.JPG 1200w,/publication/dblp-confaaai-wu-wj-17/featured_hu703fb9dff318081c89de1de4167dd036_98867_c2707fbad111d55f29a208d05bfe4a4f.JPG 800w,/publication/dblp-confaaai-wu-wj-17/featured_hu703fb9dff318081c89de1de4167dd036_98867_fc2719919d1bcc59ee45e8bbe3b7b669.JPG 400w" width="450" height="260" alt="Capturing Dependencies among Labels and Features for Multiple Emotion Tagging of Multimedia Data">

    
    
  </div>
</div>

    
  

  
<nav class="mt-1">
  <ul class="pagination justify-content-center">
    
    <li class="page-item"><a class="page-link" href="/publication-type/1/page/2/">&laquo;</a></li>
    
    
    <li class="page-item"><a class="page-link" href="/publication-type/1/page/4/">&raquo;</a></li>
    
  </ul>
</nav>


</div>
  </div>

  <div class="page-footer">
    
    
    <div class="container">
      <footer class="site-footer">

  



  <div class="powered-by d-flex flex-wrap pb-2 justify-content-center">
    <span id="busuanzi_container_site_pv">Page View:<span id="busuanzi_value_site_pv"></span></span>
  </div>
  <div class="powered-by d-flex flex-wrap pb-2 justify-content-center">
    <span id="busuanzi_container_site_uv">Unique Visitor:<span id="busuanzi_value_site_uv"></span></span>
  </div>
  

  

  

  
  






  <p class="powered-by">
    
    
    
      
      
      
      
      
      
      Published with <a href="https://wowchemy.com/?utm_campaign=poweredby" target="_blank" rel="noopener">Wowchemy</a> — the free, <a href="https://github.com/wowchemy/wowchemy-hugo-modules" target="_blank" rel="noopener">open source</a> website builder that empowers creators.
    
  </p>
</footer>

    </div>
    
  </div>

  
<div id="modal" class="modal fade" role="dialog">
  <div class="modal-dialog">
    <div class="modal-content">
      <div class="modal-header">
        <h5 class="modal-title">Cite</h5>
        <button type="button" class="close" data-dismiss="modal" aria-label="Close">
          <span aria-hidden="true">&times;</span>
        </button>
      </div>
      <div class="modal-body">
        <pre><code class="tex hljs"></code></pre>
      </div>
      <div class="modal-footer">
        <a class="btn btn-outline-primary my-1 js-copy-cite" href="#" target="_blank">
          <i class="fas fa-copy"></i> Copy
        </a>
        <a class="btn btn-outline-primary my-1 js-download-cite" href="#" target="_blank">
          <i class="fas fa-download"></i> Download
        </a>
        <div id="modal-error"></div>
      </div>
    </div>
  </div>
</div>

      

    
    
    
      <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.5.1/jquery.min.js" integrity="sha256-9/aliU8dGd2tb6OSsuzixeV4y/faTqgFtohetphbbj0=" crossorigin="anonymous"></script>
      <script src="https://cdnjs.cloudflare.com/ajax/libs/instant.page/5.1.0/instantpage.min.js" integrity="sha512-1+qUtKoh9XZW7j+6LhRMAyOrgSQKenQ4mluTR+cvxXjP1Z54RxZuzstR/H9kgPXQsVB8IW7DMDFUJpzLjvhGSQ==" crossorigin="anonymous"></script>

      
      
        <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery.imagesloaded/4.1.4/imagesloaded.pkgd.min.js" integrity="sha256-lqvxZrPLtfffUl2G/e7szqSvPBILGbwmsGE1MKlOi0Q=" crossorigin="anonymous"></script>
        <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery.isotope/3.0.6/isotope.pkgd.min.js" integrity="sha256-CBrpuqrMhXwcLLUd5tvQ4euBHCdh7wGlDfNz8vbu/iI=" crossorigin="anonymous"></script>
      

      
      

      

      

    

    
    
    
      <script src="https://cdnjs.cloudflare.com/ajax/libs/leaflet/1.7.1/leaflet.min.js" integrity="sha512-SeiQaaDh73yrb56sTW/RgVdi/mMqNeM2oBwubFHagc5BkixSpP1fvqF47mKzPGWYSSy4RwbBunrJBQ4Co8fRWA==" crossorigin="anonymous"></script>
    

    
    

    
    
    
      
      <script id="search-hit-fuse-template" type="text/x-template">
        <div class="search-hit" id="summary-{{key}}">
          <div class="search-hit-content">
            <div class="search-hit-name">
              <a href="{{relpermalink}}">{{title}}</a>
              <div class="article-metadata search-hit-type">{{type}}</div>
              <p class="search-hit-description">{{snippet}}</p>
            </div>
          </div>
        </div>
      </script>
      
        <script src="https://cdnjs.cloudflare.com/ajax/libs/fuse.js/3.2.1/fuse.min.js" integrity="sha256-VzgmKYmhsGNNN4Ph1kMW+BjoYJM2jV5i4IlFoeZA9XI=" crossorigin="anonymous"></script>
        <script src="https://cdnjs.cloudflare.com/ajax/libs/mark.js/8.11.1/jquery.mark.min.js" integrity="sha256-4HLtjeVgH0eIB3aZ9mLYF6E8oU5chNdjU6p6rrXpl9U=" crossorigin="anonymous"></script>
      
    

    
    

    
    
    
    

    
    <script src="/js/bootstrap.bundle.min.js"></script>

    
    
      
      
      
      
      
      
      
    

    
    
    
    
    
    
    
    
      
      
    
    
    <script src="/en/js/wowchemy.min.js"></script>

    






</body>
</html>
