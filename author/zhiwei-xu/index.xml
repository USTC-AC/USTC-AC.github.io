<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Zhiwei Xu | USTC-AC</title>
    <link>https://ustc-ac.github.io/author/zhiwei-xu/</link>
      <atom:link href="https://ustc-ac.github.io/author/zhiwei-xu/index.xml" rel="self" type="application/rss+xml" />
    <description>Zhiwei Xu</description>
    <generator>Wowchemy (https://wowchemy.com)</generator><language>en-us</language><lastBuildDate>Tue, 06 Apr 2021 11:54:51 +0000</lastBuildDate>
    <image>
      <url>https://ustc-ac.github.io/author/zhiwei-xu/avatar_hu210a3f1eefa065a575378b12fd20a0f9_352023_270x270_fill_q75_lanczos_center.jpg</url>
      <title>Zhiwei Xu</title>
      <link>https://ustc-ac.github.io/author/zhiwei-xu/</link>
    </image>
    
    <item>
      <title>Emotional Attention Detection and Correlation Exploration for Image Emotion Distribution Learning</title>
      <link>https://ustc-ac.github.io/publication/emotional-attention-detection-and-correlation-exploration-for-image-emotion-distribution-learning/</link>
      <pubDate>Tue, 06 Apr 2021 11:54:51 +0000</pubDate>
      <guid>https://ustc-ac.github.io/publication/emotional-attention-detection-and-correlation-exploration-for-image-emotion-distribution-learning/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Capturing Emotion Distribution for Multimedia Emotion Tagging</title>
      <link>https://ustc-ac.github.io/publication/capturing-emotion-distribution-for-multimedia-emotion-tagging/</link>
      <pubDate>Thu, 18 Feb 2021 11:58:00 +0000</pubDate>
      <guid>https://ustc-ac.github.io/publication/capturing-emotion-distribution-for-multimedia-emotion-tagging/</guid>
      <description>&lt;p&gt;Multimedia collections usually induce multiple emotions in audiences. The data distribution of multiple emotions can be leveraged to facilitate the learning process of emotion tagging, yet has not been thoroughly explored. To address this, we propose adversarial learning to fully capture emotion distributions for emotion tagging of multimedia data. The proposed multimedia emotion tagging approach includes an emotion classifier and a discriminator. The emotion classifier predicts emotion labels of multimedia data from their content. The discriminator distinguishes the predicted emotion labels from the ground truth labels. The emotion classifier and the discriminator are trained simultaneously in competition with each other. By jointly minimizing the traditional supervised loss and maximizing the distribution similarity between the predicted emotion labels and the ground truth emotion labels, the proposed multimedia emotion tagging approach successfully captures both the mapping function between multimedia content and emotion labels as well as prior distribution in emotion labels, and thus achieves state-of-the-art performance for multiple emotion tagging, as demonstrated by the experimental results on four benchmark databases.&lt;/p&gt;














&lt;figure  id=&#34;figure-fig-the-framework-of-the-proposed-multi-emotion-tagging-model-c-is-the-multiple-emotion-tag-classifier-and-d-is-the-multiple-emotion-tag-discriminator-cx-is-the-multiple-emotion-prediction-for-feature-vector-x-y-is-the-ground-truth-multiple-emotion-label-of-x-y-0-is-the-real-multiple-emotion-label-from-ground-truth-label-set-the-dotted-line-indicates-that-there-is-an-one-to-one-correspondence-between-cx-and-y-see-text-for-details&#34;&gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img alt=&#34;Fig. The framework of the proposed multi-emotion tagging model. C is the multiple emotion tag classifier, and D is the multiple emotion tag discriminator. C(x) is the multiple emotion prediction for feature vector x. y is the ground truth multiple emotion label of x. y 0 is the real multiple emotion label from ground truth label set. The dotted line indicates that there is an one-to-one correspondence between C(x) and y. See text for details.&#34; srcset=&#34;
               /publication/capturing-emotion-distribution-for-multimedia-emotion-tagging/featured_hufb81d44ede70ae46d4815dea23d83fa1_71767_247e857c1e6d823735aaa3289daa1c24.jpg 400w,
               /publication/capturing-emotion-distribution-for-multimedia-emotion-tagging/featured_hufb81d44ede70ae46d4815dea23d83fa1_71767_9d15f47da7b78c49ba5849d19a69178e.jpg 760w,
               /publication/capturing-emotion-distribution-for-multimedia-emotion-tagging/featured_hufb81d44ede70ae46d4815dea23d83fa1_71767_1200x1200_fit_q75_lanczos.jpg 1200w&#34;
               src=&#34;https://ustc-ac.github.io/publication/capturing-emotion-distribution-for-multimedia-emotion-tagging/featured_hufb81d44ede70ae46d4815dea23d83fa1_71767_247e857c1e6d823735aaa3289daa1c24.jpg&#34;
               width=&#34;760&#34;
               height=&#34;324&#34;
               loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;figcaption&gt;
      Fig. The framework of the proposed multi-emotion tagging model. C is the multiple emotion tag classifier, and D is the multiple emotion tag discriminator. C(x) is the multiple emotion prediction for feature vector x. y is the ground truth multiple emotion label of x. y 0 is the real multiple emotion label from ground truth label set. The dotted line indicates that there is an one-to-one correspondence between C(x) and y. See text for details.
    &lt;/figcaption&gt;&lt;/figure&gt;
</description>
    </item>
    
    <item>
      <title>Exploiting Multi-Emotion Relations at Feature and Label Levels for Emotion Tagging</title>
      <link>https://ustc-ac.github.io/publication/dblp-confmm-xu-ww-20/</link>
      <pubDate>Wed, 01 Jan 2020 00:00:00 +0000</pubDate>
      <guid>https://ustc-ac.github.io/publication/dblp-confmm-xu-ww-20/</guid>
      <description>&lt;p&gt;The dependence among emotions is crucial to boost emotion tagging. In this paper, we propose a novel emotion tagging method, that thoroughly explores emotion relations from both the feature and label levels. Specifically, a graph convolutional network is introduced to inject local dependence among emotions into the model at the feature level, while an adversarial learning strategy is applied to constrain the joint distribution of multiple emotions at the label level. In addition, a new balanced loss function that mitigates the adverse effects of intra-class and inter-class imbalance is introduced to deal with the imbalance of emotion labels. Experimental results on several benchmark databases demonstrate the superiority of the proposed method compared to state-of-the-art works.&lt;/p&gt;














&lt;figure  id=&#34;figure-fig-the-architecture-of-the-proposed-emotion-tagging-method-it-consists-of-an-emotional-gcn-φg-an-encoder-φe--a-classifier-φc--and-a-discriminator-d-dgcn-denotes-the-product-of-the-output-of-emotional-gcn-and-the-encoder-feature-and-d-denotes-the-concatenation-of-dgcn-and-the-output-of-the-encoder-y-and-ŷrepresent-the-real-label-and-the-predicted-label-respectively&#34;&gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img alt=&#34;Fig. The architecture of the proposed emotion tagging method. It consists of an emotional GCN Φg, an encoder Φe , a classifier Φc , and a discriminator D. dgcn denotes the product of the output of emotional GCN and the encoder feature, and d denotes the concatenation of dgcn and the output of the encoder. y and ŷ represent the real label and the predicted label, respectively.&#34; srcset=&#34;
               /publication/dblp-confmm-xu-ww-20/featured_hu3761510e6e147991b8afa8272c353f87_120835_ea8d120282a84a3099ac9e43758706b0.jpg 400w,
               /publication/dblp-confmm-xu-ww-20/featured_hu3761510e6e147991b8afa8272c353f87_120835_3e2c15bb1a0f5c524a41632e86e8c53a.jpg 760w,
               /publication/dblp-confmm-xu-ww-20/featured_hu3761510e6e147991b8afa8272c353f87_120835_1200x1200_fit_q75_lanczos.jpg 1200w&#34;
               src=&#34;https://ustc-ac.github.io/publication/dblp-confmm-xu-ww-20/featured_hu3761510e6e147991b8afa8272c353f87_120835_ea8d120282a84a3099ac9e43758706b0.jpg&#34;
               width=&#34;760&#34;
               height=&#34;305&#34;
               loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;figcaption&gt;
      Fig. The architecture of the proposed emotion tagging method. It consists of an emotional GCN Φ&lt;sub&gt;g&lt;/sub&gt;, an encoder Φ&lt;sub&gt;e&lt;/sub&gt; , a classifier Φ&lt;sub&gt;c&lt;/sub&gt; , and a discriminator D. d&lt;sub&gt;gcn&lt;/sub&gt; denotes the product of the output of emotional GCN and the encoder feature, and d denotes the concatenation of d&lt;sub&gt;gcn&lt;/sub&gt; and the output of the encoder. y and ŷ represent the real label and the predicted label, respectively.
    &lt;/figcaption&gt;&lt;/figure&gt;
</description>
    </item>
    
  </channel>
</rss>
