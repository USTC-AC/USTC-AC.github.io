<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Zhuangqiang Zheng | USTC-AC</title>
    <link>http://localhost:1313/author/zhuangqiang-zheng/</link>
      <atom:link href="http://localhost:1313/author/zhuangqiang-zheng/index.xml" rel="self" type="application/rss+xml" />
    <description>Zhuangqiang Zheng</description>
    <generator>Wowchemy (https://wowchemy.com)</generator><language>en-us</language><lastBuildDate>Thu, 18 Feb 2021 11:58:00 +0000</lastBuildDate>
    <image>
      <url>http://localhost:1313/author/zhuangqiang-zheng/avatar_huf57b1dedfce8498c6317a36bc3138c51_8831_270x270_fill_q75_lanczos_center.jpg</url>
      <title>Zhuangqiang Zheng</title>
      <link>http://localhost:1313/author/zhuangqiang-zheng/</link>
    </image>
    
    <item>
      <title>Capturing Emotion Distribution for Multimedia Emotion Tagging</title>
      <link>http://localhost:1313/publication/capturing-emotion-distribution-for-multimedia-emotion-tagging/</link>
      <pubDate>Thu, 18 Feb 2021 11:58:00 +0000</pubDate>
      <guid>http://localhost:1313/publication/capturing-emotion-distribution-for-multimedia-emotion-tagging/</guid>
      <description>&lt;p&gt;Multimedia collections usually induce multiple emotions in audiences. The data distribution of multiple emotions can be leveraged to facilitate the learning process of emotion tagging, yet has not been thoroughly explored. To address this, we propose adversarial learning to fully capture emotion distributions for emotion tagging of multimedia data. The proposed multimedia emotion tagging approach includes an emotion classifier and a discriminator. The emotion classifier predicts emotion labels of multimedia data from their content. The discriminator distinguishes the predicted emotion labels from the ground truth labels. The emotion classifier and the discriminator are trained simultaneously in competition with each other. By jointly minimizing the traditional supervised loss and maximizing the distribution similarity between the predicted emotion labels and the ground truth emotion labels, the proposed multimedia emotion tagging approach successfully captures both the mapping function between multimedia content and emotion labels as well as prior distribution in emotion labels, and thus achieves state-of-the-art performance for multiple emotion tagging, as demonstrated by the experimental results on four benchmark databases.&lt;/p&gt;














&lt;figure  id=&#34;figure-fig-the-framework-of-the-proposed-multi-emotion-tagging-model-c-is-the-multiple-emotion-tag-classifier-and-d-is-the-multiple-emotion-tag-discriminator-cx-is-the-multiple-emotion-prediction-for-feature-vector-x-y-is-the-ground-truth-multiple-emotion-label-of-x-y-0-is-the-real-multiple-emotion-label-from-ground-truth-label-set-the-dotted-line-indicates-that-there-is-an-one-to-one-correspondence-between-cx-and-y-see-text-for-details&#34;&gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img alt=&#34;Fig. The framework of the proposed multi-emotion tagging model. C is the multiple emotion tag classifier, and D is the multiple emotion tag discriminator. C(x) is the multiple emotion prediction for feature vector x. y is the ground truth multiple emotion label of x. y 0 is the real multiple emotion label from ground truth label set. The dotted line indicates that there is an one-to-one correspondence between C(x) and y. See text for details.&#34; srcset=&#34;
               /publication/capturing-emotion-distribution-for-multimedia-emotion-tagging/featured_hufb81d44ede70ae46d4815dea23d83fa1_71767_247e857c1e6d823735aaa3289daa1c24.jpg 400w,
               /publication/capturing-emotion-distribution-for-multimedia-emotion-tagging/featured_hufb81d44ede70ae46d4815dea23d83fa1_71767_9d15f47da7b78c49ba5849d19a69178e.jpg 760w,
               /publication/capturing-emotion-distribution-for-multimedia-emotion-tagging/featured_hufb81d44ede70ae46d4815dea23d83fa1_71767_1200x1200_fit_q75_lanczos.jpg 1200w&#34;
               src=&#34;http://localhost:1313/publication/capturing-emotion-distribution-for-multimedia-emotion-tagging/featured_hufb81d44ede70ae46d4815dea23d83fa1_71767_247e857c1e6d823735aaa3289daa1c24.jpg&#34;
               width=&#34;760&#34;
               height=&#34;324&#34;
               loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;figcaption&gt;
      Fig. The framework of the proposed multi-emotion tagging model. C is the multiple emotion tag classifier, and D is the multiple emotion tag discriminator. C(x) is the multiple emotion prediction for feature vector x. y is the ground truth multiple emotion label of x. y 0 is the real multiple emotion label from ground truth label set. The dotted line indicates that there is an one-to-one correspondence between C(x) and y. See text for details.
    &lt;/figcaption&gt;&lt;/figure&gt;
</description>
    </item>
    
    <item>
      <title>A Novel Dynamic Model Capturing Spatial and Temporal Patterns for Facial Expression Analysis</title>
      <link>http://localhost:1313/publication/dblp-journalspami-wang-zyyj-20/</link>
      <pubDate>Wed, 01 Jan 2020 00:00:00 +0000</pubDate>
      <guid>http://localhost:1313/publication/dblp-journalspami-wang-zyyj-20/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Capturing Joint Label Distribution for Multi-Label Classification Through Adversarial Learning</title>
      <link>http://localhost:1313/publication/dblp-journalstkde-wang-pz-20/</link>
      <pubDate>Wed, 01 Jan 2020 00:00:00 +0000</pubDate>
      <guid>http://localhost:1313/publication/dblp-journalstkde-wang-pz-20/</guid>
      <description></description>
    </item>
    
  </channel>
</rss>
