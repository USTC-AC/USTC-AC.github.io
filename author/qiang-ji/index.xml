<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Qiang Ji | USTC-AC</title>
    <link>https://ustc-ac.github.io/author/qiang-ji/</link>
      <atom:link href="https://ustc-ac.github.io/author/qiang-ji/index.xml" rel="self" type="application/rss+xml" />
    <description>Qiang Ji</description>
    <generator>Wowchemy (https://wowchemy.com)</generator><language>en-us</language><lastBuildDate>Wed, 01 Jan 2020 00:00:00 +0000</lastBuildDate>
    <image>
      <url>https://ustc-ac.github.io/media/icon_huc3da470f1a231b6fd15916059be12d5d_66382_512x512_fill_lanczos_center_3.png</url>
      <title>Qiang Ji</title>
      <link>https://ustc-ac.github.io/author/qiang-ji/</link>
    </image>
    
    <item>
      <title>A Novel Dynamic Model Capturing Spatial and Temporal Patterns for Facial Expression Analysis</title>
      <link>https://ustc-ac.github.io/publication/dblp-journalspami-wang-zyyj-20/</link>
      <pubDate>Wed, 01 Jan 2020 00:00:00 +0000</pubDate>
      <guid>https://ustc-ac.github.io/publication/dblp-journalspami-wang-zyyj-20/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Exploring Domain Knowledge for Facial Expression-Assisted Action Unit Activation Recognition</title>
      <link>https://ustc-ac.github.io/publication/dblp-journalstaffco-wang-pj-20/</link>
      <pubDate>Wed, 01 Jan 2020 00:00:00 +0000</pubDate>
      <guid>https://ustc-ac.github.io/publication/dblp-journalstaffco-wang-pj-20/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Knowledge-Augmented Multimodal Deep Regression Bayesian Networks for Emotion Video Tagging</title>
      <link>https://ustc-ac.github.io/publication/dblp-journalstmm-wang-hj-20/</link>
      <pubDate>Wed, 01 Jan 2020 00:00:00 +0000</pubDate>
      <guid>https://ustc-ac.github.io/publication/dblp-journalstmm-wang-hj-20/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Posed and Spontaneous Expression Distinction Using Latent Regression Bayesian Networks</title>
      <link>https://ustc-ac.github.io/publication/dblp-journalstomccap-wang-hj-20/</link>
      <pubDate>Wed, 01 Jan 2020 00:00:00 +0000</pubDate>
      <guid>https://ustc-ac.github.io/publication/dblp-journalstomccap-wang-hj-20/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Affective Computing for Large-Scale Heterogeneous Multimedia Data: A Survey</title>
      <link>https://ustc-ac.github.io/publication/dblp-journalscorrabs-1911-05609/</link>
      <pubDate>Tue, 01 Jan 2019 00:00:00 +0000</pubDate>
      <guid>https://ustc-ac.github.io/publication/dblp-journalscorrabs-1911-05609/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Capturing Feature and Label Relations Simultaneously for Multiple Facial Action Unit Recognition</title>
      <link>https://ustc-ac.github.io/publication/dblp-journalstaffco-wang-wpj-19/</link>
      <pubDate>Tue, 01 Jan 2019 00:00:00 +0000</pubDate>
      <guid>https://ustc-ac.github.io/publication/dblp-journalstaffco-wang-wpj-19/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Content-Based Video Emotion Tagging Augmented by Users&#39; Multiple Physiological Responses</title>
      <link>https://ustc-ac.github.io/publication/dblp-journalstaffco-wang-cj-19/</link>
      <pubDate>Tue, 01 Jan 2019 00:00:00 +0000</pubDate>
      <guid>https://ustc-ac.github.io/publication/dblp-journalstaffco-wang-cj-19/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Facial Action Unit Recognition and Intensity Estimation Enhanced Through Label Dependencies</title>
      <link>https://ustc-ac.github.io/publication/dblp-journalstip-wang-hj-19/</link>
      <pubDate>Tue, 01 Jan 2019 00:00:00 +0000</pubDate>
      <guid>https://ustc-ac.github.io/publication/dblp-journalstip-wang-hj-19/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Facial Action Unit Recognition Augmented by Their Dependencies</title>
      <link>https://ustc-ac.github.io/publication/dblp-conffgr-hao-wpj-18/</link>
      <pubDate>Mon, 01 Jan 2018 00:00:00 +0000</pubDate>
      <guid>https://ustc-ac.github.io/publication/dblp-conffgr-hao-wpj-18/</guid>
      <description>&lt;p&gt;Due to the underlying anatomic mechanism that govern facial muscular interactions, there exist inherent de-pendencies between facial action units (AU). Such dependen-cies carry crucial information for AU recognition, yet have not been thoroughly exploited. Therefore, in this paper, we propose a novel AU recognition method with a three-layer hybrid Bayesian network, whose top two layers consist of a latent regression Bayesian network (LRBN), and the bottom two layers are Bayesian networks. The LRBN is a directed graphical model consisting of one latent layer and one visible layer. Specifically, the visible nodes of LRBN represent the ground-truth AU labels. Due to the “explaining away” effect in Bayesian networks, LRBN is able to capture both the depen-dencies among the latent variables given the observation and the dependencies among visible variables. Such dependencie ssuccessfully and faithfully represent relations among multiple AUs. The bottom two layers are two node Bayesian networks, connecting the ground truth AU labels and their measurements.Efficient learning and inference algorithms are also proposed.Furthermore, we extend the proposed hybrid Bayesian network model for facial expression-assisted AU recognition, since AUrelations are influenced by expressions. By introducing facial expression nodes in the middle visible layer, facial expressions,which are only required during training, facilitate the estima-tion of label dependencies among AUs. Experimental results on three benchmark databases, i.e. the CK+ database, the SEMAINE database, and the BP4D database, demonstrate that the proposed approaches can successfully capture complex AU relationships, and the expression labels available only during training are benefit for AU recognition during testing.&lt;/p&gt;














&lt;figure  id=&#34;figure-fig-the-proposed-au-recognition-through-au-relation-modeling&#34;&gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img alt=&#34;Fig. The proposed AU recognition through AU-relation modeling&#34; srcset=&#34;
               /publication/dblp-conffgr-hao-wpj-18/featured_hu186a5b8e8dbef9b8cf16ed6628856e29_401689_33d9177604e731c05ae08715c5f01605.jpg 400w,
               /publication/dblp-conffgr-hao-wpj-18/featured_hu186a5b8e8dbef9b8cf16ed6628856e29_401689_ba6243d78c78efbb864395b00b0b72ca.jpg 760w,
               /publication/dblp-conffgr-hao-wpj-18/featured_hu186a5b8e8dbef9b8cf16ed6628856e29_401689_1200x1200_fit_q75_lanczos.jpg 1200w&#34;
               src=&#34;https://ustc-ac.github.io/publication/dblp-conffgr-hao-wpj-18/featured_hu186a5b8e8dbef9b8cf16ed6628856e29_401689_33d9177604e731c05ae08715c5f01605.jpg&#34;
               width=&#34;760&#34;
               height=&#34;380&#34;
               loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;figcaption&gt;
      Fig. The proposed AU recognition through AU-relation modeling
    &lt;/figcaption&gt;&lt;/figure&gt;
</description>
    </item>
    
    <item>
      <title>Thermal Augmented Expression Recognition</title>
      <link>https://ustc-ac.github.io/publication/dblp-journalstcyb-wang-pcj-18/</link>
      <pubDate>Mon, 01 Jan 2018 00:00:00 +0000</pubDate>
      <guid>https://ustc-ac.github.io/publication/dblp-journalstcyb-wang-pcj-18/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Weakly Supervised Facial Action Unit Recognition With Domain Knowledge</title>
      <link>https://ustc-ac.github.io/publication/dblp-journalstcyb-wang-pcj-18-a/</link>
      <pubDate>Mon, 01 Jan 2018 00:00:00 +0000</pubDate>
      <guid>https://ustc-ac.github.io/publication/dblp-journalstcyb-wang-pcj-18-a/</guid>
      <description></description>
    </item>
    
    <item>
      <title>A Multimodal Deep Regression Bayesian Network for Affective Video Content Analyses</title>
      <link>https://ustc-ac.github.io/publication/dblp-conficcv-gan-whj-17/</link>
      <pubDate>Sun, 01 Jan 2017 00:00:00 +0000</pubDate>
      <guid>https://ustc-ac.github.io/publication/dblp-conficcv-gan-whj-17/</guid>
      <description>&lt;p&gt;The inherent dependencies between visual elements and aural elements are crucial for affective video content analyses, yet have not been successfully exploited. Therefore, we propose a multimodal deep regression Bayesian network (MMDRBN) to capture the dependencies between visual elements and aural elements for affective video content analyses. The regression Bayesian network (RBN) is a directed graphical model consisting of one latent layer and one visible layer. Due to the explaining away effect in Bayesian networks (BN), RBN is able to capture both the dependencies among the latent variables given the observation and the dependencies among visible variables. We propose a fast learning algorithm to learn the RBN. For the MMDRBN, first, we learn several RBNs layer-wisely from visual modality and audio modality respectively. Then we stack these RBNs and obtain two deep networks. After that, a joint representation is extracted from the top layers of the two deep networks, and thus captures the high order dependencies between visual modality and audio modality. In order to predict the valence or arousal score of video contents, we initialize a feed-forward inference network from the MMDRBN whose inference is intractable by minimizing the KullbackCLeibler (KL)divergence between the two networks. The back propagation algorithm is adopted for finetuning the inference network. Experimental results on the LIRIS-ACCEDE database demonstrate that the proposed MMDRBN successfully captures the dependencies between visual and audio elements, and thus achieves better performance compared with state-of-the-art work.














&lt;figure  id=&#34;figure-fig-the-framework-of-our-proposed-method-first-we-train-a-multimodal-generative-network-it-consists-of-two-stacked-rbns-that-are-created-for-visual-and-audio-modalities-respectively&#34;&gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img alt=&#34;Fig. The framework of our proposed method. First, we train a multimodal generative network. It consists of two stacked RBNs that are created for visual and audio modalities respectively.&#34; srcset=&#34;
               /publication/dblp-conficcv-gan-whj-17/featured_hu0ff0903a4051f103a27ffc3b5d24d0b2_144232_33de511fe787d72c7424d929317c4505.jpg 400w,
               /publication/dblp-conficcv-gan-whj-17/featured_hu0ff0903a4051f103a27ffc3b5d24d0b2_144232_70c0a87a200289b1d89f4c6a966a6481.jpg 760w,
               /publication/dblp-conficcv-gan-whj-17/featured_hu0ff0903a4051f103a27ffc3b5d24d0b2_144232_1200x1200_fit_q75_lanczos.jpg 1200w&#34;
               src=&#34;https://ustc-ac.github.io/publication/dblp-conficcv-gan-whj-17/featured_hu0ff0903a4051f103a27ffc3b5d24d0b2_144232_33de511fe787d72c7424d929317c4505.jpg&#34;
               width=&#34;760&#34;
               height=&#34;411&#34;
               loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;figcaption&gt;
      Fig. The framework of our proposed method. First, we train a multimodal generative network. It consists of two stacked RBNs that are created for visual and audio modalities respectively.
    &lt;/figcaption&gt;&lt;/figure&gt;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Capturing Dependencies among Labels and Features for Multiple Emotion Tagging of Multimedia Data</title>
      <link>https://ustc-ac.github.io/publication/dblp-confaaai-wu-wj-17/</link>
      <pubDate>Sun, 01 Jan 2017 00:00:00 +0000</pubDate>
      <guid>https://ustc-ac.github.io/publication/dblp-confaaai-wu-wj-17/</guid>
      <description>&lt;p&gt;In this paper,we tackle the problem of emotion tagging of multimedia data by modeling the dependencies among multiple emotions in both the feature and label spaces. These dependencies,which carry crucial top-down and bottom-up evidence for improving multimedia affective content analysis,have not been thoroughly exploited yet. To this end, we propose two hierarchical models that independently and dependently learn the shared features and global semantic relationships among emotion labels to jointly tag multiple emotion labels of multimedia data. Efficient learning and inference algorithms of the proposed models are also developed. Experiments on three benchmark emotion databases demonstrate the superior performance of our methods to existing methods.&lt;/p&gt;














&lt;figure  id=&#34;figure-fig-two-proposed-methodsa-combining-a-multi-task-rbm-with-a-three-layer-rbm-to-capture-dependencies-among-features-and-labels-independentlyb-capturing-dependencies-among-features-and-labels-dependently&#34;&gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img alt=&#34;Fig. Two proposed methods.(a) Combining a multi-task RBM with a three-layer RBM to capture dependencies among features and labels independently.(b) Capturing dependencies among features and labels dependently.&#34; srcset=&#34;
               /publication/dblp-confaaai-wu-wj-17/featured_hu703fb9dff318081c89de1de4167dd036_98867_f9dbd9aa622db9e1b1e017a870c9ff02.JPG 400w,
               /publication/dblp-confaaai-wu-wj-17/featured_hu703fb9dff318081c89de1de4167dd036_98867_dfd10bd9528c4784cc86e20ff1f3a372.JPG 760w,
               /publication/dblp-confaaai-wu-wj-17/featured_hu703fb9dff318081c89de1de4167dd036_98867_1200x1200_fit_q75_lanczos.JPG 1200w&#34;
               src=&#34;https://ustc-ac.github.io/publication/dblp-confaaai-wu-wj-17/featured_hu703fb9dff318081c89de1de4167dd036_98867_f9dbd9aa622db9e1b1e017a870c9ff02.JPG&#34;
               width=&#34;760&#34;
               height=&#34;439&#34;
               loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;figcaption&gt;
      Fig. Two proposed methods.(a) Combining a multi-task RBM with a three-layer RBM to capture dependencies among features and labels independently.(b) Capturing dependencies among features and labels dependently.
    &lt;/figcaption&gt;&lt;/figure&gt;
</description>
    </item>
    
    <item>
      <title>Deep Facial Action Unit Recognition from Partially Labeled Data</title>
      <link>https://ustc-ac.github.io/publication/dblp-conficcv-wu-wpj-17/</link>
      <pubDate>Sun, 01 Jan 2017 00:00:00 +0000</pubDate>
      <guid>https://ustc-ac.github.io/publication/dblp-conficcv-wu-wpj-17/</guid>
      <description>&lt;p&gt;Current work on facial action unit (AU) recognition requires AU-labeled facial images. Although large amounts of facial images are readily available, AU annotation is expensive and time consuming. To address this, we propose a deep facial action unit recognition approach learning from partially AU-labeled data. The proposed approach makes full use of both partly available ground-truth AU labels and the readily available large scale facial images without annotation. Specifically, we propose to learn label distribution from the ground-truth AU labels, and then train the AU classifiers from the large-scale facial images by maximizing the log likelihood of the mapping functions of AUs with regard to the learnt label distribution for all training data and minimizing the error between predicted AUs and ground-truth AUs for labeled data simultaneously. A restricted Boltzmann machine is adopted to model AU label distribution, a deep neural network is used to learn facial representation from facial images, and the support vector machine is employed as the classifier. Experiments on two benchmark databases demonstrate the effectiveness of the proposed approach.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Differentiating Between Posed and Spontaneous Expressions with Latent Regression Bayesian Network</title>
      <link>https://ustc-ac.github.io/publication/dblp-confaaai-gan-nwj-17/</link>
      <pubDate>Sun, 01 Jan 2017 00:00:00 +0000</pubDate>
      <guid>https://ustc-ac.github.io/publication/dblp-confaaai-gan-nwj-17/</guid>
      <description>&lt;p&gt;Spatial patterns embedded in human faces are crucial for differentiating posed expressions from spontaneous ones, yet they have not been thoroughly exploited in the literature. To tackle this problem, we present a generative model, i.e., Latent Regression Bayesian Network (LRBN), to effectively capture the spatial patterns embedded in facial landmark points to differentiate between posed and spontaneous facial expressions. The LRBN is a directed graphical model consisting of one latent layer and one visible layer. Due to the “explaining away” effect in Bayesian networks, LRBN is able to capture both the dependencies among the latent variables given the observation and the dependencies among visible variables. We believe that such dependencies are crucial for faithful data representation. Specifically, during training, we construct two LRBNs to capture spatial patterns inherent in displacements of landmark points from spontaneous facial expressions and posed facial expressions respectively. During testing, the samples are classified into posed or spontaneous expressions according to their likelihoods on two models. Efficient learning and inference algorithms are proposed. Experimental results on two benchmark databases demonstrate the advantages of the proposed approach in modeling spatial patterns as well as its superior performance to the existing methods in differentiating between posed and spontaneous expressions.&lt;/p&gt;














&lt;figure  id=&#34;figure-fig-the-framework-of-capturing-spatial-patterns&#34;&gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img alt=&#34;Fig. The framework of capturing spatial patterns&#34; srcset=&#34;
               /publication/dblp-confaaai-gan-nwj-17/featured_hu5eafe8c2628672eb14e0c65744466ffa_86294_efc2b202e0496b643e7469c436369d59.jpg 400w,
               /publication/dblp-confaaai-gan-nwj-17/featured_hu5eafe8c2628672eb14e0c65744466ffa_86294_b9cbd7521ce6355a25d7e9d9350ea896.jpg 760w,
               /publication/dblp-confaaai-gan-nwj-17/featured_hu5eafe8c2628672eb14e0c65744466ffa_86294_1200x1200_fit_q75_lanczos.jpg 1200w&#34;
               src=&#34;https://ustc-ac.github.io/publication/dblp-confaaai-gan-nwj-17/featured_hu5eafe8c2628672eb14e0c65744466ffa_86294_efc2b202e0496b643e7469c436369d59.jpg&#34;
               width=&#34;760&#34;
               height=&#34;377&#34;
               loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;figcaption&gt;
      Fig. The framework of capturing spatial patterns
    &lt;/figcaption&gt;&lt;/figure&gt;
</description>
    </item>
    
    <item>
      <title>Expression-assisted facial action unit recognition under incomplete AU annotation</title>
      <link>https://ustc-ac.github.io/publication/dblp-journalspr-wang-gj-17/</link>
      <pubDate>Sun, 01 Jan 2017 00:00:00 +0000</pubDate>
      <guid>https://ustc-ac.github.io/publication/dblp-journalspr-wang-gj-17/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Feature and label relation modeling for multiple-facial action unit classification and intensity estimation</title>
      <link>https://ustc-ac.github.io/publication/dblp-journalspr-wang-ygj-17/</link>
      <pubDate>Sun, 01 Jan 2017 00:00:00 +0000</pubDate>
      <guid>https://ustc-ac.github.io/publication/dblp-journalspr-wang-ygj-17/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Capturing global spatial patterns for distinguishing posed and spontaneous expressions</title>
      <link>https://ustc-ac.github.io/publication/dblp-journalscviu-wang-wj-16/</link>
      <pubDate>Fri, 01 Jan 2016 00:00:00 +0000</pubDate>
      <guid>https://ustc-ac.github.io/publication/dblp-journalscviu-wang-wj-16/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Employing subjects&#39; information as privileged information for emotion recognition from EEG signals</title>
      <link>https://ustc-ac.github.io/publication/dblp-conficpr-wu-wzgyj-16/</link>
      <pubDate>Fri, 01 Jan 2016 00:00:00 +0000</pubDate>
      <guid>https://ustc-ac.github.io/publication/dblp-conficpr-wu-wzgyj-16/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Facial Expression Intensity Estimation Using Ordinal Information</title>
      <link>https://ustc-ac.github.io/publication/dblp-confcvpr-zhao-gwj-16/</link>
      <pubDate>Fri, 01 Jan 2016 00:00:00 +0000</pubDate>
      <guid>https://ustc-ac.github.io/publication/dblp-confcvpr-zhao-gwj-16/</guid>
      <description>&lt;p&gt;Previous studies on facial expression analysis have been focused on recognizing basic expression categories.  There is limited  amount  of  work  on  the  continuous  expression intensity estimation,  which is important for detecting and tracking emotion change.  Part of the reason is the lack of labeled data with annotated expression intensity since ex-pression intensity annotation requires expertise and is time consuming.  In this work, we treat the expression intensity estimation as a regression problem. By taking advantage of the natural onset-apex-offset evolution pattern of facial ex-pression, the proposed method can handle different amounts of annotations to perform frame-level expression intensity estimation.    In  fully  supervised  case,  all  the  frames  are provided with intensity annotations.  In weakly supervised case, only the annotations of selected key frames are used.While in unsupervised case, expression intensity can be es-timated without any annotations.  An efficient optimization algorithm based on Alternating Direction Method of Mul-tipliers (ADMM) is developed for solving the optimization problem associated with parameter learning.   We demon-strate the effectiveness of proposed method by comparing it against both fully supervised and unsupervised approaches on benchmark facial expression datasets.&lt;/p&gt;














&lt;figure  id=&#34;figure-a-diagram-showing-the-experiment-process-depending-on-the-experiment-setting-different-amounts-of-intensity-annotation-information-are-fed-into-model-learning-process-resulting-different-models-training-is-performed-using-complete-expression-sequences-while-testing-is-performed-on-each-frame-of-a-sequence&#34;&gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img alt=&#34;A diagram showing the experiment process. Depending on the experiment setting, different amounts of intensity annotation information are fed into model learning process, resulting different models. Training is performed using complete expression sequences while testing is performed on each frame of a sequence.&#34; srcset=&#34;
               /publication/dblp-confcvpr-zhao-gwj-16/featured_hu82eae57b0a637c9c9c6659cadeeccee4_129242_5b734835e2304d704d3efb8a8632198b.jpg 400w,
               /publication/dblp-confcvpr-zhao-gwj-16/featured_hu82eae57b0a637c9c9c6659cadeeccee4_129242_0b80fe4ea1c3b64497154329948583d3.jpg 760w,
               /publication/dblp-confcvpr-zhao-gwj-16/featured_hu82eae57b0a637c9c9c6659cadeeccee4_129242_1200x1200_fit_q75_lanczos.jpg 1200w&#34;
               src=&#34;https://ustc-ac.github.io/publication/dblp-confcvpr-zhao-gwj-16/featured_hu82eae57b0a637c9c9c6659cadeeccee4_129242_5b734835e2304d704d3efb8a8632198b.jpg&#34;
               width=&#34;760&#34;
               height=&#34;153&#34;
               loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;figcaption&gt;
      A diagram showing the experiment process. Depending on the experiment setting, different amounts of intensity annotation information are fed into model learning process, resulting different models. Training is performed using complete expression sequences while testing is performed on each frame of a sequence.
    &lt;/figcaption&gt;&lt;/figure&gt;
</description>
    </item>
    
    <item>
      <title>Facial expression recognition through modeling age-related spatial patterns</title>
      <link>https://ustc-ac.github.io/publication/dblp-journalsmta-wang-wgj-16/</link>
      <pubDate>Fri, 01 Jan 2016 00:00:00 +0000</pubDate>
      <guid>https://ustc-ac.github.io/publication/dblp-journalsmta-wang-wgj-16/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Gender recognition from visible and thermal infrared facial images</title>
      <link>https://ustc-ac.github.io/publication/dblp-journalsmta-wang-ghhj-16/</link>
      <pubDate>Fri, 01 Jan 2016 00:00:00 +0000</pubDate>
      <guid>https://ustc-ac.github.io/publication/dblp-journalsmta-wang-ghhj-16/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Implicit hybrid video emotion tagging by integrating video content and users&#39; multiple physiological responses</title>
      <link>https://ustc-ac.github.io/publication/dblp-conficpr-chen-wwgsj-16/</link>
      <pubDate>Fri, 01 Jan 2016 00:00:00 +0000</pubDate>
      <guid>https://ustc-ac.github.io/publication/dblp-conficpr-chen-wwgsj-16/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Multiple Facial Action Unit recognition by learning joint features and label relations</title>
      <link>https://ustc-ac.github.io/publication/dblp-conficpr-wu-wj-16/</link>
      <pubDate>Fri, 01 Jan 2016 00:00:00 +0000</pubDate>
      <guid>https://ustc-ac.github.io/publication/dblp-conficpr-wu-wj-16/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Multiple facial action unit recognition enhanced by facial expressions</title>
      <link>https://ustc-ac.github.io/publication/dblp-conficpr-yang-wwj-16/</link>
      <pubDate>Fri, 01 Jan 2016 00:00:00 +0000</pubDate>
      <guid>https://ustc-ac.github.io/publication/dblp-conficpr-yang-wwj-16/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Emotion Recognition with the Help of Privileged Information</title>
      <link>https://ustc-ac.github.io/publication/dblp-journalstamd-wang-zyj-15/</link>
      <pubDate>Thu, 01 Jan 2015 00:00:00 +0000</pubDate>
      <guid>https://ustc-ac.github.io/publication/dblp-journalstamd-wang-zyj-15/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Facial Action Unit Classification with Hidden Knowledge under Incomplete Annotation</title>
      <link>https://ustc-ac.github.io/publication/dblp-confmir-wang-wj-15/</link>
      <pubDate>Thu, 01 Jan 2015 00:00:00 +0000</pubDate>
      <guid>https://ustc-ac.github.io/publication/dblp-confmir-wang-wj-15/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Implicit video emotion tagging from audiences&#39; facial expression</title>
      <link>https://ustc-ac.github.io/publication/dblp-journalsmta-wang-lzhcj-15/</link>
      <pubDate>Thu, 01 Jan 2015 00:00:00 +0000</pubDate>
      <guid>https://ustc-ac.github.io/publication/dblp-journalsmta-wang-lzhcj-15/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Learning with privileged information using Bayesian networks</title>
      <link>https://ustc-ac.github.io/publication/dblp-journalsfcsc-wang-hzhlj-15/</link>
      <pubDate>Thu, 01 Jan 2015 00:00:00 +0000</pubDate>
      <guid>https://ustc-ac.github.io/publication/dblp-journalsfcsc-wang-hzhlj-15/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Multi-instance Hidden Markov Model for facial expression recognition</title>
      <link>https://ustc-ac.github.io/publication/dblp-conffgr-wu-wj-15/</link>
      <pubDate>Thu, 01 Jan 2015 00:00:00 +0000</pubDate>
      <guid>https://ustc-ac.github.io/publication/dblp-conffgr-wu-wj-15/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Multiple Aesthetic Attribute Assessment by Exploiting Relations Among Aesthetic Attributes</title>
      <link>https://ustc-ac.github.io/publication/dblp-confmir-gao-wj-15/</link>
      <pubDate>Thu, 01 Jan 2015 00:00:00 +0000</pubDate>
      <guid>https://ustc-ac.github.io/publication/dblp-confmir-gao-wj-15/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Multiple Emotion Tagging for Multimedia Data by Exploiting High-Order Dependencies Among Emotions</title>
      <link>https://ustc-ac.github.io/publication/dblp-journalstmm-wang-wwj-15/</link>
      <pubDate>Thu, 01 Jan 2015 00:00:00 +0000</pubDate>
      <guid>https://ustc-ac.github.io/publication/dblp-journalstmm-wang-wwj-15/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Multiple emotional tagging of multimedia data by exploiting dependencies among emotions</title>
      <link>https://ustc-ac.github.io/publication/dblp-journalsmta-wang-wj-15/</link>
      <pubDate>Thu, 01 Jan 2015 00:00:00 +0000</pubDate>
      <guid>https://ustc-ac.github.io/publication/dblp-journalsmta-wang-wj-15/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Posed and spontaneous expression recognition through modeling their spatial patterns</title>
      <link>https://ustc-ac.github.io/publication/dblp-journalsmva-wang-whwj-15/</link>
      <pubDate>Thu, 01 Jan 2015 00:00:00 +0000</pubDate>
      <guid>https://ustc-ac.github.io/publication/dblp-journalsmva-wang-whwj-15/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Posed and spontaneous facial expression differentiation using deep Boltzmann machines</title>
      <link>https://ustc-ac.github.io/publication/dblp-confacii-gan-wwj-15/</link>
      <pubDate>Thu, 01 Jan 2015 00:00:00 +0000</pubDate>
      <guid>https://ustc-ac.github.io/publication/dblp-confacii-gan-wwj-15/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Video Affective Content Analysis: A Survey of State-of-the-Art Methods</title>
      <link>https://ustc-ac.github.io/publication/dblp-journalstaffco-wang-j-15/</link>
      <pubDate>Thu, 01 Jan 2015 00:00:00 +0000</pubDate>
      <guid>https://ustc-ac.github.io/publication/dblp-journalstaffco-wang-j-15/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Capture expression-dependent AU relations for expression recognition</title>
      <link>https://ustc-ac.github.io/publication/dblp-conficmcs-wang-whwglj-14/</link>
      <pubDate>Wed, 01 Jan 2014 00:00:00 +0000</pubDate>
      <guid>https://ustc-ac.github.io/publication/dblp-conficmcs-wang-whwglj-14/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Early Facial Expression Recognition Using Hidden Markov Models</title>
      <link>https://ustc-ac.github.io/publication/dblp-conficpr-wang-wj-14-a/</link>
      <pubDate>Wed, 01 Jan 2014 00:00:00 +0000</pubDate>
      <guid>https://ustc-ac.github.io/publication/dblp-conficpr-wang-wj-14-a/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Emotion recognition from thermal infrared images using deep Boltzmann machine</title>
      <link>https://ustc-ac.github.io/publication/dblp-journalsfcsc-wang-hghj-14/</link>
      <pubDate>Wed, 01 Jan 2014 00:00:00 +0000</pubDate>
      <guid>https://ustc-ac.github.io/publication/dblp-journalsfcsc-wang-hghj-14/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Emotion recognition from users&#39; EEG signals with the help of stimulus VIDEOS</title>
      <link>https://ustc-ac.github.io/publication/dblp-conficmcs-zhu-wj-14/</link>
      <pubDate>Wed, 01 Jan 2014 00:00:00 +0000</pubDate>
      <guid>https://ustc-ac.github.io/publication/dblp-conficmcs-zhu-wj-14/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Enhancing multi-label classification by modeling dependencies among labels</title>
      <link>https://ustc-ac.github.io/publication/dblp-journalspr-wang-wwj-14/</link>
      <pubDate>Wed, 01 Jan 2014 00:00:00 +0000</pubDate>
      <guid>https://ustc-ac.github.io/publication/dblp-journalspr-wang-wwj-14/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Exploiting multi-expression dependences for implicit multi-emotion video tagging</title>
      <link>https://ustc-ac.github.io/publication/dblp-journalsivc-wang-lwwlcj-14/</link>
      <pubDate>Wed, 01 Jan 2014 00:00:00 +0000</pubDate>
      <guid>https://ustc-ac.github.io/publication/dblp-journalsivc-wang-lwwlcj-14/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Facial Action Unit recognition by relation modeling from both qualitative knowledge and quantitative data</title>
      <link>https://ustc-ac.github.io/publication/dblp-conficmcs-gao-wwwj-14/</link>
      <pubDate>Wed, 01 Jan 2014 00:00:00 +0000</pubDate>
      <guid>https://ustc-ac.github.io/publication/dblp-conficmcs-gao-wwwj-14/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Fusion of visible and thermal images for facial expression recognition</title>
      <link>https://ustc-ac.github.io/publication/dblp-journalsfcsc-wang-hwhj-14/</link>
      <pubDate>Wed, 01 Jan 2014 00:00:00 +0000</pubDate>
      <guid>https://ustc-ac.github.io/publication/dblp-journalsfcsc-wang-hwhj-14/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Hybrid video emotional tagging using users&#39; EEG and video content</title>
      <link>https://ustc-ac.github.io/publication/dblp-journalsmta-wang-zwj-14/</link>
      <pubDate>Wed, 01 Jan 2014 00:00:00 +0000</pubDate>
      <guid>https://ustc-ac.github.io/publication/dblp-journalsmta-wang-zwj-14/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Multi-label Learning with Missing Labels</title>
      <link>https://ustc-ac.github.io/publication/dblp-conficpr-wu-lwhj-14/</link>
      <pubDate>Wed, 01 Jan 2014 00:00:00 +0000</pubDate>
      <guid>https://ustc-ac.github.io/publication/dblp-conficpr-wu-lwhj-14/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Multiple-Facial Action Unit Recognition by Shared Feature Learning and Semantic Relation Modeling</title>
      <link>https://ustc-ac.github.io/publication/dblp-conficpr-zhu-wyj-14/</link>
      <pubDate>Wed, 01 Jan 2014 00:00:00 +0000</pubDate>
      <guid>https://ustc-ac.github.io/publication/dblp-conficpr-zhu-wyj-14/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Sequence-based bias analysis of spontaneous facial expression databases</title>
      <link>https://ustc-ac.github.io/publication/dblp-conficmcs-wang-wwj-14/</link>
      <pubDate>Wed, 01 Jan 2014 00:00:00 +0000</pubDate>
      <guid>https://ustc-ac.github.io/publication/dblp-conficmcs-wang-wwj-14/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Active Labeling of Facial Feature Points</title>
      <link>https://ustc-ac.github.io/publication/dblp-confacii-he-wj-13/</link>
      <pubDate>Tue, 01 Jan 2013 00:00:00 +0000</pubDate>
      <guid>https://ustc-ac.github.io/publication/dblp-confacii-he-wj-13/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Capturing Complex Spatio-temporal Relations among Facial Muscles for Facial Expression Recognition</title>
      <link>https://ustc-ac.github.io/publication/dblp-confcvpr-wang-wj-13/</link>
      <pubDate>Tue, 01 Jan 2013 00:00:00 +0000</pubDate>
      <guid>https://ustc-ac.github.io/publication/dblp-confcvpr-wang-wj-13/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Capturing Global Semantic Relationships for Facial Action Unit Recognition</title>
      <link>https://ustc-ac.github.io/publication/dblp-conficcv-wang-lwj-13/</link>
      <pubDate>Tue, 01 Jan 2013 00:00:00 +0000</pubDate>
      <guid>https://ustc-ac.github.io/publication/dblp-conficcv-wang-lwj-13/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Emotional tagging of videos by exploring multiple emotions&#39; coexistence</title>
      <link>https://ustc-ac.github.io/publication/dblp-conffgr-wang-whlj-13/</link>
      <pubDate>Tue, 01 Jan 2013 00:00:00 +0000</pubDate>
      <guid>https://ustc-ac.github.io/publication/dblp-conffgr-wang-whlj-13/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Eye localization from thermal infrared images</title>
      <link>https://ustc-ac.github.io/publication/dblp-journalspr-wang-lsj-13/</link>
      <pubDate>Tue, 01 Jan 2013 00:00:00 +0000</pubDate>
      <guid>https://ustc-ac.github.io/publication/dblp-journalspr-wang-lsj-13/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Facial Expression Recognition Using Deep Boltzmann Machine from Thermal Infrared Images</title>
      <link>https://ustc-ac.github.io/publication/dblp-confacii-he-wlfj-13/</link>
      <pubDate>Tue, 01 Jan 2013 00:00:00 +0000</pubDate>
      <guid>https://ustc-ac.github.io/publication/dblp-confacii-he-wlfj-13/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Implicit video multi-emotion tagging by exploiting multi-expression relations</title>
      <link>https://ustc-ac.github.io/publication/dblp-conffgr-liu-wwj-13/</link>
      <pubDate>Tue, 01 Jan 2013 00:00:00 +0000</pubDate>
      <guid>https://ustc-ac.github.io/publication/dblp-conffgr-liu-wwj-13/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Simultaneous Facial Feature Tracking and Facial Expression Recognition</title>
      <link>https://ustc-ac.github.io/publication/dblp-journalstip-li-wzj-13/</link>
      <pubDate>Tue, 01 Jan 2013 00:00:00 +0000</pubDate>
      <guid>https://ustc-ac.github.io/publication/dblp-journalstip-li-wzj-13/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Bias analyses of spontaneous facial expression database</title>
      <link>https://ustc-ac.github.io/publication/dblp-conficpr-wang-wzj-12/</link>
      <pubDate>Sun, 01 Jan 2012 00:00:00 +0000</pubDate>
      <guid>https://ustc-ac.github.io/publication/dblp-conficpr-wang-wzj-12/</guid>
      <description></description>
    </item>
    
  </channel>
</rss>
