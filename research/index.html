<!DOCTYPE html><html lang="en-us" >

<head>
  <meta charset="utf-8" />
  <meta name="viewport" content="width=device-width, initial-scale=1" />
  <meta name="author" content="Indigo6" src="https://github.com/Indigo6"  />
  <meta name="copyright" content="Copyright ¬© Indigo6. All rights reserved."  src="https://github.com/Indigo6" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge" />
  
  
  
    <meta name="generator" content="Wowchemy 5.3.0 for Hugo" />
  

  <!--busuanzi view count -->
  <script async src="//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script>
  
  

  
  
  
  
  
    
    
    
  
  

  <meta name="author" content="Lin Fang" />

  
  
  
    
  
  <meta name="description" content="" />

  
  <link rel="alternate" hreflang="en-us" href="https://ustc-ac.github.io/research/" />

  









  




  
  

  
  
  
    <meta name="theme-color" content="#1565c0" />
  

  
  

  
  
  
  
    
    
      <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/academicons/1.9.0/css/academicons.min.css" integrity="sha512-W4yqoT1+8NLkinBLBZko+dFB2ZbHsYLDdr50VElllRcNt2Q4/GSs6u71UHKxB7S6JEMCp5Ve4xjh3eGQl/HRvg==" crossorigin="anonymous" media="print" onload="this.media='all'">
    
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.14.0/css/all.min.css" integrity="sha256-FMvZuGapsJLjouA6k7Eo2lusoAX9i0ShlWFG6qt7SLc=" crossorigin="anonymous" media="print" onload="this.media='all'">

    
    
    
    
      
      
      
      
        
      
    
    
    

    
    
    
      
    
    

    
    
    
      <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/leaflet/1.7.1/leaflet.min.css" integrity="sha512-1xoFisiGdy9nvho8EgXuXvnpR5GAMSjFwp40gSRE3NwdUdIMIKuPa7bqoUhLD0O/5tPNhteAsE5XyyMi5reQVA==" crossorigin="anonymous" media="print" onload="this.media='all'">
    

    

    
    
      
      

      
      

      
    
      
      

      
      

      
    
      
      

      
      

      
    
      
      

      
      

      
    
      
      

      
      

      
    
      
      

      
      

      
    
      
      

      
      

      
    
      
      

      
      

      
    
      
      

      
      

      
    
      
      

      
      

      
    
      
      

      
      

      
    
      
      

      
      

      
    
      
      

      
      

      
    
      
      

      
      

      
    
      
      

      
      
        
      

      
    
      
      

      
      

      
    
      
      

      
      

      
    
  

  
  
  

  
  
  
  
  
  <link rel="stylesheet" href="/css/wowchemy.edf39e98fe042890b919bc1e96a1fd57.css" />

  




<script async src="https://www.googletagmanager.com/gtag/js?id=UA-202839921-1"></script>
<script>
  window.dataLayer = window.dataLayer || [];

  function gtag() {
      dataLayer.push(arguments);
  }

  function trackOutboundLink(url, target) {
    gtag('event', 'click', {
         'event_category': 'outbound',
         'event_label': url,
         'transport_type': 'beacon',
         'event_callback': function () {
           if (target !== '_blank') {
             document.location = url;
           }
         }
    });
    console.debug("Outbound link clicked: " + url);
  }

  function onClickCallback(event) {
    if ((event.target.tagName !== 'A') || (event.target.host === window.location.host)) {
      return;
    }
    trackOutboundLink(event.target, event.target.getAttribute('target'));  
  }

  gtag('js', new Date());
  gtag('config', 'UA-202839921-1', {});
  gtag('set', {'cookie_flags': 'SameSite=None;Secure'});

  
  document.addEventListener('click', onClickCallback, false);
</script>


  

  

  




  
  
  

  

  
    <link rel="manifest" href="/manifest.webmanifest" />
  

  <link rel="icon" type="image/png" href="/media/icon_huc3da470f1a231b6fd15916059be12d5d_66382_32x32_fill_lanczos_center_3.png" />
  <link rel="apple-touch-icon" type="image/png" href="/media/icon_huc3da470f1a231b6fd15916059be12d5d_66382_180x180_fill_lanczos_center_3.png" />

  <link rel="canonical" href="https://ustc-ac.github.io/research/" />

  
  
  
  
  
  
  
  
    
    
  
  
  <meta property="twitter:card" content="summary" />
  
  <meta property="og:site_name" content="USTC-AC" />
  <meta property="og:url" content="https://ustc-ac.github.io/research/" />
  <meta property="og:title" content="Research | USTC-AC" />
  <meta property="og:description" content="" /><meta property="og:image" content="https://ustc-ac.github.io/media/icon_huc3da470f1a231b6fd15916059be12d5d_66382_512x512_fill_lanczos_center_3.png" />
    <meta property="twitter:image" content="https://ustc-ac.github.io/media/icon_huc3da470f1a231b6fd15916059be12d5d_66382_512x512_fill_lanczos_center_3.png" /><meta property="og:locale" content="en-us" />
  
    
    
  

  



  

  

  





  <title>Research | USTC-AC</title>
</head>


<body id="top" data-spy="scroll" data-offset="70" data-target="#TableOfContents" class="page-wrapper   " data-wc-page-id="f1d044c0738ab9f19347f15c290a71a1" >

  
  
  
  
  
  
  
  
  
  <script src="/js/wowchemy-init.min.1d309c6b3f55725f8869af2651084961.js"></script>

  


<aside class="search-modal" id="search">
  <div class="container">
    <section class="search-header">

      <div class="row no-gutters justify-content-between mb-3">
        <div class="col-6">
          <h1>Search</h1>
        </div>
        <div class="col-6 col-search-close">
          <a class="js-search" href="#" aria-label="Close"><i class="fas fa-times-circle text-muted" aria-hidden="true"></i></a>
        </div>
      </div>

      <div id="search-box">
        
        <input name="q" id="search-query" placeholder="Search..." autocapitalize="off"
        autocomplete="off" autocorrect="off" spellcheck="false" type="search" class="form-control"
        aria-label="Search...">
        
      </div>

      
      

      

    </section>
    <section class="section-search-results">

      <div id="search-hits">
        
      </div>

    </section>
  </div>
</aside>



  <div class="page-header">
    












<nav class="navbar navbar-expand-lg navbar-light compensate-for-scrollbar" id="navbar-main">
  <div class="container-xl">

    
    <div class="d-none d-lg-inline-flex">
      <a class="navbar-brand" href="/">USTC-AC</a>
    </div>
    

    
    <button type="button" class="navbar-toggler" data-toggle="collapse"
            data-target="#navbar-content" aria-controls="navbar-content" aria-expanded="false" aria-label="Toggle navigation">
    <span><i class="fas fa-bars"></i></span>
    </button>
    

    
    <div class="navbar-brand-mobile-wrapper d-inline-flex d-lg-none">
      <a class="navbar-brand" href="/">USTC-AC</a>
    </div>
    

    
    
    <div class="navbar-collapse main-menu-item collapse justify-content-end" id="navbar-content">

      
      <ul class="navbar-nav d-md-inline-flex">
        

        

        
        
        

        

        
        
        
        

        
          
            
            
          
          
        

        <li class="nav-item">
          <a class="nav-link " href="/post"><span>News</span></a>
        </li>

        
        

        

        
        
        
          
        

        

        
        
        
        

        
          
            
            
          
          
        

        <li class="nav-item">
          <a class="nav-link " href="/people"><span>People</span></a>
        </li>

        
        

        

        
        
        
          
        

        

        
        
        
        

        
          
            
            
          
          
        

        <li class="nav-item">
          <a class="nav-link " href="/graduated"><span>Graduated</span></a>
        </li>

        
        

        

        
        
        
          
        

        

        
        
        
        

        
          
            
            
          
          
        

        <li class="nav-item">
          <a class="nav-link " href="/research" data-target="[]"><span>Research</span></a>
        </li>

        
        

        

        
        
        
          
        

        

        
        
        
        

        
          
            
            
          
          
        

        <li class="nav-item">
          <a class="nav-link " href="/publication"><span>Publications</span></a>
        </li>

        
        

        

        
        
        
          
        

        

        
        
        
        

        
          
            
            
          
          
        

        <li class="nav-item">
          <a class="nav-link " href="/datasets"><span>Datasets</span></a>
        </li>

        
        

        

        
        
        
          
        

        

        
        
        
        

        
          
            
            
          
          
        

        <li class="nav-item">
          <a class="nav-link " href="/contact"><span>Contact</span></a>
        </li>

        
        

      

        
      </ul>
    </div>

    <ul class="nav-icons navbar-nav flex-row ml-auto d-flex pl-md-2">

      
      

      
      
      <li class="nav-item">
        <a class="nav-link js-search" href="#" aria-label="Search"><i class="fas fa-search" aria-hidden="true"></i></a>
      </li>
      

      
      
      <li class="nav-item dropdown theme-dropdown">
        <a href="#" class="nav-link" data-toggle="dropdown" aria-haspopup="true" aria-label="Display preferences">
          <i class="fas fa-moon" aria-hidden="true"></i>
        </a>
        <div class="dropdown-menu">
          <a href="#" class="dropdown-item js-set-theme-light">
            <span>Light</span>
          </a>
          <a href="#" class="dropdown-item js-set-theme-dark">
            <span>Dark</span>
          </a>
          <a href="#" class="dropdown-item js-set-theme-auto">
            <span>Automatic</span>
          </a>
        </div>
      </li>
      

      
      

    </ul>

  </div>
</nav>


  </div>

  <div class="page-body">
    
<span class="js-widget-page d-none"></span>





  
  
  
  




  
  
  
  

  

  

  

  

  

  
  

  

  
  

  
  
  

  
  
  
  
  

  
  

  

  
  

  
  

  
  <section id="research" class="home-section wg-featured  "  >
   <div class="home-section-bg " ></div>
    <div class="container">

    
      <div class="row  justify-content-center" align="justify">
      
    

      






























  




<div class="col-12 ">

  

  
    
      







  







  


<div class="media stream-item">
  <div class="col-12 col-md-9">

    <div class="section-subheading article-title mb-0 mt-0">
      <a href="/publication/manual-fg23-fang/" >Human Pose Estimation with Shape Aware Loss</a>
    </div>

    
    <a href="/publication/manual-fg23-fang/"  class="summary-link">
      <div class="article-style">
        Although the mean square error (mse) of heatmap is an intuitive loss for heatmap-based human pose estimation, the joints localization ‚Ä¶
      </div>
    </a>
    

    <div class="stream-meta article-metadata">

      

      
      <div>
        

  <span >
      <a href="/author/lin-fang/">Lin Fang</a></span>, <span >
      <a href="/author/shangfei-wang/">Shangfei Wang</a></span>
      </div>
      
    </div>

    

  </div>
  <div class="col-12 col-md-3">
    
    
    
    
    
    
        
        
        

        
          
          
            
          
          
        
          
          
          
        
          
          
          
        
        

        <img src="/publication/manual-fg23-fang/featured_hu43c98b023c940b0481dcc28a9f67193a_160922_38eb804227dcd0298de829b49a879168.jpg" srcset="/publication/manual-fg23-fang/featured_hu43c98b023c940b0481dcc28a9f67193a_160922_2fb6d2daab8200191f44e39699d48270.jpg 1200w,/publication/manual-fg23-fang/featured_hu43c98b023c940b0481dcc28a9f67193a_160922_db57ea84932cd31380374d1b6234c622.jpg 800w,/publication/manual-fg23-fang/featured_hu43c98b023c940b0481dcc28a9f67193a_160922_38eb804227dcd0298de829b49a879168.jpg 400w" width="450" height="184" alt="Human Pose Estimation with Shape Aware Loss">

    
    
  </div>
</div>

    
  
    
      







  







  


<div class="media stream-item">
  <div class="col-12 col-md-9">

    <div class="section-subheading article-title mb-0 mt-0">
      <a href="/publication/manual-fg21-liang/" >Pose-Invariant Facial Expression Recognition</a>
    </div>

    
    <a href="/publication/manual-fg21-liang/"  class="summary-link">
      <div class="article-style">
        Pose-invariant facial expression recognition is quite challenging due to variations in facial appearance and self-occlusion caused by ‚Ä¶
      </div>
    </a>
    

    <div class="stream-meta article-metadata">

      

      
      <div>
        

  <span >
      <a href="/author/guang-liang/">Guang Liang</a></span>, <span >
      <a href="/author/shangfei-wang/">Shangfei Wang</a></span>, <span >
      <a href="/author/can-wang/">Can Wang</a></span>
      </div>
      
    </div>

    
    <div class="btn-links">
      








  
    
  



<a class="btn btn-outline-primary btn-page-header btn-sm" href="https://ieeexplore.ieee.org/document/9666974" target="_blank" rel="noopener">
  PDF
</a>















<a class="btn btn-outline-primary btn-page-header btn-sm" href="https://doi.org/10.1109/FG52635.2021.9666974" target="_blank" rel="noopener">
  DOI
</a>



    </div>
    

  </div>
  <div class="col-12 col-md-3">
    
    
    
    
    
    
        
        
        

        
          
          
            
          
          
        
          
          
          
        
          
          
          
        
        

        <img src="/publication/manual-fg21-liang/featured_hu5438825b9b6d1014226d20d231e650c2_1500562_9f0e50a7676ae18598d04941db82419b.jpg" srcset="/publication/manual-fg21-liang/featured_hu5438825b9b6d1014226d20d231e650c2_1500562_5dd63fa22d573c6fc213e4090ff51a29.jpg 1200w,/publication/manual-fg21-liang/featured_hu5438825b9b6d1014226d20d231e650c2_1500562_24da995eff36cbc12e9a705d7a0d3fb5.jpg 800w,/publication/manual-fg21-liang/featured_hu5438825b9b6d1014226d20d231e650c2_1500562_9f0e50a7676ae18598d04941db82419b.jpg 400w" width="450" height="253" alt="Pose-Invariant Facial Expression Recognition">

    
    
  </div>
</div>

    
  
    
      







  







  


<div class="media stream-item">
  <div class="col-12 col-md-9">

    <div class="section-subheading article-title mb-0 mt-0">
      <a href="/publication/capturing-emotion-distribution-for-multimedia-emotion-tagging/" >Capturing Emotion Distribution for Multimedia Emotion Tagging</a>
    </div>

    
    <a href="/publication/capturing-emotion-distribution-for-multimedia-emotion-tagging/"  class="summary-link">
      <div class="article-style">
        To address the statistical similarity between the predicted emotion labels and ground-truth emotion labels without any assumptions, we propose a novel emotion tagging approach through adversarial learning. Specifically, the proposed emotion tagging approach consists of an emotional tag classifier C and a discriminator D.
      </div>
    </a>
    

    <div class="stream-meta article-metadata">

      

      
      <div>
        

  <span >
      <a href="/author/shangfei-wang/">Shangfei Wang</a></span>, <span >
      <a href="/author/guozhu-peng/">Guozhu Peng</a></span>, <span >
      <a href="/author/zhuangqiang-zheng/">Zhuangqiang Zheng</a></span>, <span >
      <a href="/author/zhiwei-xu/">Zhiwei Xu</a></span>
      </div>
      
    </div>

    
    <div class="btn-links">
      








  





<a href="#" class="btn btn-outline-primary btn-page-header btn-sm js-cite-modal"
        data-filename="/publication/capturing-emotion-distribution-for-multimedia-emotion-tagging/cite.bib">
  Cite
</a>













<a class="btn btn-outline-primary btn-page-header btn-sm" href="https://doi.org/10.1109/TAFFC.2019.2900240" target="_blank" rel="noopener">
  DOI
</a>



    </div>
    

  </div>
  <div class="col-12 col-md-3">
    
    
    
    
    
    
        
        
        

        
          
          
            
          
          
        
          
          
          
        
          
          
          
        
        

        <img src="/publication/capturing-emotion-distribution-for-multimedia-emotion-tagging/featured_hufb81d44ede70ae46d4815dea23d83fa1_71767_bd4acce5aa592eb1d0e005cb2977be28.jpg" srcset="/publication/capturing-emotion-distribution-for-multimedia-emotion-tagging/featured_hufb81d44ede70ae46d4815dea23d83fa1_71767_b563a54f50db20732dc5c1532c567756.jpg 1200w,/publication/capturing-emotion-distribution-for-multimedia-emotion-tagging/featured_hufb81d44ede70ae46d4815dea23d83fa1_71767_889d66254289be11b6211b472748cd7f.jpg 800w,/publication/capturing-emotion-distribution-for-multimedia-emotion-tagging/featured_hufb81d44ede70ae46d4815dea23d83fa1_71767_bd4acce5aa592eb1d0e005cb2977be28.jpg 400w" width="450" height="192" alt="Capturing Emotion Distribution for Multimedia Emotion Tagging">

    
    
  </div>
</div>

    
  
    
      







  







  


<div class="media stream-item">
  <div class="col-12 col-md-9">

    <div class="section-subheading article-title mb-0 mt-0">
      <a href="/publication/dblp-confmm-yin-wccl-20/" >Attentive One-Dimensional Heatmap Regression for Facial Landmark Detection and Tracking</a>
    </div>

    
    <a href="/publication/dblp-confmm-yin-wccl-20/"  class="summary-link">
      <div class="article-style">
        The main contributions of the method are three folds. First, we are the first that propose to predict 1D heatmaps on the ùë• and ùë¶ axes instead of using 2D heatmaps to locate landmarks and successfully alleviate the quantization error with a fully boosted output resolution. Second, we propose a co-attention module to capture the joint coordinate distribution on the two axes. Third, based on the proposed heatmap regression method, we design a facial landmark detector and tracker which achieve state-of-the-art performance.
      </div>
    </a>
    

    <div class="stream-meta article-metadata">

      

      
      <div>
        

  <span >
      <a href="/author/shi-yin/">Shi Yin</a></span>, <span >
      <a href="/author/shangfei-wang/">Shangfei Wang</a></span>, <span >
      <a href="/author/xiaoping-chen/">Xiaoping Chen</a></span>, <span >
      <a href="/author/enhong-chen/">Enhong Chen</a></span>, <span >
      <a href="/author/cong-liang/">Cong Liang</a></span>
      </div>
      
    </div>

    
    <div class="btn-links">
      








  
    
  



<a class="btn btn-outline-primary btn-page-header btn-sm" href="https://doi.org/10.1145/3394171.3413509" target="_blank" rel="noopener">
  PDF
</a>



<a href="#" class="btn btn-outline-primary btn-page-header btn-sm js-cite-modal"
        data-filename="/publication/dblp-confmm-yin-wccl-20/cite.bib">
  Cite
</a>













<a class="btn btn-outline-primary btn-page-header btn-sm" href="https://doi.org/10.1145/3394171.3413509" target="_blank" rel="noopener">
  DOI
</a>



    </div>
    

  </div>
  <div class="col-12 col-md-3">
    
    
    
    
    
    
        
        
        

        
          
          
            
          
          
        
          
          
          
        
          
          
          
        
        

        <img src="/publication/dblp-confmm-yin-wccl-20/featured_hu22c09b2bbf7a4fd412912d6cba403c45_869902_43481045afbcec0f9844f86e4ec34cfd.png" srcset="/publication/dblp-confmm-yin-wccl-20/featured_hu22c09b2bbf7a4fd412912d6cba403c45_869902_73cb60b5af68f4246e8e8a1465554b5f.png 1200w,/publication/dblp-confmm-yin-wccl-20/featured_hu22c09b2bbf7a4fd412912d6cba403c45_869902_6a28ba0c000be97c1c35542887d58c59.png 800w,/publication/dblp-confmm-yin-wccl-20/featured_hu22c09b2bbf7a4fd412912d6cba403c45_869902_43481045afbcec0f9844f86e4ec34cfd.png 400w" width="450" height="295" alt="Attentive One-Dimensional Heatmap Regression for Facial Landmark Detection and Tracking">

    
    
  </div>
</div>

    
  
    
      







  







  


<div class="media stream-item">
  <div class="col-12 col-md-9">

    <div class="section-subheading article-title mb-0 mt-0">
      <a href="/publication/dblp-confmm-xu-ww-20/" >Exploiting Multi-Emotion Relations at Feature and Label Levels for Emotion Tagging</a>
    </div>

    
    <a href="/publication/dblp-confmm-xu-ww-20/"  class="summary-link">
      <div class="article-style">
        To address the shortcomings in emotion tagging, we consider applying emotion relationship patterns at both feature and label levels. We propose a novel emotion tagging framework, that makes full use of the emotion relationship patterns in local and global distribution.
      </div>
    </a>
    

    <div class="stream-meta article-metadata">

      

      
      <div>
        

  <span >
      <a href="/author/zhiwei-xu/">Zhiwei Xu</a></span>, <span >
      <a href="/author/shangfei-wang/">Shangfei Wang</a></span>, <span >
      <a href="/author/can-wang/">Can Wang</a></span>
      </div>
      
    </div>

    
    <div class="btn-links">
      








  
    
  



<a class="btn btn-outline-primary btn-page-header btn-sm" href="https://doi.org/10.1145/3394171.3413506" target="_blank" rel="noopener">
  PDF
</a>



<a href="#" class="btn btn-outline-primary btn-page-header btn-sm js-cite-modal"
        data-filename="/publication/dblp-confmm-xu-ww-20/cite.bib">
  Cite
</a>













<a class="btn btn-outline-primary btn-page-header btn-sm" href="https://doi.org/10.1145/3394171.3413506" target="_blank" rel="noopener">
  DOI
</a>



    </div>
    

  </div>
  <div class="col-12 col-md-3">
    
    
    
    
    
    
        
        
        

        
          
          
            
          
          
        
          
          
          
        
          
          
          
        
        

        <img src="/publication/dblp-confmm-xu-ww-20/featured_hu3761510e6e147991b8afa8272c353f87_120835_f5a9dd2198c840aadbfaa76945d277a6.jpg" srcset="/publication/dblp-confmm-xu-ww-20/featured_hu3761510e6e147991b8afa8272c353f87_120835_90ca1dac7dcf75569db8b4362e78e627.jpg 1200w,/publication/dblp-confmm-xu-ww-20/featured_hu3761510e6e147991b8afa8272c353f87_120835_4ac60dec3ee94c314dcf4d94f000cf00.jpg 800w,/publication/dblp-confmm-xu-ww-20/featured_hu3761510e6e147991b8afa8272c353f87_120835_f5a9dd2198c840aadbfaa76945d277a6.jpg 400w" width="450" height="180" alt="Exploiting Multi-Emotion Relations at Feature and Label Levels for Emotion Tagging">

    
    
  </div>
</div>

    
  
    
      







  







  


<div class="media stream-item">
  <div class="col-12 col-md-9">

    <div class="section-subheading article-title mb-0 mt-0">
      <a href="/publication/dblp-confmm-yin-wcc-20/" >Exploiting Self-Supervised and Semi-Supervised Learning for Facial Landmark Tracking with Unlabeled Data</a>
    </div>

    
    <a href="/publication/dblp-confmm-yin-wcc-20/"  class="summary-link">
      <div class="article-style">
        We propose a new semi-supervised learning strategy which trains the tracker by regression tasks from the consistency constraints on the long facial sequence instead of two adjacent frames, such that the long-term dependencies existed in a facial sequence are captured. The proposed semisupervised learning strategy does not require any extra labels. Thus, large scale unlabeled data can be exploited for training.
      </div>
    </a>
    

    <div class="stream-meta article-metadata">

      

      
      <div>
        

  <span >
      <a href="/author/shi-yin/">Shi Yin</a></span>, <span >
      <a href="/author/shangfei-wang/">Shangfei Wang</a></span>, <span >
      <a href="/author/xiaoping-chen/">Xiaoping Chen</a></span>, <span >
      <a href="/author/enhong-chen/">Enhong Chen</a></span>
      </div>
      
    </div>

    
    <div class="btn-links">
      








  
    
  



<a class="btn btn-outline-primary btn-page-header btn-sm" href="https://doi.org/10.1145/3394171.3413547" target="_blank" rel="noopener">
  PDF
</a>



<a href="#" class="btn btn-outline-primary btn-page-header btn-sm js-cite-modal"
        data-filename="/publication/dblp-confmm-yin-wcc-20/cite.bib">
  Cite
</a>













<a class="btn btn-outline-primary btn-page-header btn-sm" href="https://doi.org/10.1145/3394171.3413547" target="_blank" rel="noopener">
  DOI
</a>



    </div>
    

  </div>
  <div class="col-12 col-md-3">
    
    
    
    
    
    
        
        
        

        
          
          
            
          
          
        
          
          
          
        
          
          
          
        
        

        <img src="/publication/dblp-confmm-yin-wcc-20/featured_hub08f116c5b40a4b0bfaddc695f5ffa21_256954_5b6335b88b1442f10857a1130426c8d0.jpg" srcset="/publication/dblp-confmm-yin-wcc-20/featured_hub08f116c5b40a4b0bfaddc695f5ffa21_256954_6892f46a3751114af5ddb66c827fea35.jpg 1200w,/publication/dblp-confmm-yin-wcc-20/featured_hub08f116c5b40a4b0bfaddc695f5ffa21_256954_62adde5b198f78c5383ac56dd987565c.jpg 800w,/publication/dblp-confmm-yin-wcc-20/featured_hub08f116c5b40a4b0bfaddc695f5ffa21_256954_5b6335b88b1442f10857a1130426c8d0.jpg 400w" width="450" height="220" alt="Exploiting Self-Supervised and Semi-Supervised Learning for Facial Landmark Tracking with Unlabeled Data">

    
    
  </div>
</div>

    
  
    
      







  







  


<div class="media stream-item">
  <div class="col-12 col-md-9">

    <div class="section-subheading article-title mb-0 mt-0">
      <a href="/publication/dblp-confmm-0012-wwc-20/" >Learning from Macro-expression: a Micro-expression Recognition Framework</a>
    </div>

    
    <a href="/publication/dblp-confmm-0012-wwc-20/"  class="summary-link">
      <div class="article-style">
        In order to address problems in micro-expression recognition, we propose a micro-expression recognition framework that leverages macroexpression as guidance. Since subjects in macro-expression and micro-expression databases are different, Expression-Identity Disentangle Network (EIDNet) is introduced as feature extractor to disentangle expression-related features for expression samples.
      </div>
    </a>
    

    <div class="stream-meta article-metadata">

      

      
      <div>
        

  <span >
      <a href="/author/bin-xia/">Bin Xia</a></span>, <span >
      <a href="/author/weikang-wang/">Weikang Wang</a></span>, <span >
      <a href="/author/shangfei-wang/">Shangfei Wang</a></span>, <span >
      <a href="/author/enhong-chen/">Enhong Chen</a></span>
      </div>
      
    </div>

    
    <div class="btn-links">
      








  
    
  



<a class="btn btn-outline-primary btn-page-header btn-sm" href="https://doi.org/10.1145/3394171.3413774" target="_blank" rel="noopener">
  PDF
</a>



<a href="#" class="btn btn-outline-primary btn-page-header btn-sm js-cite-modal"
        data-filename="/publication/dblp-confmm-0012-wwc-20/cite.bib">
  Cite
</a>













<a class="btn btn-outline-primary btn-page-header btn-sm" href="https://doi.org/10.1145/3394171.3413774" target="_blank" rel="noopener">
  DOI
</a>



    </div>
    

  </div>
  <div class="col-12 col-md-3">
    
    
    
    
    
    
        
        
        

        
          
          
            
          
          
        
          
          
          
        
          
          
          
        
        

        <img src="/publication/dblp-confmm-0012-wwc-20/featured_hucbe6b30a302d6c912ad5aaa64ac11de4_85027_c1bb20ee2b7045a1967d40c977d033dc.jpg" srcset="/publication/dblp-confmm-0012-wwc-20/featured_hucbe6b30a302d6c912ad5aaa64ac11de4_85027_c4972dbde6b6e62010b30c5e61a8443a.jpg 1200w,/publication/dblp-confmm-0012-wwc-20/featured_hucbe6b30a302d6c912ad5aaa64ac11de4_85027_e7a8bd1513b3c421043883a770a0ccc9.jpg 800w,/publication/dblp-confmm-0012-wwc-20/featured_hucbe6b30a302d6c912ad5aaa64ac11de4_85027_c1bb20ee2b7045a1967d40c977d033dc.jpg 400w" width="450" height="240" alt="Learning from Macro-expression: a Micro-expression Recognition Framework">

    
    
  </div>
</div>

    
  
    
      







  







  


<div class="media stream-item">
  <div class="col-12 col-md-9">

    <div class="section-subheading article-title mb-0 mt-0">
      <a href="/publication/dblp-confmm-0012-w-20/" >Occluded Facial Expression Recognition with Step-Wise Assistance from Unpaired Non-Occluded Images</a>
    </div>

    
    <a href="/publication/dblp-confmm-0012-w-20/"  class="summary-link">
      <div class="article-style">
        To tackle the challenges in occluded facial expression recognition, we propose a step-wise learning strategy including two types of complementary adversarial learning. In this way, the occluded classifier can learn effective information from large-scale unpaired non-occluded facial images.
      </div>
    </a>
    

    <div class="stream-meta article-metadata">

      

      
      <div>
        

  <span >
      <a href="/author/bin-xia/">Bin Xia</a></span>, <span >
      <a href="/author/shangfei-wang/">Shangfei Wang</a></span>
      </div>
      
    </div>

    
    <div class="btn-links">
      








  
    
  



<a class="btn btn-outline-primary btn-page-header btn-sm" href="https://doi.org/10.1145/3394171.3413773" target="_blank" rel="noopener">
  PDF
</a>



<a href="#" class="btn btn-outline-primary btn-page-header btn-sm js-cite-modal"
        data-filename="/publication/dblp-confmm-0012-w-20/cite.bib">
  Cite
</a>













<a class="btn btn-outline-primary btn-page-header btn-sm" href="https://doi.org/10.1145/3394171.3413773" target="_blank" rel="noopener">
  DOI
</a>



    </div>
    

  </div>
  <div class="col-12 col-md-3">
    
    
    
    
    
    
        
        
        

        
          
          
            
          
          
        
          
          
          
        
          
          
          
        
        

        <img src="/publication/dblp-confmm-0012-w-20/featured_hu82f0b636e3d7f43c2e4e1586bc541f2e_109475_b8f5b691541651ba9f5a2c07b74496f2.jpg" srcset="/publication/dblp-confmm-0012-w-20/featured_hu82f0b636e3d7f43c2e4e1586bc541f2e_109475_4c4bdfd67c436f77a9d243fb599bb5e6.jpg 1200w,/publication/dblp-confmm-0012-w-20/featured_hu82f0b636e3d7f43c2e4e1586bc541f2e_109475_b6a039d3697cc2df0a5c6af04a6be7ec.jpg 800w,/publication/dblp-confmm-0012-w-20/featured_hu82f0b636e3d7f43c2e4e1586bc541f2e_109475_b8f5b691541651ba9f5a2c07b74496f2.jpg 400w" width="450" height="162" alt="Occluded Facial Expression Recognition with Step-Wise Assistance from Unpaired Non-Occluded Images">

    
    
  </div>
</div>

    
  
    
      







  







  


<div class="media stream-item">
  <div class="col-12 col-md-9">

    <div class="section-subheading article-title mb-0 mt-0">
      <a href="/publication/dblp-confaccv-0012-w-20/" >Unpaired Multimodal Facial Expression Recognition</a>
    </div>

    
    <a href="/publication/dblp-confaccv-0012-w-20/"  class="summary-link">
      <div class="article-style">
        Since collecting paired visible and thermal facial images is often difficult, requiring paired data during training prevents the usage of the many available unpaired visible and thermal images, and thus may degenerate the learning effect of the visible facial expression classifier.
To address this, we propose an unpaired adversarial facial expression recognition method. We tackle the unbalanced quantity of visible and thermal images by utilizing thermal images as privileged information. We introduce adversarial learning on the feature-level and label-level spaces to cope with unpaired training data. Finally, we add a decoder network to preserve the inherent visible features.
      </div>
    </a>
    

    <div class="stream-meta article-metadata">

      

      
      <div>
        

  <span >
      <a href="/author/bin-xia/">Bin Xia</a></span>, <span >
      <a href="/author/shangfei-wang/">Shangfei Wang</a></span>
      </div>
      
    </div>

    
    <div class="btn-links">
      








  
    
  



<a class="btn btn-outline-primary btn-page-header btn-sm" href="https://doi.org/10.1007/978-3-030-69541-5_4" target="_blank" rel="noopener">
  PDF
</a>



<a href="#" class="btn btn-outline-primary btn-page-header btn-sm js-cite-modal"
        data-filename="/publication/dblp-confaccv-0012-w-20/cite.bib">
  Cite
</a>













<a class="btn btn-outline-primary btn-page-header btn-sm" href="https://doi.org/10.1007/978-3-030-69541-5_4" target="_blank" rel="noopener">
  DOI
</a>



    </div>
    

  </div>
  <div class="col-12 col-md-3">
    
    
    
    
    
    
        
        
        

        
          
          
            
          
          
        
          
          
          
        
          
          
          
        
        

        <img src="/publication/dblp-confaccv-0012-w-20/featured_hu3699aaa94be3003441bff909318fbaba_230095_9e551947ef3bcdca637c2bb5c050fcbf.png" srcset="/publication/dblp-confaccv-0012-w-20/featured_hu3699aaa94be3003441bff909318fbaba_230095_862d72f0490051bb43a6663b92f8dd73.png 1200w,/publication/dblp-confaccv-0012-w-20/featured_hu3699aaa94be3003441bff909318fbaba_230095_f86ba04a087b624366df91a22ed0c753.png 800w,/publication/dblp-confaccv-0012-w-20/featured_hu3699aaa94be3003441bff909318fbaba_230095_9e551947ef3bcdca637c2bb5c050fcbf.png 400w" width="450" height="213" alt="Unpaired Multimodal Facial Expression Recognition">

    
    
  </div>
</div>

    
  
    
      







  







  


<div class="media stream-item">
  <div class="col-12 col-md-9">

    <div class="section-subheading article-title mb-0 mt-0">
      <a href="/publication/dblp-confijcai-yin-wpcp-19/" >Capturing Spatial and Temporal Patterns for Facial Landmark Tracking through Adversarial Learning</a>
    </div>

    
    <a href="/publication/dblp-confijcai-yin-wpcp-19/"  class="summary-link">
      <div class="article-style">
        To address the inconsistency between explicit forms of joint label distribution and the ground truth facial landmark distribution, we propose an adversarial learning framework to close the joint distribution inherent in predicted and ground truth facial landmarks.
      </div>
    </a>
    

    <div class="stream-meta article-metadata">

      

      
      <div>
        

  <span >
      <a href="/author/shi-yin/">Shi Yin</a></span>, <span >
      <a href="/author/shangfei-wang/">Shangfei Wang</a></span>, <span >
      <a href="/author/guozhu-peng/">Guozhu Peng</a></span>, <span >
      <a href="/author/xiaoping-chen/">Xiaoping Chen</a></span>, <span >
      <a href="/author/bowen-pan/">Bowen Pan</a></span>
      </div>
      
    </div>

    
    <div class="btn-links">
      








  
    
  



<a class="btn btn-outline-primary btn-page-header btn-sm" href="https://doi.org/10.24963/ijcai.2019/142" target="_blank" rel="noopener">
  PDF
</a>



<a href="#" class="btn btn-outline-primary btn-page-header btn-sm js-cite-modal"
        data-filename="/publication/dblp-confijcai-yin-wpcp-19/cite.bib">
  Cite
</a>













<a class="btn btn-outline-primary btn-page-header btn-sm" href="https://doi.org/10.24963/ijcai.2019/142" target="_blank" rel="noopener">
  DOI
</a>



    </div>
    

  </div>
  <div class="col-12 col-md-3">
    
    
    
    
    
    
        
        
        

        
          
          
            
          
          
        
          
          
          
        
          
          
          
        
        

        <img src="/publication/dblp-confijcai-yin-wpcp-19/featured_hu83191d9c4948d0db84b838d1cd056b88_246371_203eeff9d147b115ffa0366f9d391da8.jpg" srcset="/publication/dblp-confijcai-yin-wpcp-19/featured_hu83191d9c4948d0db84b838d1cd056b88_246371_047819c914bd33c4771fee5c8e595572.jpg 1200w,/publication/dblp-confijcai-yin-wpcp-19/featured_hu83191d9c4948d0db84b838d1cd056b88_246371_b251731992d8b9fa6b85e5380248108a.jpg 800w,/publication/dblp-confijcai-yin-wpcp-19/featured_hu83191d9c4948d0db84b838d1cd056b88_246371_203eeff9d147b115ffa0366f9d391da8.jpg 400w" width="450" height="159" alt="Capturing Spatial and Temporal Patterns for Facial Landmark Tracking through Adversarial Learning">

    
    
  </div>
</div>

    
  
    
      







  







  


<div class="media stream-item">
  <div class="col-12 col-md-9">

    <div class="section-subheading article-title mb-0 mt-0">
      <a href="/publication/dblp-confaaai-peng-w-19/" >Dual Semi-Supervised Learning for Facial Action Unit Recognition</a>
    </div>

    
    <a href="/publication/dblp-confaaai-peng-w-19/"  class="summary-link">
      <div class="article-style">
        Instead of minimizing the distance of two joint distributions directly, which requires the estimation of the marginal distribution of the input, the proposed approach uses an adversarial strategy to exploit the probabilistic duality, thus avoiding the estimation of marginal distribution.
      </div>
    </a>
    

    <div class="stream-meta article-metadata">

      

      
      <div>
        

  <span >
      <a href="/author/guozhu-peng/">Guozhu Peng</a></span>, <span >
      <a href="/author/shangfei-wang/">Shangfei Wang</a></span>
      </div>
      
    </div>

    
    <div class="btn-links">
      








  
    
  



<a class="btn btn-outline-primary btn-page-header btn-sm" href="https://doi.org/10.1609/aaai.v33i01.33018827" target="_blank" rel="noopener">
  PDF
</a>



<a href="#" class="btn btn-outline-primary btn-page-header btn-sm js-cite-modal"
        data-filename="/publication/dblp-confaaai-peng-w-19/cite.bib">
  Cite
</a>













<a class="btn btn-outline-primary btn-page-header btn-sm" href="https://doi.org/10.1609/aaai.v33i01.33018827" target="_blank" rel="noopener">
  DOI
</a>



    </div>
    

  </div>
  <div class="col-12 col-md-3">
    
    
    
    
    
    
        
        
        

        
          
          
            
          
          
        
          
          
          
        
          
          
          
        
        

        <img src="/publication/dblp-confaaai-peng-w-19/featured_hudd7daa1dca088bf83220ca908efacfc0_71773_089b54974db4556ba93b7e166bdddc5c.JPG" srcset="/publication/dblp-confaaai-peng-w-19/featured_hudd7daa1dca088bf83220ca908efacfc0_71773_397388136cfcbe0c20df51a10d7235aa.JPG 1200w,/publication/dblp-confaaai-peng-w-19/featured_hudd7daa1dca088bf83220ca908efacfc0_71773_8b1bb0cc537cef96719bdf0ec4a1393c.JPG 800w,/publication/dblp-confaaai-peng-w-19/featured_hudd7daa1dca088bf83220ca908efacfc0_71773_089b54974db4556ba93b7e166bdddc5c.JPG 400w" width="450" height="186" alt="Dual Semi-Supervised Learning for Facial Action Unit Recognition">

    
    
  </div>
</div>

    
  
    
      







  







  


<div class="media stream-item">
  <div class="col-12 col-md-9">

    <div class="section-subheading article-title mb-0 mt-0">
      <a href="/publication/dblp-confmm-wang-wl-19/" >Identity- and Pose-Robust Facial Expression Recognition through Adversarial Feature Learning</a>
    </div>

    
    <a href="/publication/dblp-confmm-wang-wl-19/"  class="summary-link">
      <div class="article-style">
        Previous facial expression recognition methods either focus on pose variations or identity bias; there is no work that considers both at the same time. To this end, we propose a novel feature representation method that uses adversarial learning to overcome the challenges of both pose variations and identity bias.
      </div>
    </a>
    

    <div class="stream-meta article-metadata">

      

      
      <div>
        

  <span >
      <a href="/author/can-wang/">Can Wang</a></span>, <span >
      <a href="/author/shangfei-wang/">Shangfei Wang</a></span>, <span >
      <a href="/author/guang-liang/">Guang Liang</a></span>
      </div>
      
    </div>

    
    <div class="btn-links">
      








  
    
  



<a class="btn btn-outline-primary btn-page-header btn-sm" href="https://doi.org/10.1145/3343031.3350872" target="_blank" rel="noopener">
  PDF
</a>



<a href="#" class="btn btn-outline-primary btn-page-header btn-sm js-cite-modal"
        data-filename="/publication/dblp-confmm-wang-wl-19/cite.bib">
  Cite
</a>













<a class="btn btn-outline-primary btn-page-header btn-sm" href="https://doi.org/10.1145/3343031.3350872" target="_blank" rel="noopener">
  DOI
</a>



    </div>
    

  </div>
  <div class="col-12 col-md-3">
    
    
    
    
    
    
        
        
        

        
          
          
            
          
          
        
          
          
          
        
          
          
          
        
        

        <img src="/publication/dblp-confmm-wang-wl-19/featured_hu6a8143bd29530d43f3d1f85b0d638a7e_147884_8322093bc655759ac50acd78f63d2948.JPG" srcset="/publication/dblp-confmm-wang-wl-19/featured_hu6a8143bd29530d43f3d1f85b0d638a7e_147884_f16752c4d3629fd3c97e95b3cb37c1e1.JPG 1200w,/publication/dblp-confmm-wang-wl-19/featured_hu6a8143bd29530d43f3d1f85b0d638a7e_147884_1423689b20982e81a2de5a1fadb217ad.JPG 800w,/publication/dblp-confmm-wang-wl-19/featured_hu6a8143bd29530d43f3d1f85b0d638a7e_147884_8322093bc655759ac50acd78f63d2948.JPG 400w" width="450" height="277" alt="Identity- and Pose-Robust Facial Expression Recognition through Adversarial Feature Learning">

    
    
  </div>
</div>

    
  
    
      







  







  


<div class="media stream-item">
  <div class="col-12 col-md-9">

    <div class="section-subheading article-title mb-0 mt-0">
      <a href="/publication/dblp-confaaai-pan-wj-19/" >Image Aesthetic Assessment Assisted by Attributes through Adversarial Learning</a>
    </div>

    
    <a href="/publication/dblp-confaaai-pan-wj-19/"  class="summary-link">
      <div class="article-style">
        In this paper, we propose a novel attributes enhanced image aesthetic assessment, where the attributes are used as privileged information.
      </div>
    </a>
    

    <div class="stream-meta article-metadata">

      

      
      <div>
        

  <span >
      <a href="/author/bowen-pan/">Bowen Pan</a></span>, <span >
      <a href="/author/shangfei-wang/">Shangfei Wang</a></span>, <span >
      <a href="/author/qisheng-jiang/">Qisheng Jiang</a></span>
      </div>
      
    </div>

    
    <div class="btn-links">
      








  
    
  



<a class="btn btn-outline-primary btn-page-header btn-sm" href="https://doi.org/10.1609/aaai.v33i01.3301679" target="_blank" rel="noopener">
  PDF
</a>



<a href="#" class="btn btn-outline-primary btn-page-header btn-sm js-cite-modal"
        data-filename="/publication/dblp-confaaai-pan-wj-19/cite.bib">
  Cite
</a>













<a class="btn btn-outline-primary btn-page-header btn-sm" href="https://doi.org/10.1609/aaai.v33i01.3301679" target="_blank" rel="noopener">
  DOI
</a>



    </div>
    

  </div>
  <div class="col-12 col-md-3">
    
    
    
    
    
    
        
        
        

        
          
          
            
          
          
        
          
          
          
        
          
          
          
        
        

        <img src="/publication/dblp-confaaai-pan-wj-19/featured_hu2a714477b00e9d9445d21b555b800bff_206782_1aa0903c7926b51abb2e357eb4a283a7.PNG" srcset="/publication/dblp-confaaai-pan-wj-19/featured_hu2a714477b00e9d9445d21b555b800bff_206782_169ca3d33492725134ceb088215de3d6.PNG 1200w,/publication/dblp-confaaai-pan-wj-19/featured_hu2a714477b00e9d9445d21b555b800bff_206782_6d7d94f873330971c33927e75c9f8df4.PNG 800w,/publication/dblp-confaaai-pan-wj-19/featured_hu2a714477b00e9d9445d21b555b800bff_206782_1aa0903c7926b51abb2e357eb4a283a7.PNG 400w" width="450" height="281" alt="Image Aesthetic Assessment Assisted by Attributes through Adversarial Learning">

    
    
  </div>
</div>

    
  
    
      







  







  


<div class="media stream-item">
  <div class="col-12 col-md-9">

    <div class="section-subheading article-title mb-0 mt-0">
      <a href="/publication/dblp-conffgr-yin-fwwdw-19/" >Integrating Facial Images, Speeches and Time for Empathy Prediction</a>
    </div>

    
    <a href="/publication/dblp-conffgr-yin-fwwdw-19/"  class="summary-link">
      <div class="article-style">
        In this paper, we propose a multi-modal deep network to predict the empathy of the listener during the conversation between two people. First, we use a bottleneck residual network proposed by to learn visual representation from facial images, and adopt fully connected network to extract audio features from the listener‚Äôs speech. Second, we propose to use the current time stage as a temporal feature, and fuse it with the learned visual and audio representations. Neural network regression is used to predict the empathy level. We further select the representative subset training data to train the proposed multi-modal deep network. Experimental results on the One-Minute Empathy Prediction dataset demonstrate the effectiveness of the proposed method.
      </div>
    </a>
    

    <div class="stream-meta article-metadata">

      

      
      <div>
        

  <span >
      <a href="/author/shi-yin/">Shi Yin</a></span>, <span >
      <a href="/author/yonggan-fu/">Yonggan Fu</a></span>, <span >
      <a href="/author/can-wang/">Can Wang</a></span>, <span >
      <a href="/author/runlong-wu/">Runlong Wu</a></span>, <span >
      <a href="/author/heyan-ding/">Heyan Ding</a></span>, <span >
      <a href="/author/shangfei-wang/">Shangfei Wang</a></span>
      </div>
      
    </div>

    
    <div class="btn-links">
      








  
    
  



<a class="btn btn-outline-primary btn-page-header btn-sm" href="https://doi.org/10.1109/FG.2019.8756621" target="_blank" rel="noopener">
  PDF
</a>



<a href="#" class="btn btn-outline-primary btn-page-header btn-sm js-cite-modal"
        data-filename="/publication/dblp-conffgr-yin-fwwdw-19/cite.bib">
  Cite
</a>













<a class="btn btn-outline-primary btn-page-header btn-sm" href="https://doi.org/10.1109/FG.2019.8756621" target="_blank" rel="noopener">
  DOI
</a>



    </div>
    

  </div>
  <div class="col-12 col-md-3">
    
    
    
    
    
    
        
        
        

        
          
          
            
          
          
        
          
          
          
        
          
          
          
        
        

        <img src="/publication/dblp-conffgr-yin-fwwdw-19/featured_hu2c69917c91855ca124047f9f7218696c_145547_73b16d004db21e231b2d143ae054666b.jpg" srcset="/publication/dblp-conffgr-yin-fwwdw-19/featured_hu2c69917c91855ca124047f9f7218696c_145547_8f86281a72809879b36ac3dfd090be20.jpg 1200w,/publication/dblp-conffgr-yin-fwwdw-19/featured_hu2c69917c91855ca124047f9f7218696c_145547_6c81e6c620ad74ab22f9bf8e33a2d5e3.jpg 800w,/publication/dblp-conffgr-yin-fwwdw-19/featured_hu2c69917c91855ca124047f9f7218696c_145547_73b16d004db21e231b2d143ae054666b.jpg 400w" width="450" height="257" alt="Integrating Facial Images, Speeches and Time for Empathy Prediction">

    
    
  </div>
</div>

    
  
    
      







  







  


<div class="media stream-item">
  <div class="col-12 col-md-9">

    <div class="section-subheading article-title mb-0 mt-0">
      <a href="/publication/dblp-confmm-pan-wx-19/" >Occluded Facial Expression Recognition Enhanced through Privileged Information</a>
    </div>

    
    <a href="/publication/dblp-confmm-pan-wx-19/"  class="summary-link">
      <div class="article-style">
        we propose using non-occluded facial images as privileged information to assist the learning process of the occluded view. Specifically, two deep neural networks are first trained from occluded and non-occluded images respectively.
      </div>
    </a>
    

    <div class="stream-meta article-metadata">

      

      
      <div>
        

  <span >
      <a href="/author/bowen-pan/">Bowen Pan</a></span>, <span >
      <a href="/author/shangfei-wang/">Shangfei Wang</a></span>, <span >
      <a href="/author/bin-xia/">Bin Xia</a></span>
      </div>
      
    </div>

    
    <div class="btn-links">
      








  
    
  



<a class="btn btn-outline-primary btn-page-header btn-sm" href="https://doi.org/10.1145/3343031.3351049" target="_blank" rel="noopener">
  PDF
</a>



<a href="#" class="btn btn-outline-primary btn-page-header btn-sm js-cite-modal"
        data-filename="/publication/dblp-confmm-pan-wx-19/cite.bib">
  Cite
</a>













<a class="btn btn-outline-primary btn-page-header btn-sm" href="https://doi.org/10.1145/3343031.3351049" target="_blank" rel="noopener">
  DOI
</a>



    </div>
    

  </div>
  <div class="col-12 col-md-3">
    
    
    
    
    
    
        
        
        

        
          
          
            
          
          
        
          
          
          
        
          
          
          
        
        

        <img src="/publication/dblp-confmm-pan-wx-19/featured_hucf65a7887d8ff00a6d7a1a8fcf59d04f_218950_b4a29631e522b4e6470db369efa91f8a.png" srcset="/publication/dblp-confmm-pan-wx-19/featured_hucf65a7887d8ff00a6d7a1a8fcf59d04f_218950_a4606a5b5320fd2d1b6d846834d0b4ed.png 1200w,/publication/dblp-confmm-pan-wx-19/featured_hucf65a7887d8ff00a6d7a1a8fcf59d04f_218950_c93c12d80b71b21a0891b5f7d12e4869.png 800w,/publication/dblp-confmm-pan-wx-19/featured_hucf65a7887d8ff00a6d7a1a8fcf59d04f_218950_b4a29631e522b4e6470db369efa91f8a.png 400w" width="450" height="291" alt="Occluded Facial Expression Recognition Enhanced through Privileged Information">

    
    
  </div>
</div>

    
  
    
      







  







  


<div class="media stream-item">
  <div class="col-12 col-md-9">

    <div class="section-subheading article-title mb-0 mt-0">
      <a href="/publication/dblp-conffgr-hao-wpj-18/" >Facial Action Unit Recognition Augmented by Their Dependencies</a>
    </div>

    
    <a href="/publication/dblp-conffgr-hao-wpj-18/"  class="summary-link">
      <div class="article-style">
        We propose employing the latent regression Bayesian network to effectively capture the high-order and global dependencies among AUs.
      </div>
    </a>
    

    <div class="stream-meta article-metadata">

      

      
      <div>
        

  <span >
      <a href="/author/longfei-hao/">Longfei Hao</a></span>, <span >
      <a href="/author/shangfei-wang/">Shangfei Wang</a></span>, <span >
      <a href="/author/guozhu-peng/">Guozhu Peng</a></span>, <span >
      <a href="/author/qiang-ji/">Qiang Ji</a></span>
      </div>
      
    </div>

    
    <div class="btn-links">
      








  
    
  



<a class="btn btn-outline-primary btn-page-header btn-sm" href="https://doi.org/10.1109/FG.2018.00036" target="_blank" rel="noopener">
  PDF
</a>



<a href="#" class="btn btn-outline-primary btn-page-header btn-sm js-cite-modal"
        data-filename="/publication/dblp-conffgr-hao-wpj-18/cite.bib">
  Cite
</a>













<a class="btn btn-outline-primary btn-page-header btn-sm" href="https://doi.org/10.1109/FG.2018.00036" target="_blank" rel="noopener">
  DOI
</a>



    </div>
    

  </div>
  <div class="col-12 col-md-3">
    
    
    
    
    
    
        
        
        

        
          
          
            
          
          
        
          
          
          
        
          
          
          
        
        

        <img src="/publication/dblp-conffgr-hao-wpj-18/featured_hu186a5b8e8dbef9b8cf16ed6628856e29_401689_4e5aed58f4f12f680ea9f495b9deccb7.jpg" srcset="/publication/dblp-conffgr-hao-wpj-18/featured_hu186a5b8e8dbef9b8cf16ed6628856e29_401689_851a8c69bd5eca99c456bfbc95d65496.jpg 1200w,/publication/dblp-conffgr-hao-wpj-18/featured_hu186a5b8e8dbef9b8cf16ed6628856e29_401689_33a134ae6a03059b8c7d8b37bda51b22.jpg 800w,/publication/dblp-conffgr-hao-wpj-18/featured_hu186a5b8e8dbef9b8cf16ed6628856e29_401689_4e5aed58f4f12f680ea9f495b9deccb7.jpg 400w" width="450" height="225" alt="Facial Action Unit Recognition Augmented by Their Dependencies">

    
    
  </div>
</div>

    
  
    
      







  







  


<div class="media stream-item">
  <div class="col-12 col-md-9">

    <div class="section-subheading article-title mb-0 mt-0">
      <a href="/publication/dblp-confmm-pan-w-18/" >Facial Expression Recognition Enhanced by Thermal Images through Adversarial Learning</a>
    </div>

    
    <a href="/publication/dblp-confmm-pan-w-18/"  class="summary-link">
      <div class="article-style">
        In this paper, we propose a novel facial expression recognition method enhanced by thermal images. Our method leverages thermal images to construct better visible feature representation and classifiers during training through adversarial learning and similarity constraints. Specifically, we learn two deep neural networks for expression classification from visible and thermal images.
      </div>
    </a>
    

    <div class="stream-meta article-metadata">

      

      
      <div>
        

  <span >
      <a href="/author/bowen-pan/">Bowen Pan</a></span>, <span >
      <a href="/author/shangfei-wang/">Shangfei Wang</a></span>
      </div>
      
    </div>

    
    <div class="btn-links">
      








  
    
  



<a class="btn btn-outline-primary btn-page-header btn-sm" href="https://doi.org/10.1145/3240508.3240608" target="_blank" rel="noopener">
  PDF
</a>



<a href="#" class="btn btn-outline-primary btn-page-header btn-sm js-cite-modal"
        data-filename="/publication/dblp-confmm-pan-w-18/cite.bib">
  Cite
</a>













<a class="btn btn-outline-primary btn-page-header btn-sm" href="https://doi.org/10.1145/3240508.3240608" target="_blank" rel="noopener">
  DOI
</a>



    </div>
    

  </div>
  <div class="col-12 col-md-3">
    
    
    
    
    
    
        
        
        

        
          
          
            
          
          
        
          
          
          
        
          
          
          
        
        

        <img src="/publication/dblp-confmm-pan-w-18/featured_hubbc45ea9bf79dd9eb975f82036f90e3d_46195_1f791eea86e50dfa6e30ca0216207fee.jpg" srcset="/publication/dblp-confmm-pan-w-18/featured_hubbc45ea9bf79dd9eb975f82036f90e3d_46195_b0d24c5046306cf24ec3e7b1927afd38.jpg 1200w,/publication/dblp-confmm-pan-w-18/featured_hubbc45ea9bf79dd9eb975f82036f90e3d_46195_0e1a6fe593d50f19eb6dde8bc2cbfc8c.jpg 800w,/publication/dblp-confmm-pan-w-18/featured_hubbc45ea9bf79dd9eb975f82036f90e3d_46195_1f791eea86e50dfa6e30ca0216207fee.jpg 400w" width="450" height="293" alt="Facial Expression Recognition Enhanced by Thermal Images through Adversarial Learning">

    
    
  </div>
</div>

    
  
    
      







  







  


<div class="media stream-item">
  <div class="col-12 col-md-9">

    <div class="section-subheading article-title mb-0 mt-0">
      <a href="/publication/dblp-confmm-wang-w-18/" >Personalized Multiple Facial Action Unit Recognition through Generative Adversarial Recognition Network</a>
    </div>

    
    <a href="/publication/dblp-confmm-wang-w-18/"  class="summary-link">
      <div class="article-style">
        we propose a novel generative adversarial recognition network (GARN) for personalized AU recognition without any assumptions. Specifically, the proposed GARN consists of a generator, a discriminator, and a classifier.
      </div>
    </a>
    

    <div class="stream-meta article-metadata">

      

      
      <div>
        

  <span >
      <a href="/author/can-wang/">Can Wang</a></span>, <span >
      <a href="/author/shangfei-wang/">Shangfei Wang</a></span>
      </div>
      
    </div>

    
    <div class="btn-links">
      








  
    
  



<a class="btn btn-outline-primary btn-page-header btn-sm" href="https://doi.org/10.1145/3240508.3240613" target="_blank" rel="noopener">
  PDF
</a>



<a href="#" class="btn btn-outline-primary btn-page-header btn-sm js-cite-modal"
        data-filename="/publication/dblp-confmm-wang-w-18/cite.bib">
  Cite
</a>













<a class="btn btn-outline-primary btn-page-header btn-sm" href="https://doi.org/10.1145/3240508.3240613" target="_blank" rel="noopener">
  DOI
</a>



    </div>
    

  </div>
  <div class="col-12 col-md-3">
    
    
    
    
    
    
        
        
        

        
          
          
            
          
          
        
          
          
          
        
          
          
          
        
        

        <img src="/publication/dblp-confmm-wang-w-18/featured_hu2b619861a230c980126dcce3eefea7b7_166103_b0154ae2cdcb7222c38c08ab743fd4b8.png" srcset="/publication/dblp-confmm-wang-w-18/featured_hu2b619861a230c980126dcce3eefea7b7_166103_588c971da9bdb565ab7ef81a180df4f4.png 1200w,/publication/dblp-confmm-wang-w-18/featured_hu2b619861a230c980126dcce3eefea7b7_166103_6d0772433fa9c175cfbc853d189ccf16.png 800w,/publication/dblp-confmm-wang-w-18/featured_hu2b619861a230c980126dcce3eefea7b7_166103_b0154ae2cdcb7222c38c08ab743fd4b8.png 400w" width="450" height="196" alt="Personalized Multiple Facial Action Unit Recognition through Generative Adversarial Recognition Network">

    
    
  </div>
</div>

    
  
    
      







  







  


<div class="media stream-item">
  <div class="col-12 col-md-9">

    <div class="section-subheading article-title mb-0 mt-0">
      <a href="/publication/dblp-confcvpr-peng-w-18/" >Weakly Supervised Facial Action Unit Recognition Through Adversarial Training</a>
    </div>

    
    <a href="/publication/dblp-confcvpr-peng-w-18/"  class="summary-link">
      <div class="article-style">
        We propose a novel weakly supervised AU recognition method to learn AU classifiers with only expression labels. Specifically, we notice that there exist domain knowledge about expressions and AUs that can be represented as prior probabilities. We generate pseudo AU data for each expression; for AU classifiers‚Äô training, we propose an RAN model, which consists of a recognition model and a discrimination mode trained simultaneously by leveraging an adversarial process, to make the distribution of the recognized AU close to the distribution of the pseudo AU data. Furthermore, we extend the proposed method to semi-supervised learning with partially AU-annotated images.
      </div>
    </a>
    

    <div class="stream-meta article-metadata">

      

      
      <div>
        

  <span >
      <a href="/author/guozhu-peng/">Guozhu Peng</a></span>, <span >
      <a href="/author/shangfei-wang/">Shangfei Wang</a></span>
      </div>
      
    </div>

    
    <div class="btn-links">
      








  
    
  



<a class="btn btn-outline-primary btn-page-header btn-sm" href="http://openaccess.thecvf.com/content_cvpr_2018/html/Peng_Weakly_Supervised_Facial_CVPR_2018_paper.html" target="_blank" rel="noopener">
  PDF
</a>



<a href="#" class="btn btn-outline-primary btn-page-header btn-sm js-cite-modal"
        data-filename="/publication/dblp-confcvpr-peng-w-18/cite.bib">
  Cite
</a>













<a class="btn btn-outline-primary btn-page-header btn-sm" href="https://doi.org/10.1109/CVPR.2018.00233" target="_blank" rel="noopener">
  DOI
</a>



    </div>
    

  </div>
  <div class="col-12 col-md-3">
    
    
    
    
    
    
        
        
        

        
          
          
            
          
          
        
          
          
          
        
          
          
          
        
        

        <img src="/publication/dblp-confcvpr-peng-w-18/featured_hu1005fd87fac0ca886d0287f946d2eb97_137194_e1237ffd5f8876a8ef2a7432ef0f0626.jpg" srcset="/publication/dblp-confcvpr-peng-w-18/featured_hu1005fd87fac0ca886d0287f946d2eb97_137194_eb845323703deb37263a01cdd2794b56.jpg 1200w,/publication/dblp-confcvpr-peng-w-18/featured_hu1005fd87fac0ca886d0287f946d2eb97_137194_e8a295a412a8af7dea13b83b886c461c.jpg 800w,/publication/dblp-confcvpr-peng-w-18/featured_hu1005fd87fac0ca886d0287f946d2eb97_137194_e1237ffd5f8876a8ef2a7432ef0f0626.jpg 400w" width="450" height="224" alt="Weakly Supervised Facial Action Unit Recognition Through Adversarial Training">

    
    
  </div>
</div>

    
  
    
      







  







  


<div class="media stream-item">
  <div class="col-12 col-md-9">

    <div class="section-subheading article-title mb-0 mt-0">
      <a href="/publication/dblp-conficcv-gan-whj-17/" >A Multimodal Deep Regression Bayesian Network for Affective Video Content Analyses</a>
    </div>

    
    <a href="/publication/dblp-conficcv-gan-whj-17/"  class="summary-link">
      <div class="article-style">
        In this paper, we propose a new multimodal learning method, multimodal deep regression Bayesian network (MMDRBN), to construct the high-level joint representation of visual and audio modalities for emotion tagging. Then the MMDRBN is transformed into an inference network by minimizing the KL-divergence. After that, the inference network is used to predict discrete or continuous affective scores from video content.
      </div>
    </a>
    

    <div class="stream-meta article-metadata">

      

      
      <div>
        

  <span >
      <a href="/author/quan-gan/">Quan Gan</a></span>, <span >
      <a href="/author/shangfei-wang/">Shangfei Wang</a></span>, <span >
      <a href="/author/longfei-hao/">Longfei Hao</a></span>, <span >
      <a href="/author/qiang-ji/">Qiang Ji</a></span>
      </div>
      
    </div>

    
    <div class="btn-links">
      








  
    
  



<a class="btn btn-outline-primary btn-page-header btn-sm" href="https://doi.org/10.1109/ICCV.2017.547" target="_blank" rel="noopener">
  PDF
</a>



<a href="#" class="btn btn-outline-primary btn-page-header btn-sm js-cite-modal"
        data-filename="/publication/dblp-conficcv-gan-whj-17/cite.bib">
  Cite
</a>













<a class="btn btn-outline-primary btn-page-header btn-sm" href="https://doi.org/10.1109/ICCV.2017.547" target="_blank" rel="noopener">
  DOI
</a>



    </div>
    

  </div>
  <div class="col-12 col-md-3">
    
    
    
    
    
    
        
        
        

        
          
          
            
          
          
        
          
          
          
        
          
          
          
        
        

        <img src="/publication/dblp-conficcv-gan-whj-17/featured_hu0ff0903a4051f103a27ffc3b5d24d0b2_144232_73833da6a7494787d2806f1758247750.jpg" srcset="/publication/dblp-conficcv-gan-whj-17/featured_hu0ff0903a4051f103a27ffc3b5d24d0b2_144232_751564fd9d32dff94ee9db54beb8d778.jpg 1200w,/publication/dblp-conficcv-gan-whj-17/featured_hu0ff0903a4051f103a27ffc3b5d24d0b2_144232_8690e66d2d92c552193cfe97d4866ff2.jpg 800w,/publication/dblp-conficcv-gan-whj-17/featured_hu0ff0903a4051f103a27ffc3b5d24d0b2_144232_73833da6a7494787d2806f1758247750.jpg 400w" width="450" height="243" alt="A Multimodal Deep Regression Bayesian Network for Affective Video Content Analyses">

    
    
  </div>
</div>

    
  
    
      







  







  


<div class="media stream-item">
  <div class="col-12 col-md-9">

    <div class="section-subheading article-title mb-0 mt-0">
      <a href="/publication/dblp-confaaai-wu-wj-17/" >Capturing Dependencies among Labels and Features for Multiple Emotion Tagging of Multimedia Data</a>
    </div>

    
    <a href="/publication/dblp-confaaai-wu-wj-17/"  class="summary-link">
      <div class="article-style">
        To the best of our knowledge, this paper is the first work to assign multiple emotions to multimedia data by exploring the emotional relationships at both feature and label levels. By learning the shared features with a multi-task RBM classifier and modeling the dependencies among emotion labels with a hierarchy RBM model, the proposed approaches can exploit both top-down and bottom-up relations among emotions independently and dependently to improve multiple emotions tagging for multimedia.
      </div>
    </a>
    

    <div class="stream-meta article-metadata">

      

      
      <div>
        

  <span >
      <a href="/author/shan-wu/">Shan Wu</a></span>, <span >
      <a href="/author/shangfei-wang/">Shangfei Wang</a></span>, <span >
      <a href="/author/qiang-ji/">Qiang Ji</a></span>
      </div>
      
    </div>

    
    <div class="btn-links">
      








  
    
  



<a class="btn btn-outline-primary btn-page-header btn-sm" href="http://aaai.org/ocs/index.php/AAAI/AAAI17/paper/view/14672" target="_blank" rel="noopener">
  PDF
</a>



<a href="#" class="btn btn-outline-primary btn-page-header btn-sm js-cite-modal"
        data-filename="/publication/dblp-confaaai-wu-wj-17/cite.bib">
  Cite
</a>















    </div>
    

  </div>
  <div class="col-12 col-md-3">
    
    
    
    
    
    
        
        
        

        
          
          
            
          
          
        
          
          
          
        
          
          
          
        
        

        <img src="/publication/dblp-confaaai-wu-wj-17/featured_hu703fb9dff318081c89de1de4167dd036_98867_fc2719919d1bcc59ee45e8bbe3b7b669.JPG" srcset="/publication/dblp-confaaai-wu-wj-17/featured_hu703fb9dff318081c89de1de4167dd036_98867_0710191a3a1426d0908c53e974bcce47.JPG 1200w,/publication/dblp-confaaai-wu-wj-17/featured_hu703fb9dff318081c89de1de4167dd036_98867_c2707fbad111d55f29a208d05bfe4a4f.JPG 800w,/publication/dblp-confaaai-wu-wj-17/featured_hu703fb9dff318081c89de1de4167dd036_98867_fc2719919d1bcc59ee45e8bbe3b7b669.JPG 400w" width="450" height="260" alt="Capturing Dependencies among Labels and Features for Multiple Emotion Tagging of Multimedia Data">

    
    
  </div>
</div>

    
  
    
      







  







  


<div class="media stream-item">
  <div class="col-12 col-md-9">

    <div class="section-subheading article-title mb-0 mt-0">
      <a href="/publication/dblp-confmm-yang-w-17/" >Capturing Spatial and Temporal Patterns for Distinguishing between Posed and Spontaneous Expressions</a>
    </div>

    
    <a href="/publication/dblp-confmm-yang-w-17/"  class="summary-link">
      <div class="article-style">
        In this paper, we introduce a novel dynamic model, termed as interval temporal restricted Boltzmann machine(IT-RBM), to jointly capture global spatial patterns and complex temporal patterns embedded in posed expressions and spontaneous expressions respectively for distinguishing posed and spontaneous expressions. The proposed IT-RBM is a three-layer hierarchical probabilistic graphical model.
      </div>
    </a>
    

    <div class="stream-meta article-metadata">

      

      
      <div>
        

  <span >
      <a href="/author/jiajia-yang/">Jiajia Yang</a></span>, <span >
      <a href="/author/shangfei-wang/">Shangfei Wang</a></span>
      </div>
      
    </div>

    
    <div class="btn-links">
      








  
    
  



<a class="btn btn-outline-primary btn-page-header btn-sm" href="https://doi.org/10.1145/3123266.3123350" target="_blank" rel="noopener">
  PDF
</a>



<a href="#" class="btn btn-outline-primary btn-page-header btn-sm js-cite-modal"
        data-filename="/publication/dblp-confmm-yang-w-17/cite.bib">
  Cite
</a>













<a class="btn btn-outline-primary btn-page-header btn-sm" href="https://doi.org/10.1145/3123266.3123350" target="_blank" rel="noopener">
  DOI
</a>



    </div>
    

  </div>
  <div class="col-12 col-md-3">
    
    
    
    
    
    
        
        
        

        
          
          
            
          
          
        
          
          
          
        
          
          
          
        
        

        <img src="/publication/dblp-confmm-yang-w-17/featured_hu0bd183b9574ce216c435855d547551be_121258_1de23f25ced74ea692793faa0b57e8be.jpg" srcset="/publication/dblp-confmm-yang-w-17/featured_hu0bd183b9574ce216c435855d547551be_121258_e81852d36fbc461258f08aaf4503d4b1.jpg 1200w,/publication/dblp-confmm-yang-w-17/featured_hu0bd183b9574ce216c435855d547551be_121258_6eb9ab66864a77092a3aa7b0e10fb177.jpg 800w,/publication/dblp-confmm-yang-w-17/featured_hu0bd183b9574ce216c435855d547551be_121258_1de23f25ced74ea692793faa0b57e8be.jpg 400w" width="450" height="271" alt="Capturing Spatial and Temporal Patterns for Distinguishing between Posed and Spontaneous Expressions">

    
    
  </div>
</div>

    
  
    
      







  







  


<div class="media stream-item">
  <div class="col-12 col-md-9">

    <div class="section-subheading article-title mb-0 mt-0">
      <a href="/publication/dblp-conficcv-wu-wpj-17/" >Deep Facial Action Unit Recognition from Partially Labeled Data</a>
    </div>

    
    <a href="/publication/dblp-conficcv-wu-wpj-17/"  class="summary-link">
      <div class="article-style">
        Inspired by the observations that AUs are samples of the underlying AU label distributions, we propose a deep facial action unit recognition approach learning from partially AU-labeled training data through incorporating such spatial regular patterns of AU labels presented in ground-truth AU labels into the learning process of AU classifiers from a large-scale facial images without AU annotations.
      </div>
    </a>
    

    <div class="stream-meta article-metadata">

      

      
      <div>
        

  <span >
      <a href="/author/shan-wu/">Shan Wu</a></span>, <span >
      <a href="/author/shangfei-wang/">Shangfei Wang</a></span>, <span >
      <a href="/author/bowen-pan/">Bowen Pan</a></span>, <span >
      <a href="/author/qiang-ji/">Qiang Ji</a></span>
      </div>
      
    </div>

    
    <div class="btn-links">
      








  
    
  



<a class="btn btn-outline-primary btn-page-header btn-sm" href="https://doi.org/10.1109/ICCV.2017.426" target="_blank" rel="noopener">
  PDF
</a>



<a href="#" class="btn btn-outline-primary btn-page-header btn-sm js-cite-modal"
        data-filename="/publication/dblp-conficcv-wu-wpj-17/cite.bib">
  Cite
</a>













<a class="btn btn-outline-primary btn-page-header btn-sm" href="https://doi.org/10.1109/ICCV.2017.426" target="_blank" rel="noopener">
  DOI
</a>



    </div>
    

  </div>
  <div class="col-12 col-md-3">
    
    
    
  </div>
</div>

    
  
    
      







  







  


<div class="media stream-item">
  <div class="col-12 col-md-9">

    <div class="section-subheading article-title mb-0 mt-0">
      <a href="/publication/dblp-confaaai-gan-nwj-17/" >Differentiating Between Posed and Spontaneous Expressions with Latent Regression Bayesian Network</a>
    </div>

    
    <a href="/publication/dblp-confaaai-gan-nwj-17/"  class="summary-link">
      <div class="article-style">
        In this paper, we propose employing the LRBN to effectively capture the high-order and global dependencies among facial geometric features
      </div>
    </a>
    

    <div class="stream-meta article-metadata">

      

      
      <div>
        

  <span >
      <a href="/author/quan-gan/">Quan Gan</a></span>, <span >
      <a href="/author/siqi-nie/">Siqi Nie</a></span>, <span >
      <a href="/author/shangfei-wang/">Shangfei Wang</a></span>, <span >
      <a href="/author/qiang-ji/">Qiang Ji</a></span>
      </div>
      
    </div>

    
    <div class="btn-links">
      








  
    
  



<a class="btn btn-outline-primary btn-page-header btn-sm" href="http://aaai.org/ocs/index.php/AAAI/AAAI17/paper/view/14673" target="_blank" rel="noopener">
  PDF
</a>



<a href="#" class="btn btn-outline-primary btn-page-header btn-sm js-cite-modal"
        data-filename="/publication/dblp-confaaai-gan-nwj-17/cite.bib">
  Cite
</a>















    </div>
    

  </div>
  <div class="col-12 col-md-3">
    
    
    
    
    
    
        
        
        

        
          
          
            
          
          
        
          
          
          
        
          
          
          
        
        

        <img src="/publication/dblp-confaaai-gan-nwj-17/featured_hu5eafe8c2628672eb14e0c65744466ffa_86294_8a22fcfdfc11c0c108c0b6813e43c222.jpg" srcset="/publication/dblp-confaaai-gan-nwj-17/featured_hu5eafe8c2628672eb14e0c65744466ffa_86294_719a94ee2f1f8fc7cc31722d492562c4.jpg 1200w,/publication/dblp-confaaai-gan-nwj-17/featured_hu5eafe8c2628672eb14e0c65744466ffa_86294_3f29ad2ce852f571f88ee9379184ddb1.jpg 800w,/publication/dblp-confaaai-gan-nwj-17/featured_hu5eafe8c2628672eb14e0c65744466ffa_86294_8a22fcfdfc11c0c108c0b6813e43c222.jpg 400w" width="450" height="223" alt="Differentiating Between Posed and Spontaneous Expressions with Latent Regression Bayesian Network">

    
    
  </div>
</div>

    
  
    
      







  







  


<div class="media stream-item">
  <div class="col-12 col-md-9">

    <div class="section-subheading article-title mb-0 mt-0">
      <a href="/publication/dblp-confmm-chen-wwc-17/" >Exploring Domain Knowledge for Affective Video Content Analyses</a>
    </div>

    
    <a href="/publication/dblp-confmm-chen-wwc-17/"  class="summary-link">
      <div class="article-style">
        Most current works employ discriminative features and efficient classifiers for affective video content analyses, without explicitly exploring and leveraging domain knowledge for affective video content analyses. Therefore, in this paper, we propose a novel method to analyze affective video content through exploring domain knowledge. Both audio elements and visual elements are used by film makers to communicate emotions to audience. As a primary study to explore film grammar for affective video content analyses, this paper takes visual elements as an example to demonstrate the feasibility of the proposed affective video content analyses method enhanced through exploring domain knowledge.
      </div>
    </a>
    

    <div class="stream-meta article-metadata">

      

      
      <div>
        

  <span >
      <a href="/author/tanfang-chen/">Tanfang Chen</a></span>, <span >
      <a href="/author/yaxin-wang/">Yaxin Wang</a></span>, <span >
      <a href="/author/shangfei-wang/">Shangfei Wang</a></span>, <span >
      <a href="/author/shiyu-chen/">Shiyu Chen</a></span>
      </div>
      
    </div>

    
    <div class="btn-links">
      








  
    
  



<a class="btn btn-outline-primary btn-page-header btn-sm" href="https://doi.org/10.1145/3123266.3123352" target="_blank" rel="noopener">
  PDF
</a>



<a href="#" class="btn btn-outline-primary btn-page-header btn-sm js-cite-modal"
        data-filename="/publication/dblp-confmm-chen-wwc-17/cite.bib">
  Cite
</a>













<a class="btn btn-outline-primary btn-page-header btn-sm" href="https://doi.org/10.1145/3123266.3123352" target="_blank" rel="noopener">
  DOI
</a>



    </div>
    

  </div>
  <div class="col-12 col-md-3">
    
    
    
  </div>
</div>

    
  
    
      







  







  


<div class="media stream-item">
  <div class="col-12 col-md-9">

    <div class="section-subheading article-title mb-0 mt-0">
      <a href="/publication/dblp-confcvpr-zhao-gwj-16/" >Facial Expression Intensity Estimation Using Ordinal Information</a>
    </div>

    
    <a href="/publication/dblp-confcvpr-zhao-gwj-16/"  class="summary-link">
      <div class="article-style">
        Our contributions include the following aspects. First,we propose a regression approach for expression intensity estimation which exploits both ordinal relationship among different frames within an expression sequence and absolute intensity labels if available. Second, we introduce a unified max-margin learning framework to simultaneously exploit the two sources of information. An efficient algorithm to solve the optimization problem is developed.  Third, our method can generalize to different learning settings depend-ing on the availability of expression intensity annotations.
      </div>
    </a>
    

    <div class="stream-meta article-metadata">

      

      
      <div>
        

  <span >
      <a href="/author/rui-zhao/">Rui Zhao</a></span>, <span >
      <a href="/author/quan-gan/">Quan Gan</a></span>, <span >
      <a href="/author/shangfei-wang/">Shangfei Wang</a></span>, <span >
      <a href="/author/qiang-ji/">Qiang Ji</a></span>
      </div>
      
    </div>

    
    <div class="btn-links">
      








  
    
  



<a class="btn btn-outline-primary btn-page-header btn-sm" href="https://doi.org/10.1109/CVPR.2016.377" target="_blank" rel="noopener">
  PDF
</a>



<a href="#" class="btn btn-outline-primary btn-page-header btn-sm js-cite-modal"
        data-filename="/publication/dblp-confcvpr-zhao-gwj-16/cite.bib">
  Cite
</a>













<a class="btn btn-outline-primary btn-page-header btn-sm" href="https://doi.org/10.1109/CVPR.2016.377" target="_blank" rel="noopener">
  DOI
</a>



    </div>
    

  </div>
  <div class="col-12 col-md-3">
    
    
    
    
    
    
        
        
        

        
          
          
            
          
          
        
          
          
          
        
          
          
          
        
        

        <img src="/publication/dblp-confcvpr-zhao-gwj-16/featured_hu82eae57b0a637c9c9c6659cadeeccee4_129242_bb18464a82e7b688170dcb0e1641dff4.jpg" srcset="/publication/dblp-confcvpr-zhao-gwj-16/featured_hu82eae57b0a637c9c9c6659cadeeccee4_129242_5a29aab009b1b53ad996496f7eb7017d.jpg 1200w,/publication/dblp-confcvpr-zhao-gwj-16/featured_hu82eae57b0a637c9c9c6659cadeeccee4_129242_85f939b75c9c5c81a00f4313d8cdbf74.jpg 800w,/publication/dblp-confcvpr-zhao-gwj-16/featured_hu82eae57b0a637c9c9c6659cadeeccee4_129242_bb18464a82e7b688170dcb0e1641dff4.jpg 400w" width="450" height="91" alt="Facial Expression Intensity Estimation Using Ordinal Information">

    
    
  </div>
</div>

    
  
    
      







  







  


<div class="media stream-item">
  <div class="col-12 col-md-9">

    <div class="section-subheading article-title mb-0 mt-0">
      <a href="/publication/dblp-confmm-wu-wpc-16/" >Facial Expression Recognition with Deep two-view Support Vector Machine</a>
    </div>

    
    <a href="/publication/dblp-confmm-wu-wpc-16/"  class="summary-link">
      <div class="article-style">
        Although their constructed representation reflects thermal infrared images‚Äô supplementary role for visible images, it has no direct relationship to target expression labels. Furthermore, the hand-craft visible and thermal features may not thoroughly reflect the expression patterns embedded in images. Therefore, in this paper, we propose a new deep two-view approach to learn features from both visible and thermal images and leverage the commonality among visible and thermal images for expression recognition.
      </div>
    </a>
    

    <div class="stream-meta article-metadata">

      

      
      <div>
        

  <span >
      <a href="/author/chongliang-wu/">Chongliang Wu</a></span>, <span >
      <a href="/author/shangfei-wang/">Shangfei Wang</a></span>, <span >
      <a href="/author/bowen-pan/">Bowen Pan</a></span>, <span >
      <a href="/author/huaping-chen/">Huaping Chen</a></span>
      </div>
      
    </div>

    
    <div class="btn-links">
      








  
    
  



<a class="btn btn-outline-primary btn-page-header btn-sm" href="https://doi.org/10.1145/2964284.2967295" target="_blank" rel="noopener">
  PDF
</a>



<a href="#" class="btn btn-outline-primary btn-page-header btn-sm js-cite-modal"
        data-filename="/publication/dblp-confmm-wu-wpc-16/cite.bib">
  Cite
</a>













<a class="btn btn-outline-primary btn-page-header btn-sm" href="https://doi.org/10.1145/2964284.2967295" target="_blank" rel="noopener">
  DOI
</a>



    </div>
    

  </div>
  <div class="col-12 col-md-3">
    
    
    
    
    
    
        
        
        

        
          
          
            
          
          
        
          
          
          
        
          
          
          
        
        

        <img src="/publication/dblp-confmm-wu-wpc-16/featured_hu4a007d55c83268e1c22352006fcba45d_180260_635f72ba902a0622704564938eb82871.JPG" srcset="/publication/dblp-confmm-wu-wpc-16/featured_hu4a007d55c83268e1c22352006fcba45d_180260_1d798d35859f55685d4669cede375518.JPG 1200w,/publication/dblp-confmm-wu-wpc-16/featured_hu4a007d55c83268e1c22352006fcba45d_180260_52f7fc3723ec1a01bfd78772bad9217e.JPG 800w,/publication/dblp-confmm-wu-wpc-16/featured_hu4a007d55c83268e1c22352006fcba45d_180260_635f72ba902a0622704564938eb82871.JPG 400w" width="450" height="194" alt="Facial Expression Recognition with Deep two-view Support Vector Machine">

    
    
  </div>
</div>

    
  

  
  
  

</div>


    
      </div>
    

    </div>
  </section>


  </div>

  <div class="page-footer">
    
    
    <div class="container">
      <footer class="site-footer">

  



  <div class="powered-by d-flex flex-wrap pb-2 justify-content-center">
    <span id="busuanzi_container_site_pv">Page View:<span id="busuanzi_value_site_pv"></span></span>
  </div>
  <div class="powered-by d-flex flex-wrap pb-2 justify-content-center">
    <span id="busuanzi_container_site_uv">Unique Visitor:<span id="busuanzi_value_site_uv"></span></span>
  </div>
  

  

  

  
  






  <p class="powered-by">
    
    
    
      
      
      
      
      
      
      Published with <a href="https://wowchemy.com/?utm_campaign=poweredby" target="_blank" rel="noopener">Wowchemy</a> ‚Äî the free, <a href="https://github.com/wowchemy/wowchemy-hugo-modules" target="_blank" rel="noopener">open source</a> website builder that empowers creators.
    
  </p>
</footer>

    </div>
    
  </div>

  
<div id="modal" class="modal fade" role="dialog">
  <div class="modal-dialog">
    <div class="modal-content">
      <div class="modal-header">
        <h5 class="modal-title">Cite</h5>
        <button type="button" class="close" data-dismiss="modal" aria-label="Close">
          <span aria-hidden="true">&times;</span>
        </button>
      </div>
      <div class="modal-body">
        <pre><code class="tex hljs"></code></pre>
      </div>
      <div class="modal-footer">
        <a class="btn btn-outline-primary my-1 js-copy-cite" href="#" target="_blank">
          <i class="fas fa-copy"></i> Copy
        </a>
        <a class="btn btn-outline-primary my-1 js-download-cite" href="#" target="_blank">
          <i class="fas fa-download"></i> Download
        </a>
        <div id="modal-error"></div>
      </div>
    </div>
  </div>
</div>

      

    
    
    
      <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.5.1/jquery.min.js" integrity="sha256-9/aliU8dGd2tb6OSsuzixeV4y/faTqgFtohetphbbj0=" crossorigin="anonymous"></script>
      <script src="https://cdnjs.cloudflare.com/ajax/libs/instant.page/5.1.0/instantpage.min.js" integrity="sha512-1+qUtKoh9XZW7j+6LhRMAyOrgSQKenQ4mluTR+cvxXjP1Z54RxZuzstR/H9kgPXQsVB8IW7DMDFUJpzLjvhGSQ==" crossorigin="anonymous"></script>

      
      
        <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery.imagesloaded/4.1.4/imagesloaded.pkgd.min.js" integrity="sha256-lqvxZrPLtfffUl2G/e7szqSvPBILGbwmsGE1MKlOi0Q=" crossorigin="anonymous"></script>
        <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery.isotope/3.0.6/isotope.pkgd.min.js" integrity="sha256-CBrpuqrMhXwcLLUd5tvQ4euBHCdh7wGlDfNz8vbu/iI=" crossorigin="anonymous"></script>
      

      
      

      

      

    

    
    
    
      <script src="https://cdnjs.cloudflare.com/ajax/libs/leaflet/1.7.1/leaflet.min.js" integrity="sha512-SeiQaaDh73yrb56sTW/RgVdi/mMqNeM2oBwubFHagc5BkixSpP1fvqF47mKzPGWYSSy4RwbBunrJBQ4Co8fRWA==" crossorigin="anonymous"></script>
    

    
    

    
    
    
      
      <script id="search-hit-fuse-template" type="text/x-template">
        <div class="search-hit" id="summary-{{key}}">
          <div class="search-hit-content">
            <div class="search-hit-name">
              <a href="{{relpermalink}}">{{title}}</a>
              <div class="article-metadata search-hit-type">{{type}}</div>
              <p class="search-hit-description">{{snippet}}</p>
            </div>
          </div>
        </div>
      </script>
      
        <script src="https://cdnjs.cloudflare.com/ajax/libs/fuse.js/3.2.1/fuse.min.js" integrity="sha256-VzgmKYmhsGNNN4Ph1kMW+BjoYJM2jV5i4IlFoeZA9XI=" crossorigin="anonymous"></script>
        <script src="https://cdnjs.cloudflare.com/ajax/libs/mark.js/8.11.1/jquery.mark.min.js" integrity="sha256-4HLtjeVgH0eIB3aZ9mLYF6E8oU5chNdjU6p6rrXpl9U=" crossorigin="anonymous"></script>
      
    

    
    

    
    
    
    

    
    <script src="/js/bootstrap.bundle.min.6aed84840afc03ab4d5750157f69c120.js"></script>

    
    
      
      
      
      
      
      
      
    

    
    
    
    
    
    
    
    
      
      
    
    
    <script src="/en/js/wowchemy.min.732501ec195c50108b5d5b22c54532b8.js"></script>

    






</body>
</html>
